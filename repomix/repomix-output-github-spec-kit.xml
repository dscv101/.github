<repomix><file_summary>This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been formatted for parsing in xml style, security check has been disabled.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been formatted for parsing in xml style
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)</notes></file_summary><directory_structure>.github/
  workflows/
    scripts/
      check-release-exists.sh
      create-github-release.sh
      create-release-packages.sh
      generate-release-notes.sh
      get-next-version.sh
      update-version.sh
    docs.yml
    release.yml
  CODEOWNERS
docs/
  .gitignore
  docfx.json
  index.md
  installation.md
  local-development.md
  quickstart.md
  README.md
  toc.yml
memory/
  constitution.md
scripts/
  bash/
    check-prerequisites.sh
    common.sh
    create-new-feature.sh
    setup-plan.sh
    update-agent-context.sh
  powershell/
    check-prerequisites.ps1
    common.ps1
    create-new-feature.ps1
    setup-plan.ps1
    update-agent-context.ps1
src/
  specify_cli/
    __init__.py
templates/
  commands/
    analyze.md
    clarify.md
    constitution.md
    implement.md
    plan.md
    specify.md
    tasks.md
  agent-file-template.md
  plan-template.md
  spec-template.md
  tasks-template.md
.gitignore
AGENTS.md
CHANGELOG.md
CODE_OF_CONDUCT.md
CONTRIBUTING.md
LICENSE
pyproject.toml
README.md
SECURITY.md
spec-driven.md
SUPPORT.md</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path=".github/workflows/scripts/check-release-exists.sh">#!/usr/bin/env bash
set -euo pipefail

# check-release-exists.sh
# Check if a GitHub release already exists for the given version
# Usage: check-release-exists.sh &lt;version&gt;

if [[ $# -ne 1 ]]; then
  echo &quot;Usage: $0 &lt;version&gt;&quot; &gt;&amp;2
  exit 1
fi

VERSION=&quot;$1&quot;

if gh release view &quot;$VERSION&quot; &gt;/dev/null 2&gt;&amp;1; then
  echo &quot;exists=true&quot; &gt;&gt; $GITHUB_OUTPUT
  echo &quot;Release $VERSION already exists, skipping...&quot;
else
  echo &quot;exists=false&quot; &gt;&gt; $GITHUB_OUTPUT
  echo &quot;Release $VERSION does not exist, proceeding...&quot;
fi</file><file path=".github/workflows/scripts/create-github-release.sh">#!/usr/bin/env bash
set -euo pipefail

# create-github-release.sh
# Create a GitHub release with all template zip files
# Usage: create-github-release.sh &lt;version&gt;

if [[ $# -ne 1 ]]; then
  echo &quot;Usage: $0 &lt;version&gt;&quot; &gt;&amp;2
  exit 1
fi

VERSION=&quot;$1&quot;

# Remove &apos;v&apos; prefix from version for release title
VERSION_NO_V=${VERSION#v}

gh release create &quot;$VERSION&quot; \
  .genreleases/spec-kit-template-copilot-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-copilot-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-claude-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-claude-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-gemini-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-gemini-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-cursor-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-cursor-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-opencode-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-opencode-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-qwen-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-qwen-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-windsurf-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-windsurf-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-codex-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-codex-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-kilocode-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-kilocode-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-auggie-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-auggie-ps-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-roo-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-roo-ps-&quot;$VERSION&quot;.zip \
  --title &quot;Spec Kit Templates - $VERSION_NO_V&quot; \
  --notes-file release_notes.md</file><file path=".github/workflows/scripts/create-release-packages.sh">#!/usr/bin/env bash
set -euo pipefail

# create-release-packages.sh (workflow-local)
# Build Spec Kit template release archives for each supported AI assistant and script type.
# Usage: .github/workflows/scripts/create-release-packages.sh &lt;version&gt;
#   Version argument should include leading &apos;v&apos;.
#   Optionally set AGENTS and/or SCRIPTS env vars to limit what gets built.
#     AGENTS  : space or comma separated subset of: claude gemini copilot cursor qwen opencode windsurf codex (default: all)
#     SCRIPTS : space or comma separated subset of: sh ps (default: both)
#   Examples:
#     AGENTS=claude SCRIPTS=sh $0 v0.2.0
#     AGENTS=&quot;copilot,gemini&quot; $0 v0.2.0
#     SCRIPTS=ps $0 v0.2.0

if [[ $# -ne 1 ]]; then
  echo &quot;Usage: $0 &lt;version-with-v-prefix&gt;&quot; &gt;&amp;2
  exit 1
fi
NEW_VERSION=&quot;$1&quot;
if [[ ! $NEW_VERSION =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
  echo &quot;Version must look like v0.0.0&quot; &gt;&amp;2
  exit 1
fi

echo &quot;Building release packages for $NEW_VERSION&quot;

# Create and use .genreleases directory for all build artifacts
GENRELEASES_DIR=&quot;.genreleases&quot;
mkdir -p &quot;$GENRELEASES_DIR&quot;
rm -rf &quot;$GENRELEASES_DIR&quot;/* || true

rewrite_paths() {
  sed -E \
    -e &apos;s@(/?)memory/@.specify/memory/@g&apos; \
    -e &apos;s@(/?)scripts/@.specify/scripts/@g&apos; \
    -e &apos;s@(/?)templates/@.specify/templates/@g&apos;
}

generate_commands() {
  local agent=$1 ext=$2 arg_format=$3 output_dir=$4 script_variant=$5
  mkdir -p &quot;$output_dir&quot;
  for template in templates/commands/*.md; do
    [[ -f &quot;$template&quot; ]] || continue
    local name description script_command body
    name=$(basename &quot;$template&quot; .md)
    
    # Normalize line endings
    file_content=$(tr -d &apos;\r&apos; &lt; &quot;$template&quot;)
    
    # Extract description and script command from YAML frontmatter
    description=$(printf &apos;%s\n&apos; &quot;$file_content&quot; | awk &apos;/^description:/ {sub(/^description:[[:space:]]*/, &quot;&quot;); print; exit}&apos;)
    script_command=$(printf &apos;%s\n&apos; &quot;$file_content&quot; | awk -v sv=&quot;$script_variant&quot; &apos;/^[[:space:]]*&apos;&quot;$script_variant&quot;&apos;:[[:space:]]*/ {sub(/^[[:space:]]*&apos;&quot;$script_variant&quot;&apos;:[[:space:]]*/, &quot;&quot;); print; exit}&apos;)
    
    if [[ -z $script_command ]]; then
      echo &quot;Warning: no script command found for $script_variant in $template&quot; &gt;&amp;2
      script_command=&quot;(Missing script command for $script_variant)&quot;
    fi
    
    # Replace {SCRIPT} placeholder with the script command
    body=$(printf &apos;%s\n&apos; &quot;$file_content&quot; | sed &quot;s|{SCRIPT}|${script_command}|g&quot;)
    
    # Remove the scripts: section from frontmatter while preserving YAML structure
    body=$(printf &apos;%s\n&apos; &quot;$body&quot; | awk &apos;
      /^---$/ { print; if (++dash_count == 1) in_frontmatter=1; else in_frontmatter=0; next }
      in_frontmatter &amp;&amp; /^scripts:$/ { skip_scripts=1; next }
      in_frontmatter &amp;&amp; /^[a-zA-Z].*:/ &amp;&amp; skip_scripts { skip_scripts=0 }
      in_frontmatter &amp;&amp; skip_scripts &amp;&amp; /^[[:space:]]/ { next }
      { print }
    &apos;)
    
    # Apply other substitutions
    body=$(printf &apos;%s\n&apos; &quot;$body&quot; | sed &quot;s/{ARGS}/$arg_format/g&quot; | sed &quot;s/__AGENT__/$agent/g&quot; | rewrite_paths)
    
    case $ext in
      toml)
        { echo &quot;description = \&quot;$description\&quot;&quot;; echo; echo &quot;prompt = \&quot;\&quot;\&quot;&quot;; echo &quot;$body&quot;; echo &quot;\&quot;\&quot;\&quot;&quot;; } &gt; &quot;$output_dir/$name.$ext&quot; ;;
      md)
        echo &quot;$body&quot; &gt; &quot;$output_dir/$name.$ext&quot; ;;
      prompt.md)
        echo &quot;$body&quot; &gt; &quot;$output_dir/$name.$ext&quot; ;;
    esac
  done
}

build_variant() {
  local agent=$1 script=$2
  local base_dir=&quot;$GENRELEASES_DIR/sdd-${agent}-package-${script}&quot;
  echo &quot;Building $agent ($script) package...&quot;
  mkdir -p &quot;$base_dir&quot;
  
  # Copy base structure but filter scripts by variant
  SPEC_DIR=&quot;$base_dir/.specify&quot;
  mkdir -p &quot;$SPEC_DIR&quot;
  
  [[ -d memory ]] &amp;&amp; { cp -r memory &quot;$SPEC_DIR/&quot;; echo &quot;Copied memory -&gt; .specify&quot;; }
  
  # Only copy the relevant script variant directory
  if [[ -d scripts ]]; then
    mkdir -p &quot;$SPEC_DIR/scripts&quot;
    case $script in
      sh)
        [[ -d scripts/bash ]] &amp;&amp; { cp -r scripts/bash &quot;$SPEC_DIR/scripts/&quot;; echo &quot;Copied scripts/bash -&gt; .specify/scripts&quot;; }
        # Copy any script files that aren&apos;t in variant-specific directories
        find scripts -maxdepth 1 -type f -exec cp {} &quot;$SPEC_DIR/scripts/&quot; \; 2&gt;/dev/null || true
        ;;
      ps)
        [[ -d scripts/powershell ]] &amp;&amp; { cp -r scripts/powershell &quot;$SPEC_DIR/scripts/&quot;; echo &quot;Copied scripts/powershell -&gt; .specify/scripts&quot;; }
        # Copy any script files that aren&apos;t in variant-specific directories
        find scripts -maxdepth 1 -type f -exec cp {} &quot;$SPEC_DIR/scripts/&quot; \; 2&gt;/dev/null || true
        ;;
    esac
  fi
  
  [[ -d templates ]] &amp;&amp; { mkdir -p &quot;$SPEC_DIR/templates&quot;; find templates -type f -not -path &quot;templates/commands/*&quot; -exec cp --parents {} &quot;$SPEC_DIR&quot;/ \; ; echo &quot;Copied templates -&gt; .specify/templates&quot;; }
  # Inject variant into plan-template.md within .specify/templates if present
  local plan_tpl=&quot;$base_dir/.specify/templates/plan-template.md&quot;
  if [[ -f &quot;$plan_tpl&quot; ]]; then
    plan_norm=$(tr -d &apos;\r&apos; &lt; &quot;$plan_tpl&quot;)
    # Extract script command from YAML frontmatter
    script_command=$(printf &apos;%s\n&apos; &quot;$plan_norm&quot; | awk -v sv=&quot;$script&quot; &apos;/^[[:space:]]*&apos;&quot;$script&quot;&apos;:[[:space:]]*/ {sub(/^[[:space:]]*&apos;&quot;$script&quot;&apos;:[[:space:]]*/, &quot;&quot;); print; exit}&apos;)
    if [[ -n $script_command ]]; then
      # Always prefix with .specify/ for plan usage
      script_command=&quot;.specify/$script_command&quot;
      # Replace {SCRIPT} placeholder with the script command and __AGENT__ with agent name
      substituted=$(sed &quot;s|{SCRIPT}|${script_command}|g&quot; &quot;$plan_tpl&quot; | tr -d &apos;\r&apos; | sed &quot;s|__AGENT__|${agent}|g&quot;)
      # Strip YAML frontmatter from plan template output (keep body only)
      stripped=$(printf &apos;%s\n&apos; &quot;$substituted&quot; | awk &apos;BEGIN{fm=0;dash=0} /^---$/ {dash++; if(dash==1){fm=1; next} else if(dash==2){fm=0; next}} {if(!fm) print}&apos;)
      printf &apos;%s\n&apos; &quot;$stripped&quot; &gt; &quot;$plan_tpl&quot;
    else
      echo &quot;Warning: no plan-template script command found for $script in YAML frontmatter&quot; &gt;&amp;2
    fi
  fi
  # NOTE: We substitute {ARGS} internally. Outward tokens differ intentionally:
  #   * Markdown/prompt (claude, copilot, cursor, opencode): $ARGUMENTS
  #   * TOML (gemini, qwen): {{args}}
  # This keeps formats readable without extra abstraction.

  case $agent in
    claude)
      mkdir -p &quot;$base_dir/.claude/commands&quot;
      generate_commands claude md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.claude/commands&quot; &quot;$script&quot; ;;
    gemini)
      mkdir -p &quot;$base_dir/.gemini/commands&quot;
      generate_commands gemini toml &quot;{{args}}&quot; &quot;$base_dir/.gemini/commands&quot; &quot;$script&quot;
      [[ -f agent_templates/gemini/GEMINI.md ]] &amp;&amp; cp agent_templates/gemini/GEMINI.md &quot;$base_dir/GEMINI.md&quot; ;;
    copilot)
      mkdir -p &quot;$base_dir/.github/prompts&quot;
      generate_commands copilot prompt.md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.github/prompts&quot; &quot;$script&quot; ;;
    cursor)
      mkdir -p &quot;$base_dir/.cursor/commands&quot;
      generate_commands cursor md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.cursor/commands&quot; &quot;$script&quot; ;;
    qwen)
      mkdir -p &quot;$base_dir/.qwen/commands&quot;
      generate_commands qwen toml &quot;{{args}}&quot; &quot;$base_dir/.qwen/commands&quot; &quot;$script&quot;
      [[ -f agent_templates/qwen/QWEN.md ]] &amp;&amp; cp agent_templates/qwen/QWEN.md &quot;$base_dir/QWEN.md&quot; ;;
    opencode)
      mkdir -p &quot;$base_dir/.opencode/command&quot;
      generate_commands opencode md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.opencode/command&quot; &quot;$script&quot; ;;
    windsurf)
      mkdir -p &quot;$base_dir/.windsurf/workflows&quot;
      generate_commands windsurf md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.windsurf/workflows&quot; &quot;$script&quot; ;;
    codex)
      mkdir -p &quot;$base_dir/.codex/prompts&quot;
      generate_commands codex md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.codex/prompts&quot; &quot;$script&quot; ;;
    kilocode)
      mkdir -p &quot;$base_dir/.kilocode/workflows&quot;
      generate_commands kilocode md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.kilocode/workflows&quot; &quot;$script&quot; ;;
    auggie)
      mkdir -p &quot;$base_dir/.augment/commands&quot;
      generate_commands auggie md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.augment/commands&quot; &quot;$script&quot; ;;
    roo)
      mkdir -p &quot;$base_dir/.roo/commands&quot;
      generate_commands roo md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.roo/commands&quot; &quot;$script&quot; ;;
  esac
  ( cd &quot;$base_dir&quot; &amp;&amp; zip -r &quot;../spec-kit-template-${agent}-${script}-${NEW_VERSION}.zip&quot; . )
  echo &quot;Created $GENRELEASES_DIR/spec-kit-template-${agent}-${script}-${NEW_VERSION}.zip&quot;
}

# Determine agent list
ALL_AGENTS=(claude gemini copilot cursor qwen opencode windsurf codex kilocode auggie roo)
ALL_SCRIPTS=(sh ps)


norm_list() {
  # convert comma+space separated -&gt; space separated unique while preserving order of first occurrence
  tr &apos;,\n&apos; &apos;  &apos; | awk &apos;{for(i=1;i&lt;=NF;i++){if(!seen[$i]++){printf((out?&quot; &quot;:&quot;&quot;) $i)}}}END{printf(&quot;\n&quot;)}&apos;
}

validate_subset() {
  local type=$1; shift; local -n allowed=$1; shift; local items=(&quot;$@&quot;)
  local ok=1
  for it in &quot;${items[@]}&quot;; do
    local found=0
    for a in &quot;${allowed[@]}&quot;; do [[ $it == &quot;$a&quot; ]] &amp;&amp; { found=1; break; }; done
    if [[ $found -eq 0 ]]; then
      echo &quot;Error: unknown $type &apos;$it&apos; (allowed: ${allowed[*]})&quot; &gt;&amp;2
      ok=0
    fi
  done
  return $ok
}

if [[ -n ${AGENTS:-} ]]; then
  mapfile -t AGENT_LIST &lt; &lt;(printf &apos;%s&apos; &quot;$AGENTS&quot; | norm_list)
  validate_subset agent ALL_AGENTS &quot;${AGENT_LIST[@]}&quot; || exit 1
else
  AGENT_LIST=(&quot;${ALL_AGENTS[@]}&quot;)
fi

if [[ -n ${SCRIPTS:-} ]]; then
  mapfile -t SCRIPT_LIST &lt; &lt;(printf &apos;%s&apos; &quot;$SCRIPTS&quot; | norm_list)
  validate_subset script ALL_SCRIPTS &quot;${SCRIPT_LIST[@]}&quot; || exit 1
else
  SCRIPT_LIST=(&quot;${ALL_SCRIPTS[@]}&quot;)
fi

echo &quot;Agents: ${AGENT_LIST[*]}&quot;
echo &quot;Scripts: ${SCRIPT_LIST[*]}&quot;

for agent in &quot;${AGENT_LIST[@]}&quot;; do
  for script in &quot;${SCRIPT_LIST[@]}&quot;; do
    build_variant &quot;$agent&quot; &quot;$script&quot;
  done
done

echo &quot;Archives in $GENRELEASES_DIR:&quot;
ls -1 &quot;$GENRELEASES_DIR&quot;/spec-kit-template-*-&quot;${NEW_VERSION}&quot;.zip</file><file path=".github/workflows/scripts/generate-release-notes.sh">#!/usr/bin/env bash
set -euo pipefail

# generate-release-notes.sh
# Generate release notes from git history
# Usage: generate-release-notes.sh &lt;new_version&gt; &lt;last_tag&gt;

if [[ $# -ne 2 ]]; then
  echo &quot;Usage: $0 &lt;new_version&gt; &lt;last_tag&gt;&quot; &gt;&amp;2
  exit 1
fi

NEW_VERSION=&quot;$1&quot;
LAST_TAG=&quot;$2&quot;

# Get commits since last tag
if [ &quot;$LAST_TAG&quot; = &quot;v0.0.0&quot; ]; then
  # Check how many commits we have and use that as the limit
  COMMIT_COUNT=$(git rev-list --count HEAD)
  if [ &quot;$COMMIT_COUNT&quot; -gt 10 ]; then
    COMMITS=$(git log --oneline --pretty=format:&quot;- %s&quot; HEAD~10..HEAD)
  else
    COMMITS=$(git log --oneline --pretty=format:&quot;- %s&quot; HEAD~$COMMIT_COUNT..HEAD 2&gt;/dev/null || git log --oneline --pretty=format:&quot;- %s&quot;)
  fi
else
  COMMITS=$(git log --oneline --pretty=format:&quot;- %s&quot; $LAST_TAG..HEAD)
fi

# Create release notes
cat &gt; release_notes.md &lt;&lt; EOF
This is the latest set of releases that you can use with your agent of choice. We recommend using the Specify CLI to scaffold your projects, however you can download these independently and manage them yourself.

EOF

echo &quot;Generated release notes:&quot;
cat release_notes.md</file><file path=".github/workflows/scripts/get-next-version.sh">#!/usr/bin/env bash
set -euo pipefail

# get-next-version.sh
# Calculate the next version based on the latest git tag and output GitHub Actions variables
# Usage: get-next-version.sh

# Get the latest tag, or use v0.0.0 if no tags exist
LATEST_TAG=$(git describe --tags --abbrev=0 2&gt;/dev/null || echo &quot;v0.0.0&quot;)
echo &quot;latest_tag=$LATEST_TAG&quot; &gt;&gt; $GITHUB_OUTPUT

# Extract version number and increment
VERSION=$(echo $LATEST_TAG | sed &apos;s/v//&apos;)
IFS=&apos;.&apos; read -ra VERSION_PARTS &lt;&lt;&lt; &quot;$VERSION&quot;
MAJOR=${VERSION_PARTS[0]:-0}
MINOR=${VERSION_PARTS[1]:-0}
PATCH=${VERSION_PARTS[2]:-0}

# Increment patch version
PATCH=$((PATCH + 1))
NEW_VERSION=&quot;v$MAJOR.$MINOR.$PATCH&quot;

echo &quot;new_version=$NEW_VERSION&quot; &gt;&gt; $GITHUB_OUTPUT
echo &quot;New version will be: $NEW_VERSION&quot;</file><file path=".github/workflows/scripts/update-version.sh">#!/usr/bin/env bash
set -euo pipefail

# update-version.sh
# Update version in pyproject.toml (for release artifacts only)
# Usage: update-version.sh &lt;version&gt;

if [[ $# -ne 1 ]]; then
  echo &quot;Usage: $0 &lt;version&gt;&quot; &gt;&amp;2
  exit 1
fi

VERSION=&quot;$1&quot;

# Remove &apos;v&apos; prefix for Python versioning
PYTHON_VERSION=${VERSION#v}

if [ -f &quot;pyproject.toml&quot; ]; then
  sed -i &quot;s/version = \&quot;.*\&quot;/version = \&quot;$PYTHON_VERSION\&quot;/&quot; pyproject.toml
  echo &quot;Updated pyproject.toml version to $PYTHON_VERSION (for release artifacts only)&quot;
else
  echo &quot;Warning: pyproject.toml not found, skipping version update&quot;
fi</file><file path=".github/workflows/docs.yml"># Build and deploy DocFX documentation to GitHub Pages
name: Deploy Documentation to Pages

on:
  # Runs on pushes targeting the default branch
  push:
    branches: [&quot;main&quot;]
    paths:
      - &apos;docs/**&apos;

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: &quot;pages&quot;
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history for git info

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: &apos;8.x&apos;

      - name: Setup DocFX
        run: dotnet tool install -g docfx

      - name: Build with DocFX
        run: |
          cd docs
          docfx docfx.json

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: &apos;docs/_site&apos;

  # Deploy job
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4</file><file path=".github/workflows/release.yml">name: Create Release

on:
  push:
    branches: [ main ]
    paths:
      - &apos;memory/**&apos;
      - &apos;scripts/**&apos;
      - &apos;templates/**&apos;
      - &apos;.github/workflows/**&apos;
  workflow_dispatch:

jobs:
  release:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Get latest tag
        id: get_tag
        run: |
          chmod +x .github/workflows/scripts/get-next-version.sh
          .github/workflows/scripts/get-next-version.sh
      - name: Check if release already exists
        id: check_release
        run: |
          chmod +x .github/workflows/scripts/check-release-exists.sh
          .github/workflows/scripts/check-release-exists.sh ${{ steps.get_tag.outputs.new_version }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Create release package variants
        if: steps.check_release.outputs.exists == &apos;false&apos;
        run: |
          chmod +x .github/workflows/scripts/create-release-packages.sh
          .github/workflows/scripts/create-release-packages.sh ${{ steps.get_tag.outputs.new_version }}
      - name: Generate release notes
        if: steps.check_release.outputs.exists == &apos;false&apos;
        id: release_notes
        run: |
          chmod +x .github/workflows/scripts/generate-release-notes.sh
          .github/workflows/scripts/generate-release-notes.sh ${{ steps.get_tag.outputs.new_version }} ${{ steps.get_tag.outputs.latest_tag }}
      - name: Create GitHub Release
        if: steps.check_release.outputs.exists == &apos;false&apos;
        run: |
          chmod +x .github/workflows/scripts/create-github-release.sh
          .github/workflows/scripts/create-github-release.sh ${{ steps.get_tag.outputs.new_version }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Update version in pyproject.toml (for release artifacts only)
        if: steps.check_release.outputs.exists == &apos;false&apos;
        run: |
          chmod +x .github/workflows/scripts/update-version.sh
          .github/workflows/scripts/update-version.sh ${{ steps.get_tag.outputs.new_version }}</file><file path=".github/CODEOWNERS"># Global code owner
* @localden</file><file path="docs/.gitignore"># DocFX build output
_site/
obj/
.docfx/

# Temporary files
*.tmp
*.log</file><file path="docs/docfx.json">{
  &quot;build&quot;: {
    &quot;content&quot;: [
      {
        &quot;files&quot;: [
          &quot;*.md&quot;,
          &quot;toc.yml&quot;
        ]
      },
      {
        &quot;files&quot;: [
          &quot;../README.md&quot;,
          &quot;../CONTRIBUTING.md&quot;,
          &quot;../CODE_OF_CONDUCT.md&quot;,
          &quot;../SECURITY.md&quot;,
          &quot;../SUPPORT.md&quot;
        ],
        &quot;dest&quot;: &quot;.&quot;
      }
    ],
    &quot;resource&quot;: [
      {
        &quot;files&quot;: [
          &quot;images/**&quot;
        ]
      },
      {
        &quot;files&quot;: [
          &quot;../media/**&quot;
        ],
        &quot;dest&quot;: &quot;media&quot;
      }
    ],
    &quot;overwrite&quot;: [
      {
        &quot;files&quot;: [
          &quot;apidoc/**.md&quot;
        ],
        &quot;exclude&quot;: [
          &quot;obj/**&quot;,
          &quot;_site/**&quot;
        ]
      }
    ],
    &quot;dest&quot;: &quot;_site&quot;,
    &quot;globalMetadataFiles&quot;: [],
    &quot;fileMetadataFiles&quot;: [],
    &quot;template&quot;: [
      &quot;default&quot;,
      &quot;modern&quot;
    ],
    &quot;postProcessors&quot;: [],
    &quot;markdownEngineName&quot;: &quot;markdig&quot;,
    &quot;noLangKeyword&quot;: false,
    &quot;keepFileLink&quot;: false,
    &quot;cleanupCacheHistory&quot;: false,
    &quot;disableGitFeatures&quot;: false,
    &quot;globalMetadata&quot;: {
      &quot;_appTitle&quot;: &quot;Spec Kit Documentation&quot;,
      &quot;_appName&quot;: &quot;Spec Kit&quot;,
      &quot;_appFooter&quot;: &quot;Spec Kit - A specification-driven development toolkit&quot;,
      &quot;_enableSearch&quot;: true,
      &quot;_disableContribution&quot;: false,
      &quot;_gitContribute&quot;: {
        &quot;repo&quot;: &quot;https://github.com/github/spec-kit&quot;,
        &quot;branch&quot;: &quot;main&quot;
      }
    }
  }
}</file><file path="docs/index.md"># Spec Kit

*Build high-quality software faster.*

**An effort to allow organizations to focus on product scenarios rather than writing undifferentiated code with the help of Spec-Driven Development.**

## What is Spec-Driven Development?

Spec-Driven Development **flips the script** on traditional software development. For decades, code has been king — specifications were just scaffolding we built and discarded once the &quot;real work&quot; of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.

## Getting Started

- [Installation Guide](installation.md)
- [Quick Start Guide](quickstart.md)
- [Local Development](local-development.md)

## Core Philosophy

Spec-Driven Development is a structured process that emphasizes:

- **Intent-driven development** where specifications define the &quot;_what_&quot; before the &quot;_how_&quot;
- **Rich specification creation** using guardrails and organizational principles
- **Multi-step refinement** rather than one-shot code generation from prompts
- **Heavy reliance** on advanced AI model capabilities for specification interpretation

## Development Phases

| Phase | Focus | Key Activities |
|-------|-------|----------------|
| **0-to-1 Development** (&quot;Greenfield&quot;) | Generate from scratch | &lt;ul&gt;&lt;li&gt;Start with high-level requirements&lt;/li&gt;&lt;li&gt;Generate specifications&lt;/li&gt;&lt;li&gt;Plan implementation steps&lt;/li&gt;&lt;li&gt;Build production-ready applications&lt;/li&gt;&lt;/ul&gt; |
| **Creative Exploration** | Parallel implementations | &lt;ul&gt;&lt;li&gt;Explore diverse solutions&lt;/li&gt;&lt;li&gt;Support multiple technology stacks &amp; architectures&lt;/li&gt;&lt;li&gt;Experiment with UX patterns&lt;/li&gt;&lt;/ul&gt; |
| **Iterative Enhancement** (&quot;Brownfield&quot;) | Brownfield modernization | &lt;ul&gt;&lt;li&gt;Add features iteratively&lt;/li&gt;&lt;li&gt;Modernize legacy systems&lt;/li&gt;&lt;li&gt;Adapt processes&lt;/li&gt;&lt;/ul&gt; |

## Experimental Goals

Our research and experimentation focus on:

### Technology Independence
- Create applications using diverse technology stacks
- Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks

### Enterprise Constraints
- Demonstrate mission-critical application development
- Incorporate organizational constraints (cloud providers, tech stacks, engineering practices)
- Support enterprise design systems and compliance requirements

### User-Centric Development
- Build applications for different user cohorts and preferences
- Support various development approaches (from vibe-coding to AI-native development)

### Creative &amp; Iterative Processes
- Validate the concept of parallel implementation exploration
- Provide robust iterative feature development workflows
- Extend processes to handle upgrades and modernization tasks

## Contributing

Please see our [Contributing Guide](https://github.com/github/spec-kit/blob/main/CONTRIBUTING.md) for information on how to contribute to this project.

## Support

For support, please check our [Support Guide](https://github.com/github/spec-kit/blob/main/SUPPORT.md) or open an issue on GitHub.</file><file path="docs/installation.md"># Installation Guide

## Prerequisites

- **Linux/macOS** (or Windows; PowerShell scripts now supported without WSL)
- AI coding agent: [Claude Code](https://www.anthropic.com/claude-code), [GitHub Copilot](https://code.visualstudio.com/), or [Gemini CLI](https://github.com/google-gemini/gemini-cli)
- [uv](https://docs.astral.sh/uv/) for package management
- [Python 3.11+](https://www.python.org/downloads/)
- [Git](https://git-scm.com/downloads)

## Installation

### Initialize a New Project

The easiest way to get started is to initialize a new project:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt;
```

Or initialize in the current directory:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init .
# or use the --here flag
uvx --from git+https://github.com/github/spec-kit.git specify init --here
```

### Specify AI Agent

You can proactively specify your AI agent during initialization:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --ai claude
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --ai gemini
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --ai copilot
```

### Specify Script Type (Shell vs PowerShell)

All automation scripts now have both Bash (`.sh`) and PowerShell (`.ps1`) variants.

Auto behavior:
- Windows default: `ps`
- Other OS default: `sh`
- Interactive mode: you&apos;ll be prompted unless you pass `--script`

Force a specific script type:
```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --script sh
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --script ps
```

### Ignore Agent Tools Check

If you prefer to get the templates without checking for the right tools:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;project_name&gt; --ai claude --ignore-agent-tools
```

## Verification

After initialization, you should see the following commands available in your AI agent:
- `/specify` - Create specifications
- `/plan` - Generate implementation plans  
- `/tasks` - Break down into actionable tasks

The `.specify/scripts` directory will contain both `.sh` and `.ps1` scripts.

## Troubleshooting

### Git Credential Manager on Linux

If you&apos;re having issues with Git authentication on Linux, you can install Git Credential Manager:

```bash
#!/usr/bin/env bash
set -e
echo &quot;Downloading Git Credential Manager v2.6.1...&quot;
wget https://github.com/git-ecosystem/git-credential-manager/releases/download/v2.6.1/gcm-linux_amd64.2.6.1.deb
echo &quot;Installing Git Credential Manager...&quot;
sudo dpkg -i gcm-linux_amd64.2.6.1.deb
echo &quot;Configuring Git to use GCM...&quot;
git config --global credential.helper manager
echo &quot;Cleaning up...&quot;
rm gcm-linux_amd64.2.6.1.deb
```</file><file path="docs/local-development.md"># Local Development Guide

This guide shows how to iterate on the `specify` CLI locally without publishing a release or committing to `main` first.

&gt; Scripts now have both Bash (`.sh`) and PowerShell (`.ps1`) variants. The CLI auto-selects based on OS unless you pass `--script sh|ps`.

## 1. Clone and Switch Branches

```bash
git clone https://github.com/github/spec-kit.git
cd spec-kit
# Work on a feature branch
git checkout -b your-feature-branch
```

## 2. Run the CLI Directly (Fastest Feedback)

You can execute the CLI via the module entrypoint without installing anything:

```bash
# From repo root
python -m src.specify_cli --help
python -m src.specify_cli init demo-project --ai claude --ignore-agent-tools --script sh
```

If you prefer invoking the script file style (uses shebang):

```bash
python src/specify_cli/__init__.py init demo-project --script ps
```

## 3. Use Editable Install (Isolated Environment)

Create an isolated environment using `uv` so dependencies resolve exactly like end users get them:

```bash
# Create &amp; activate virtual env (uv auto-manages .venv)
uv venv
source .venv/bin/activate  # or on Windows PowerShell: .venv\Scripts\Activate.ps1

# Install project in editable mode
uv pip install -e .

# Now &apos;specify&apos; entrypoint is available
specify --help
```

Re-running after code edits requires no reinstall because of editable mode.

## 4. Invoke with uvx Directly From Git (Current Branch)

`uvx` can run from a local path (or a Git ref) to simulate user flows:

```bash
uvx --from . specify init demo-uvx --ai copilot --ignore-agent-tools --script sh
```

You can also point uvx at a specific branch without merging:

```bash
# Push your working branch first
git push origin your-feature-branch
uvx --from git+https://github.com/github/spec-kit.git@your-feature-branch specify init demo-branch-test --script ps
```

### 4a. Absolute Path uvx (Run From Anywhere)

If you&apos;re in another directory, use an absolute path instead of `.`:

```bash
uvx --from /mnt/c/GitHub/spec-kit specify --help
uvx --from /mnt/c/GitHub/spec-kit specify init demo-anywhere --ai copilot --ignore-agent-tools --script sh
```

Set an environment variable for convenience:
```bash
export SPEC_KIT_SRC=/mnt/c/GitHub/spec-kit
uvx --from &quot;$SPEC_KIT_SRC&quot; specify init demo-env --ai copilot --ignore-agent-tools --script ps
```

(Optional) Define a shell function:
```bash
specify-dev() { uvx --from /mnt/c/GitHub/spec-kit specify &quot;$@&quot;; }
# Then
specify-dev --help
```

## 5. Testing Script Permission Logic

After running an `init`, check that shell scripts are executable on POSIX systems:

```bash
ls -l scripts | grep .sh
# Expect owner execute bit (e.g. -rwxr-xr-x)
```
On Windows you will instead use the `.ps1` scripts (no chmod needed).

## 6. Run Lint / Basic Checks (Add Your Own)

Currently no enforced lint config is bundled, but you can quickly sanity check importability:
```bash
python -c &quot;import specify_cli; print(&apos;Import OK&apos;)&quot;
```

## 7. Build a Wheel Locally (Optional)

Validate packaging before publishing:

```bash
uv build
ls dist/
```
Install the built artifact into a fresh throwaway environment if needed.

## 8. Using a Temporary Workspace

When testing `init --here` in a dirty directory, create a temp workspace:

```bash
mkdir /tmp/spec-test &amp;&amp; cd /tmp/spec-test
python -m src.specify_cli init --here --ai claude --ignore-agent-tools --script sh  # if repo copied here
```
Or copy only the modified CLI portion if you want a lighter sandbox.

## 9. Debug Network / TLS Skips

If you need to bypass TLS validation while experimenting:

```bash
specify check --skip-tls
specify init demo --skip-tls --ai gemini --ignore-agent-tools --script ps
```
(Use only for local experimentation.)

## 10. Rapid Edit Loop Summary

| Action | Command |
|--------|---------|
| Run CLI directly | `python -m src.specify_cli --help` |
| Editable install | `uv pip install -e .` then `specify ...` |
| Local uvx run (repo root) | `uvx --from . specify ...` |
| Local uvx run (abs path) | `uvx --from /mnt/c/GitHub/spec-kit specify ...` |
| Git branch uvx | `uvx --from git+URL@branch specify ...` |
| Build wheel | `uv build` |

## 11. Cleaning Up

Remove build artifacts / virtual env quickly:
```bash
rm -rf .venv dist build *.egg-info
```

## 12. Common Issues

| Symptom | Fix |
|---------|-----|
| `ModuleNotFoundError: typer` | Run `uv pip install -e .` |
| Scripts not executable (Linux) | Re-run init or `chmod +x scripts/*.sh` |
| Git step skipped | You passed `--no-git` or Git not installed |
| Wrong script type downloaded | Pass `--script sh` or `--script ps` explicitly |
| TLS errors on corporate network | Try `--skip-tls` (not for production) |

## 13. Next Steps

- Update docs and run through Quick Start using your modified CLI
- Open a PR when satisfied
- (Optional) Tag a release once changes land in `main`</file><file path="docs/quickstart.md"># Quick Start Guide

This guide will help you get started with Spec-Driven Development using Spec Kit.

&gt; NEW: All automation scripts now provide both Bash (`.sh`) and PowerShell (`.ps1`) variants. The `specify` CLI auto-selects based on OS unless you pass `--script sh|ps`.

## The 4-Step Process

### 1. Install Specify

Initialize your project depending on the coding agent you&apos;re using:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt;
```

Pick script type explicitly (optional):
```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt; --script ps  # Force PowerShell
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt; --script sh  # Force POSIX shell
```

### 2. Create the Spec

Use the `/specify` command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.

```bash
/specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.
```

### 3. Create a Technical Implementation Plan

Use the `/plan` command to provide your tech stack and architecture choices.

```bash
/plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.
```

### 4. Break Down and Implement

Use `/tasks` to create an actionable task list, then ask your agent to implement the feature.

## Detailed Example: Building Taskify

Here&apos;s a complete example of building a team productivity platform:

### Step 1: Define Requirements with `/specify`

```text
Develop Taskify, a team productivity platform. It should allow users to create projects, add team members,
assign tasks, comment and move tasks between boards in Kanban style. In this initial phase for this feature,
let&apos;s call it &quot;Create Taskify,&quot; let&apos;s have multiple users but the users will be declared ahead of time, predefined.
I want five users in two different categories, one product manager and four engineers. Let&apos;s create three
different sample projects. Let&apos;s have the standard Kanban columns for the status of each task, such as &quot;To Do,&quot;
&quot;In Progress,&quot; &quot;In Review,&quot; and &quot;Done.&quot; There will be no login for this application as this is just the very
first testing thing to ensure that our basic features are set up. For each task in the UI for a task card,
you should be able to change the current status of the task between the different columns in the Kanban work board.
You should be able to leave an unlimited number of comments for a particular card. You should be able to, from that task
card, assign one of the valid users. When you first launch Taskify, it&apos;s going to give you a list of the five users to pick
from. There will be no password required. When you click on a user, you go into the main view, which displays the list of
projects. When you click on a project, you open the Kanban board for that project. You&apos;re going to see the columns.
You&apos;ll be able to drag and drop cards back and forth between different columns. You will see any cards that are
assigned to you, the currently logged in user, in a different color from all the other ones, so you can quickly
see yours. You can edit any comments that you make, but you can&apos;t edit comments that other people made. You can
delete any comments that you made, but you can&apos;t delete comments anybody else made.
```

### Step 2: Refine the Specification

After the initial specification is created, clarify any missing requirements:

```text
For each sample project or project that you create there should be a variable number of tasks between 5 and 15
tasks for each one randomly distributed into different states of completion. Make sure that there&apos;s at least
one task in each stage of completion.
```

Also validate the specification checklist:

```text
Read the review and acceptance checklist, and check off each item in the checklist if the feature spec meets the criteria. Leave it empty if it does not.
```

### Step 3: Generate Technical Plan with `/plan`

Be specific about your tech stack and technical requirements:

```text
We are going to generate this using .NET Aspire, using Postgres as the database. The frontend should use
Blazor server with drag-and-drop task boards, real-time updates. There should be a REST API created with a projects API,
tasks API, and a notifications API.
```

### Step 4: Validate and Implement

Have your AI agent audit the implementation plan:

```text
Now I want you to go and audit the implementation plan and the implementation detail files.
Read through it with an eye on determining whether or not there is a sequence of tasks that you need
to be doing that are obvious from reading this. Because I don&apos;t know if there&apos;s enough here.
```

Finally, implement the solution:

```text
implement specs/002-create-taskify/plan.md
```

## Key Principles

- **Be explicit** about what you&apos;re building and why
- **Don&apos;t focus on tech stack** during specification phase
- **Iterate and refine** your specifications before implementation
- **Validate** the plan before coding begins
- **Let the AI agent handle** the implementation details

## Next Steps

- Read the complete methodology for in-depth guidance
- Check out more examples in the repository
- Explore the source code on GitHub</file><file path="docs/README.md"># Documentation

This folder contains the documentation source files for Spec Kit, built using [DocFX](https://dotnet.github.io/docfx/).

## Building Locally

To build the documentation locally:

1. Install DocFX:
   ```bash
   dotnet tool install -g docfx
   ```

2. Build the documentation:
   ```bash
   cd docs
   docfx docfx.json --serve
   ```

3. Open your browser to `http://localhost:8080` to view the documentation.

## Structure

- `docfx.json` - DocFX configuration file
- `index.md` - Main documentation homepage
- `toc.yml` - Table of contents configuration
- `installation.md` - Installation guide
- `quickstart.md` - Quick start guide
- `_site/` - Generated documentation output (ignored by git)

## Deployment

Documentation is automatically built and deployed to GitHub Pages when changes are pushed to the `main` branch. The workflow is defined in `.github/workflows/docs.yml`.</file><file path="docs/toc.yml"># Home page
- name: Home
  href: index.md

# Getting started section
- name: Getting Started
  items:
    - name: Installation
      href: installation.md
    - name: Quick Start
      href: quickstart.md

# Development workflows
- name: Development
  items:
    - name: Local Development
      href: local-development.md</file><file path="memory/constitution.md"># [PROJECT_NAME] Constitution
&lt;!-- Example: Spec Constitution, TaskFlow Constitution, etc. --&gt;

## Core Principles

### [PRINCIPLE_1_NAME]
&lt;!-- Example: I. Library-First --&gt;
[PRINCIPLE_1_DESCRIPTION]
&lt;!-- Example: Every feature starts as a standalone library; Libraries must be self-contained, independently testable, documented; Clear purpose required - no organizational-only libraries --&gt;

### [PRINCIPLE_2_NAME]
&lt;!-- Example: II. CLI Interface --&gt;
[PRINCIPLE_2_DESCRIPTION]
&lt;!-- Example: Every library exposes functionality via CLI; Text in/out protocol: stdin/args → stdout, errors → stderr; Support JSON + human-readable formats --&gt;

### [PRINCIPLE_3_NAME]
&lt;!-- Example: III. Test-First (NON-NEGOTIABLE) --&gt;
[PRINCIPLE_3_DESCRIPTION]
&lt;!-- Example: TDD mandatory: Tests written → User approved → Tests fail → Then implement; Red-Green-Refactor cycle strictly enforced --&gt;

### [PRINCIPLE_4_NAME]
&lt;!-- Example: IV. Integration Testing --&gt;
[PRINCIPLE_4_DESCRIPTION]
&lt;!-- Example: Focus areas requiring integration tests: New library contract tests, Contract changes, Inter-service communication, Shared schemas --&gt;

### [PRINCIPLE_5_NAME]
&lt;!-- Example: V. Observability, VI. Versioning &amp; Breaking Changes, VII. Simplicity --&gt;
[PRINCIPLE_5_DESCRIPTION]
&lt;!-- Example: Text I/O ensures debuggability; Structured logging required; Or: MAJOR.MINOR.BUILD format; Or: Start simple, YAGNI principles --&gt;

## [SECTION_2_NAME]
&lt;!-- Example: Additional Constraints, Security Requirements, Performance Standards, etc. --&gt;

[SECTION_2_CONTENT]
&lt;!-- Example: Technology stack requirements, compliance standards, deployment policies, etc. --&gt;

## [SECTION_3_NAME]
&lt;!-- Example: Development Workflow, Review Process, Quality Gates, etc. --&gt;

[SECTION_3_CONTENT]
&lt;!-- Example: Code review requirements, testing gates, deployment approval process, etc. --&gt;

## Governance
&lt;!-- Example: Constitution supersedes all other practices; Amendments require documentation, approval, migration plan --&gt;

[GOVERNANCE_RULES]
&lt;!-- Example: All PRs/reviews must verify compliance; Complexity must be justified; Use [GUIDANCE_FILE] for runtime development guidance --&gt;

**Version**: [CONSTITUTION_VERSION] | **Ratified**: [RATIFICATION_DATE] | **Last Amended**: [LAST_AMENDED_DATE]
&lt;!-- Example: Version: 2.1.1 | Ratified: 2025-06-13 | Last Amended: 2025-07-16 --&gt;</file><file path="scripts/bash/check-prerequisites.sh">#!/usr/bin/env bash

# Consolidated prerequisite checking script
#
# This script provides unified prerequisite checking for Spec-Driven Development workflow.
# It replaces the functionality previously spread across multiple scripts.
#
# Usage: ./check-prerequisites.sh [OPTIONS]
#
# OPTIONS:
#   --json              Output in JSON format
#   --require-tasks     Require tasks.md to exist (for implementation phase)
#   --include-tasks     Include tasks.md in AVAILABLE_DOCS list
#   --paths-only        Only output path variables (no validation)
#   --help, -h          Show help message
#
# OUTPUTS:
#   JSON mode: {&quot;FEATURE_DIR&quot;:&quot;...&quot;, &quot;AVAILABLE_DOCS&quot;:[&quot;...&quot;]}
#   Text mode: FEATURE_DIR:... \n AVAILABLE_DOCS: \n ✓/✗ file.md
#   Paths only: REPO_ROOT: ... \n BRANCH: ... \n FEATURE_DIR: ... etc.

set -e

# Parse command line arguments
JSON_MODE=false
REQUIRE_TASKS=false
INCLUDE_TASKS=false
PATHS_ONLY=false

for arg in &quot;$@&quot;; do
    case &quot;$arg&quot; in
        --json)
            JSON_MODE=true
            ;;
        --require-tasks)
            REQUIRE_TASKS=true
            ;;
        --include-tasks)
            INCLUDE_TASKS=true
            ;;
        --paths-only)
            PATHS_ONLY=true
            ;;
        --help|-h)
            cat &lt;&lt; &apos;EOF&apos;
Usage: check-prerequisites.sh [OPTIONS]

Consolidated prerequisite checking for Spec-Driven Development workflow.

OPTIONS:
  --json              Output in JSON format
  --require-tasks     Require tasks.md to exist (for implementation phase)
  --include-tasks     Include tasks.md in AVAILABLE_DOCS list
  --paths-only        Only output path variables (no prerequisite validation)
  --help, -h          Show this help message

EXAMPLES:
  # Check task prerequisites (plan.md required)
  ./check-prerequisites.sh --json
  
  # Check implementation prerequisites (plan.md + tasks.md required)
  ./check-prerequisites.sh --json --require-tasks --include-tasks
  
  # Get feature paths only (no validation)
  ./check-prerequisites.sh --paths-only
  
EOF
            exit 0
            ;;
        *)
            echo &quot;ERROR: Unknown option &apos;$arg&apos;. Use --help for usage information.&quot; &gt;&amp;2
            exit 1
            ;;
    esac
done

# Source common functions
SCRIPT_DIR=&quot;$(cd &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot;
source &quot;$SCRIPT_DIR/common.sh&quot;

# Get feature paths and validate branch
eval $(get_feature_paths)
check_feature_branch &quot;$CURRENT_BRANCH&quot; &quot;$HAS_GIT&quot; || exit 1

# If paths-only mode, output paths and exit (support JSON + paths-only combined)
if $PATHS_ONLY; then
    if $JSON_MODE; then
        # Minimal JSON paths payload (no validation performed)
        printf &apos;{&quot;REPO_ROOT&quot;:&quot;%s&quot;,&quot;BRANCH&quot;:&quot;%s&quot;,&quot;FEATURE_DIR&quot;:&quot;%s&quot;,&quot;FEATURE_SPEC&quot;:&quot;%s&quot;,&quot;IMPL_PLAN&quot;:&quot;%s&quot;,&quot;TASKS&quot;:&quot;%s&quot;}\n&apos; \
            &quot;$REPO_ROOT&quot; &quot;$CURRENT_BRANCH&quot; &quot;$FEATURE_DIR&quot; &quot;$FEATURE_SPEC&quot; &quot;$IMPL_PLAN&quot; &quot;$TASKS&quot;
    else
        echo &quot;REPO_ROOT: $REPO_ROOT&quot;
        echo &quot;BRANCH: $CURRENT_BRANCH&quot;
        echo &quot;FEATURE_DIR: $FEATURE_DIR&quot;
        echo &quot;FEATURE_SPEC: $FEATURE_SPEC&quot;
        echo &quot;IMPL_PLAN: $IMPL_PLAN&quot;
        echo &quot;TASKS: $TASKS&quot;
    fi
    exit 0
fi

# Validate required directories and files
if [[ ! -d &quot;$FEATURE_DIR&quot; ]]; then
    echo &quot;ERROR: Feature directory not found: $FEATURE_DIR&quot; &gt;&amp;2
    echo &quot;Run /specify first to create the feature structure.&quot; &gt;&amp;2
    exit 1
fi

if [[ ! -f &quot;$IMPL_PLAN&quot; ]]; then
    echo &quot;ERROR: plan.md not found in $FEATURE_DIR&quot; &gt;&amp;2
    echo &quot;Run /plan first to create the implementation plan.&quot; &gt;&amp;2
    exit 1
fi

# Check for tasks.md if required
if $REQUIRE_TASKS &amp;&amp; [[ ! -f &quot;$TASKS&quot; ]]; then
    echo &quot;ERROR: tasks.md not found in $FEATURE_DIR&quot; &gt;&amp;2
    echo &quot;Run /tasks first to create the task list.&quot; &gt;&amp;2
    exit 1
fi

# Build list of available documents
docs=()

# Always check these optional docs
[[ -f &quot;$RESEARCH&quot; ]] &amp;&amp; docs+=(&quot;research.md&quot;)
[[ -f &quot;$DATA_MODEL&quot; ]] &amp;&amp; docs+=(&quot;data-model.md&quot;)

# Check contracts directory (only if it exists and has files)
if [[ -d &quot;$CONTRACTS_DIR&quot; ]] &amp;&amp; [[ -n &quot;$(ls -A &quot;$CONTRACTS_DIR&quot; 2&gt;/dev/null)&quot; ]]; then
    docs+=(&quot;contracts/&quot;)
fi

[[ -f &quot;$QUICKSTART&quot; ]] &amp;&amp; docs+=(&quot;quickstart.md&quot;)

# Include tasks.md if requested and it exists
if $INCLUDE_TASKS &amp;&amp; [[ -f &quot;$TASKS&quot; ]]; then
    docs+=(&quot;tasks.md&quot;)
fi

# Output results
if $JSON_MODE; then
    # Build JSON array of documents
    if [[ ${#docs[@]} -eq 0 ]]; then
        json_docs=&quot;[]&quot;
    else
        json_docs=$(printf &apos;&quot;%s&quot;,&apos; &quot;${docs[@]}&quot;)
        json_docs=&quot;[${json_docs%,}]&quot;
    fi
    
    printf &apos;{&quot;FEATURE_DIR&quot;:&quot;%s&quot;,&quot;AVAILABLE_DOCS&quot;:%s}\n&apos; &quot;$FEATURE_DIR&quot; &quot;$json_docs&quot;
else
    # Text output
    echo &quot;FEATURE_DIR:$FEATURE_DIR&quot;
    echo &quot;AVAILABLE_DOCS:&quot;
    
    # Show status of each potential document
    check_file &quot;$RESEARCH&quot; &quot;research.md&quot;
    check_file &quot;$DATA_MODEL&quot; &quot;data-model.md&quot;
    check_dir &quot;$CONTRACTS_DIR&quot; &quot;contracts/&quot;
    check_file &quot;$QUICKSTART&quot; &quot;quickstart.md&quot;
    
    if $INCLUDE_TASKS; then
        check_file &quot;$TASKS&quot; &quot;tasks.md&quot;
    fi
fi</file><file path="scripts/bash/common.sh">#!/usr/bin/env bash
# Common functions and variables for all scripts

# Get repository root, with fallback for non-git repositories
get_repo_root() {
    if git rev-parse --show-toplevel &gt;/dev/null 2&gt;&amp;1; then
        git rev-parse --show-toplevel
    else
        # Fall back to script location for non-git repos
        local script_dir=&quot;$(cd &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot;
        (cd &quot;$script_dir/../../..&quot; &amp;&amp; pwd)
    fi
}

# Get current branch, with fallback for non-git repositories
get_current_branch() {
    # First check if SPECIFY_FEATURE environment variable is set
    if [[ -n &quot;${SPECIFY_FEATURE:-}&quot; ]]; then
        echo &quot;$SPECIFY_FEATURE&quot;
        return
    fi
    
    # Then check git if available
    if git rev-parse --abbrev-ref HEAD &gt;/dev/null 2&gt;&amp;1; then
        git rev-parse --abbrev-ref HEAD
        return
    fi
    
    # For non-git repos, try to find the latest feature directory
    local repo_root=$(get_repo_root)
    local specs_dir=&quot;$repo_root/specs&quot;
    
    if [[ -d &quot;$specs_dir&quot; ]]; then
        local latest_feature=&quot;&quot;
        local highest=0
        
        for dir in &quot;$specs_dir&quot;/*; do
            if [[ -d &quot;$dir&quot; ]]; then
                local dirname=$(basename &quot;$dir&quot;)
                if [[ &quot;$dirname&quot; =~ ^([0-9]{3})- ]]; then
                    local number=${BASH_REMATCH[1]}
                    number=$((10#$number))
                    if [[ &quot;$number&quot; -gt &quot;$highest&quot; ]]; then
                        highest=$number
                        latest_feature=$dirname
                    fi
                fi
            fi
        done
        
        if [[ -n &quot;$latest_feature&quot; ]]; then
            echo &quot;$latest_feature&quot;
            return
        fi
    fi
    
    echo &quot;main&quot;  # Final fallback
}

# Check if we have git available
has_git() {
    git rev-parse --show-toplevel &gt;/dev/null 2&gt;&amp;1
}

check_feature_branch() {
    local branch=&quot;$1&quot;
    local has_git_repo=&quot;$2&quot;
    
    # For non-git repos, we can&apos;t enforce branch naming but still provide output
    if [[ &quot;$has_git_repo&quot; != &quot;true&quot; ]]; then
        echo &quot;[specify] Warning: Git repository not detected; skipped branch validation&quot; &gt;&amp;2
        return 0
    fi
    
    if [[ ! &quot;$branch&quot; =~ ^[0-9]{3}- ]]; then
        echo &quot;ERROR: Not on a feature branch. Current branch: $branch&quot; &gt;&amp;2
        echo &quot;Feature branches should be named like: 001-feature-name&quot; &gt;&amp;2
        return 1
    fi
    
    return 0
}

get_feature_dir() { echo &quot;$1/specs/$2&quot;; }

get_feature_paths() {
    local repo_root=$(get_repo_root)
    local current_branch=$(get_current_branch)
    local has_git_repo=&quot;false&quot;
    
    if has_git; then
        has_git_repo=&quot;true&quot;
    fi
    
    local feature_dir=$(get_feature_dir &quot;$repo_root&quot; &quot;$current_branch&quot;)
    
    cat &lt;&lt;EOF
REPO_ROOT=&apos;$repo_root&apos;
CURRENT_BRANCH=&apos;$current_branch&apos;
HAS_GIT=&apos;$has_git_repo&apos;
FEATURE_DIR=&apos;$feature_dir&apos;
FEATURE_SPEC=&apos;$feature_dir/spec.md&apos;
IMPL_PLAN=&apos;$feature_dir/plan.md&apos;
TASKS=&apos;$feature_dir/tasks.md&apos;
RESEARCH=&apos;$feature_dir/research.md&apos;
DATA_MODEL=&apos;$feature_dir/data-model.md&apos;
QUICKSTART=&apos;$feature_dir/quickstart.md&apos;
CONTRACTS_DIR=&apos;$feature_dir/contracts&apos;
EOF
}

check_file() { [[ -f &quot;$1&quot; ]] &amp;&amp; echo &quot;  ✓ $2&quot; || echo &quot;  ✗ $2&quot;; }
check_dir() { [[ -d &quot;$1&quot; &amp;&amp; -n $(ls -A &quot;$1&quot; 2&gt;/dev/null) ]] &amp;&amp; echo &quot;  ✓ $2&quot; || echo &quot;  ✗ $2&quot;; }</file><file path="scripts/bash/create-new-feature.sh">#!/usr/bin/env bash

set -e

JSON_MODE=false
ARGS=()
for arg in &quot;$@&quot;; do
    case &quot;$arg&quot; in
        --json) JSON_MODE=true ;;
        --help|-h) echo &quot;Usage: $0 [--json] &lt;feature_description&gt;&quot;; exit 0 ;;
        *) ARGS+=(&quot;$arg&quot;) ;;
    esac
done

FEATURE_DESCRIPTION=&quot;${ARGS[*]}&quot;
if [ -z &quot;$FEATURE_DESCRIPTION&quot; ]; then
    echo &quot;Usage: $0 [--json] &lt;feature_description&gt;&quot; &gt;&amp;2
    exit 1
fi

# Function to find the repository root by searching for existing project markers
find_repo_root() {
    local dir=&quot;$1&quot;
    while [ &quot;$dir&quot; != &quot;/&quot; ]; do
        if [ -d &quot;$dir/.git&quot; ] || [ -d &quot;$dir/.specify&quot; ]; then
            echo &quot;$dir&quot;
            return 0
        fi
        dir=&quot;$(dirname &quot;$dir&quot;)&quot;
    done
    return 1
}

# Resolve repository root. Prefer git information when available, but fall back
# to searching for repository markers so the workflow still functions in repositories that
# were initialised with --no-git.
SCRIPT_DIR=&quot;$(cd &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot;

if git rev-parse --show-toplevel &gt;/dev/null 2&gt;&amp;1; then
    REPO_ROOT=$(git rev-parse --show-toplevel)
    HAS_GIT=true
else
    REPO_ROOT=&quot;$(find_repo_root &quot;$SCRIPT_DIR&quot;)&quot;
    if [ -z &quot;$REPO_ROOT&quot; ]; then
        echo &quot;Error: Could not determine repository root. Please run this script from within the repository.&quot; &gt;&amp;2
        exit 1
    fi
    HAS_GIT=false
fi

cd &quot;$REPO_ROOT&quot;

SPECS_DIR=&quot;$REPO_ROOT/specs&quot;
mkdir -p &quot;$SPECS_DIR&quot;

HIGHEST=0
if [ -d &quot;$SPECS_DIR&quot; ]; then
    for dir in &quot;$SPECS_DIR&quot;/*; do
        [ -d &quot;$dir&quot; ] || continue
        dirname=$(basename &quot;$dir&quot;)
        number=$(echo &quot;$dirname&quot; | grep -o &apos;^[0-9]\+&apos; || echo &quot;0&quot;)
        number=$((10#$number))
        if [ &quot;$number&quot; -gt &quot;$HIGHEST&quot; ]; then HIGHEST=$number; fi
    done
fi

NEXT=$((HIGHEST + 1))
FEATURE_NUM=$(printf &quot;%03d&quot; &quot;$NEXT&quot;)

BRANCH_NAME=$(echo &quot;$FEATURE_DESCRIPTION&quot; | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos; | sed &apos;s/[^a-z0-9]/-/g&apos; | sed &apos;s/-\+/-/g&apos; | sed &apos;s/^-//&apos; | sed &apos;s/-$//&apos;)
WORDS=$(echo &quot;$BRANCH_NAME&quot; | tr &apos;-&apos; &apos;\n&apos; | grep -v &apos;^$&apos; | head -3 | tr &apos;\n&apos; &apos;-&apos; | sed &apos;s/-$//&apos;)
BRANCH_NAME=&quot;${FEATURE_NUM}-${WORDS}&quot;

if [ &quot;$HAS_GIT&quot; = true ]; then
    git checkout -b &quot;$BRANCH_NAME&quot;
else
    &gt;&amp;2 echo &quot;[specify] Warning: Git repository not detected; skipped branch creation for $BRANCH_NAME&quot;
fi

FEATURE_DIR=&quot;$SPECS_DIR/$BRANCH_NAME&quot;
mkdir -p &quot;$FEATURE_DIR&quot;

TEMPLATE=&quot;$REPO_ROOT/.specify/templates/spec-template.md&quot;
SPEC_FILE=&quot;$FEATURE_DIR/spec.md&quot;
if [ -f &quot;$TEMPLATE&quot; ]; then cp &quot;$TEMPLATE&quot; &quot;$SPEC_FILE&quot;; else touch &quot;$SPEC_FILE&quot;; fi

# Set the SPECIFY_FEATURE environment variable for the current session
export SPECIFY_FEATURE=&quot;$BRANCH_NAME&quot;

if $JSON_MODE; then
    printf &apos;{&quot;BRANCH_NAME&quot;:&quot;%s&quot;,&quot;SPEC_FILE&quot;:&quot;%s&quot;,&quot;FEATURE_NUM&quot;:&quot;%s&quot;}\n&apos; &quot;$BRANCH_NAME&quot; &quot;$SPEC_FILE&quot; &quot;$FEATURE_NUM&quot;
else
    echo &quot;BRANCH_NAME: $BRANCH_NAME&quot;
    echo &quot;SPEC_FILE: $SPEC_FILE&quot;
    echo &quot;FEATURE_NUM: $FEATURE_NUM&quot;
    echo &quot;SPECIFY_FEATURE environment variable set to: $BRANCH_NAME&quot;
fi</file><file path="scripts/bash/setup-plan.sh">#!/usr/bin/env bash

set -e

# Parse command line arguments
JSON_MODE=false
ARGS=()

for arg in &quot;$@&quot;; do
    case &quot;$arg&quot; in
        --json) 
            JSON_MODE=true 
            ;;
        --help|-h) 
            echo &quot;Usage: $0 [--json]&quot;
            echo &quot;  --json    Output results in JSON format&quot;
            echo &quot;  --help    Show this help message&quot;
            exit 0 
            ;;
        *) 
            ARGS+=(&quot;$arg&quot;) 
            ;;
    esac
done

# Get script directory and load common functions
SCRIPT_DIR=&quot;$(cd &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot;
source &quot;$SCRIPT_DIR/common.sh&quot;

# Get all paths and variables from common functions
eval $(get_feature_paths)

# Check if we&apos;re on a proper feature branch (only for git repos)
check_feature_branch &quot;$CURRENT_BRANCH&quot; &quot;$HAS_GIT&quot; || exit 1

# Ensure the feature directory exists
mkdir -p &quot;$FEATURE_DIR&quot;

# Copy plan template if it exists
TEMPLATE=&quot;$REPO_ROOT/.specify/templates/plan-template.md&quot;
if [[ -f &quot;$TEMPLATE&quot; ]]; then
    cp &quot;$TEMPLATE&quot; &quot;$IMPL_PLAN&quot;
    echo &quot;Copied plan template to $IMPL_PLAN&quot;
else
    echo &quot;Warning: Plan template not found at $TEMPLATE&quot;
    # Create a basic plan file if template doesn&apos;t exist
    touch &quot;$IMPL_PLAN&quot;
fi

# Output results
if $JSON_MODE; then
    printf &apos;{&quot;FEATURE_SPEC&quot;:&quot;%s&quot;,&quot;IMPL_PLAN&quot;:&quot;%s&quot;,&quot;SPECS_DIR&quot;:&quot;%s&quot;,&quot;BRANCH&quot;:&quot;%s&quot;,&quot;HAS_GIT&quot;:&quot;%s&quot;}\n&apos; \
        &quot;$FEATURE_SPEC&quot; &quot;$IMPL_PLAN&quot; &quot;$FEATURE_DIR&quot; &quot;$CURRENT_BRANCH&quot; &quot;$HAS_GIT&quot;
else
    echo &quot;FEATURE_SPEC: $FEATURE_SPEC&quot;
    echo &quot;IMPL_PLAN: $IMPL_PLAN&quot; 
    echo &quot;SPECS_DIR: $FEATURE_DIR&quot;
    echo &quot;BRANCH: $CURRENT_BRANCH&quot;
    echo &quot;HAS_GIT: $HAS_GIT&quot;
fi</file><file path="scripts/bash/update-agent-context.sh">#!/usr/bin/env bash

# Update agent context files with information from plan.md
#
# This script maintains AI agent context files by parsing feature specifications 
# and updating agent-specific configuration files with project information.
#
# MAIN FUNCTIONS:
# 1. Environment Validation
#    - Verifies git repository structure and branch information
#    - Checks for required plan.md files and templates
#    - Validates file permissions and accessibility
#
# 2. Plan Data Extraction
#    - Parses plan.md files to extract project metadata
#    - Identifies language/version, frameworks, databases, and project types
#    - Handles missing or incomplete specification data gracefully
#
# 3. Agent File Management
#    - Creates new agent context files from templates when needed
#    - Updates existing agent files with new project information
#    - Preserves manual additions and custom configurations
#    - Supports multiple AI agent formats and directory structures
#
# 4. Content Generation
#    - Generates language-specific build/test commands
#    - Creates appropriate project directory structures
#    - Updates technology stacks and recent changes sections
#    - Maintains consistent formatting and timestamps
#
# 5. Multi-Agent Support
#    - Handles agent-specific file paths and naming conventions
#    - Supports: Claude, Gemini, Copilot, Cursor, Qwen, opencode, Codex, Windsurf
#    - Can update single agents or all existing agent files
#    - Creates default Claude file if no agent files exist
#
# Usage: ./update-agent-context.sh [agent_type]
# Agent types: claude|gemini|copilot|cursor|qwen|opencode|codex|windsurf
# Leave empty to update all existing agent files

set -e

# Enable strict error handling
set -u
set -o pipefail

#==============================================================================
# Configuration and Global Variables
#==============================================================================

# Get script directory and load common functions
SCRIPT_DIR=&quot;$(cd &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot;
source &quot;$SCRIPT_DIR/common.sh&quot;

# Get all paths and variables from common functions
eval $(get_feature_paths)

NEW_PLAN=&quot;$IMPL_PLAN&quot;  # Alias for compatibility with existing code
AGENT_TYPE=&quot;${1:-}&quot;

# Agent-specific file paths  
CLAUDE_FILE=&quot;$REPO_ROOT/CLAUDE.md&quot;
GEMINI_FILE=&quot;$REPO_ROOT/GEMINI.md&quot;
COPILOT_FILE=&quot;$REPO_ROOT/.github/copilot-instructions.md&quot;
CURSOR_FILE=&quot;$REPO_ROOT/.cursor/rules/specify-rules.mdc&quot;
QWEN_FILE=&quot;$REPO_ROOT/QWEN.md&quot;
AGENTS_FILE=&quot;$REPO_ROOT/AGENTS.md&quot;
WINDSURF_FILE=&quot;$REPO_ROOT/.windsurf/rules/specify-rules.md&quot;
KILOCODE_FILE=&quot;$REPO_ROOT/.kilocode/rules/specify-rules.md&quot;
AUGGIE_FILE=&quot;$REPO_ROOT/.augment/rules/specify-rules.md&quot;
ROO_FILE=&quot;$REPO_ROOT/.roo/rules/specify-rules.md&quot;

# Template file
TEMPLATE_FILE=&quot;$REPO_ROOT/.specify/templates/agent-file-template.md&quot;

# Global variables for parsed plan data
NEW_LANG=&quot;&quot;
NEW_FRAMEWORK=&quot;&quot;
NEW_DB=&quot;&quot;
NEW_PROJECT_TYPE=&quot;&quot;

#==============================================================================
# Utility Functions
#==============================================================================

log_info() {
    echo &quot;INFO: $1&quot;
}

log_success() {
    echo &quot;✓ $1&quot;
}

log_error() {
    echo &quot;ERROR: $1&quot; &gt;&amp;2
}

log_warning() {
    echo &quot;WARNING: $1&quot; &gt;&amp;2
}

# Cleanup function for temporary files
cleanup() {
    local exit_code=$?
    rm -f /tmp/agent_update_*_$$
    rm -f /tmp/manual_additions_$$
    exit $exit_code
}

# Set up cleanup trap
trap cleanup EXIT INT TERM

#==============================================================================
# Validation Functions
#==============================================================================

validate_environment() {
    # Check if we have a current branch/feature (git or non-git)
    if [[ -z &quot;$CURRENT_BRANCH&quot; ]]; then
        log_error &quot;Unable to determine current feature&quot;
        if [[ &quot;$HAS_GIT&quot; == &quot;true&quot; ]]; then
            log_info &quot;Make sure you&apos;re on a feature branch&quot;
        else
            log_info &quot;Set SPECIFY_FEATURE environment variable or create a feature first&quot;
        fi
        exit 1
    fi
    
    # Check if plan.md exists
    if [[ ! -f &quot;$NEW_PLAN&quot; ]]; then
        log_error &quot;No plan.md found at $NEW_PLAN&quot;
        log_info &quot;Make sure you&apos;re working on a feature with a corresponding spec directory&quot;
        if [[ &quot;$HAS_GIT&quot; != &quot;true&quot; ]]; then
            log_info &quot;Use: export SPECIFY_FEATURE=your-feature-name or create a new feature first&quot;
        fi
        exit 1
    fi
    
    # Check if template exists (needed for new files)
    if [[ ! -f &quot;$TEMPLATE_FILE&quot; ]]; then
        log_warning &quot;Template file not found at $TEMPLATE_FILE&quot;
        log_warning &quot;Creating new agent files will fail&quot;
    fi
}

#==============================================================================
# Plan Parsing Functions
#==============================================================================

extract_plan_field() {
    local field_pattern=&quot;$1&quot;
    local plan_file=&quot;$2&quot;
    
    grep &quot;^\*\*${field_pattern}\*\*: &quot; &quot;$plan_file&quot; 2&gt;/dev/null | \
        head -1 | \
        sed &quot;s|^\*\*${field_pattern}\*\*: ||&quot; | \
        sed &apos;s/^[ \t]*//;s/[ \t]*$//&apos; | \
        grep -v &quot;NEEDS CLARIFICATION&quot; | \
        grep -v &quot;^N/A$&quot; || echo &quot;&quot;
}

parse_plan_data() {
    local plan_file=&quot;$1&quot;
    
    if [[ ! -f &quot;$plan_file&quot; ]]; then
        log_error &quot;Plan file not found: $plan_file&quot;
        return 1
    fi
    
    if [[ ! -r &quot;$plan_file&quot; ]]; then
        log_error &quot;Plan file is not readable: $plan_file&quot;
        return 1
    fi
    
    log_info &quot;Parsing plan data from $plan_file&quot;
    
    NEW_LANG=$(extract_plan_field &quot;Language/Version&quot; &quot;$plan_file&quot;)
    NEW_FRAMEWORK=$(extract_plan_field &quot;Primary Dependencies&quot; &quot;$plan_file&quot;)
    NEW_DB=$(extract_plan_field &quot;Storage&quot; &quot;$plan_file&quot;)
    NEW_PROJECT_TYPE=$(extract_plan_field &quot;Project Type&quot; &quot;$plan_file&quot;)
    
    # Log what we found
    if [[ -n &quot;$NEW_LANG&quot; ]]; then
        log_info &quot;Found language: $NEW_LANG&quot;
    else
        log_warning &quot;No language information found in plan&quot;
    fi
    
    if [[ -n &quot;$NEW_FRAMEWORK&quot; ]]; then
        log_info &quot;Found framework: $NEW_FRAMEWORK&quot;
    fi
    
    if [[ -n &quot;$NEW_DB&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;N/A&quot; ]]; then
        log_info &quot;Found database: $NEW_DB&quot;
    fi
    
    if [[ -n &quot;$NEW_PROJECT_TYPE&quot; ]]; then
        log_info &quot;Found project type: $NEW_PROJECT_TYPE&quot;
    fi
}

format_technology_stack() {
    local lang=&quot;$1&quot;
    local framework=&quot;$2&quot;
    local parts=()
    
    # Add non-empty parts
    [[ -n &quot;$lang&quot; &amp;&amp; &quot;$lang&quot; != &quot;NEEDS CLARIFICATION&quot; ]] &amp;&amp; parts+=(&quot;$lang&quot;)
    [[ -n &quot;$framework&quot; &amp;&amp; &quot;$framework&quot; != &quot;NEEDS CLARIFICATION&quot; &amp;&amp; &quot;$framework&quot; != &quot;N/A&quot; ]] &amp;&amp; parts+=(&quot;$framework&quot;)
    
    # Join with proper formatting
    if [[ ${#parts[@]} -eq 0 ]]; then
        echo &quot;&quot;
    elif [[ ${#parts[@]} -eq 1 ]]; then
        echo &quot;${parts[0]}&quot;
    else
        # Join multiple parts with &quot; + &quot;
        local result=&quot;${parts[0]}&quot;
        for ((i=1; i&lt;${#parts[@]}; i++)); do
            result=&quot;$result + ${parts[i]}&quot;
        done
        echo &quot;$result&quot;
    fi
}

#==============================================================================
# Template and Content Generation Functions
#==============================================================================

get_project_structure() {
    local project_type=&quot;$1&quot;
    
    if [[ &quot;$project_type&quot; == *&quot;web&quot;* ]]; then
        echo &quot;backend/\\nfrontend/\\ntests/&quot;
    else
        echo &quot;src/\\ntests/&quot;
    fi
}

get_commands_for_language() {
    local lang=&quot;$1&quot;
    
    case &quot;$lang&quot; in
        *&quot;Python&quot;*)
            echo &quot;cd src &amp;&amp; pytest &amp;&amp; ruff check .&quot;
            ;;
        *&quot;Rust&quot;*)
            echo &quot;cargo test &amp;&amp; cargo clippy&quot;
            ;;
        *&quot;JavaScript&quot;*|*&quot;TypeScript&quot;*)
            echo &quot;npm test &amp;&amp; npm run lint&quot;
            ;;
        *)
            echo &quot;# Add commands for $lang&quot;
            ;;
    esac
}

get_language_conventions() {
    local lang=&quot;$1&quot;
    echo &quot;$lang: Follow standard conventions&quot;
}

create_new_agent_file() {
    local target_file=&quot;$1&quot;
    local temp_file=&quot;$2&quot;
    local project_name=&quot;$3&quot;
    local current_date=&quot;$4&quot;
    
    if [[ ! -f &quot;$TEMPLATE_FILE&quot; ]]; then
        log_error &quot;Template not found at $TEMPLATE_FILE&quot;
        return 1
    fi
    
    if [[ ! -r &quot;$TEMPLATE_FILE&quot; ]]; then
        log_error &quot;Template file is not readable: $TEMPLATE_FILE&quot;
        return 1
    fi
    
    log_info &quot;Creating new agent context file from template...&quot;
    
    if ! cp &quot;$TEMPLATE_FILE&quot; &quot;$temp_file&quot;; then
        log_error &quot;Failed to copy template file&quot;
        return 1
    fi
    
    # Replace template placeholders
    local project_structure
    project_structure=$(get_project_structure &quot;$NEW_PROJECT_TYPE&quot;)
    
    local commands
    commands=$(get_commands_for_language &quot;$NEW_LANG&quot;)
    
    local language_conventions
    language_conventions=$(get_language_conventions &quot;$NEW_LANG&quot;)
    
    # Perform substitutions with error checking using safer approach
    # Escape special characters for sed by using a different delimiter or escaping
    local escaped_lang=$(printf &apos;%s\n&apos; &quot;$NEW_LANG&quot; | sed &apos;s/[\[\.*^$()+{}|]/\\&amp;/g&apos;)
    local escaped_framework=$(printf &apos;%s\n&apos; &quot;$NEW_FRAMEWORK&quot; | sed &apos;s/[\[\.*^$()+{}|]/\\&amp;/g&apos;)
    local escaped_branch=$(printf &apos;%s\n&apos; &quot;$CURRENT_BRANCH&quot; | sed &apos;s/[\[\.*^$()+{}|]/\\&amp;/g&apos;)
    
    # Build technology stack and recent change strings conditionally
    local tech_stack
    if [[ -n &quot;$escaped_lang&quot; &amp;&amp; -n &quot;$escaped_framework&quot; ]]; then
        tech_stack=&quot;- $escaped_lang + $escaped_framework ($escaped_branch)&quot;
    elif [[ -n &quot;$escaped_lang&quot; ]]; then
        tech_stack=&quot;- $escaped_lang ($escaped_branch)&quot;
    elif [[ -n &quot;$escaped_framework&quot; ]]; then
        tech_stack=&quot;- $escaped_framework ($escaped_branch)&quot;
    else
        tech_stack=&quot;- ($escaped_branch)&quot;
    fi

    local recent_change
    if [[ -n &quot;$escaped_lang&quot; &amp;&amp; -n &quot;$escaped_framework&quot; ]]; then
        recent_change=&quot;- $escaped_branch: Added $escaped_lang + $escaped_framework&quot;
    elif [[ -n &quot;$escaped_lang&quot; ]]; then
        recent_change=&quot;- $escaped_branch: Added $escaped_lang&quot;
    elif [[ -n &quot;$escaped_framework&quot; ]]; then
        recent_change=&quot;- $escaped_branch: Added $escaped_framework&quot;
    else
        recent_change=&quot;- $escaped_branch: Added&quot;
    fi

    local substitutions=(
        &quot;s|\[PROJECT NAME\]|$project_name|&quot;
        &quot;s|\[DATE\]|$current_date|&quot;
        &quot;s|\[EXTRACTED FROM ALL PLAN.MD FILES\]|$tech_stack|&quot;
        &quot;s|\[ACTUAL STRUCTURE FROM PLANS\]|$project_structure|g&quot;
        &quot;s|\[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES\]|$commands|&quot;
        &quot;s|\[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE\]|$language_conventions|&quot;
        &quot;s|\[LAST 3 FEATURES AND WHAT THEY ADDED\]|$recent_change|&quot;
    )
    
    for substitution in &quot;${substitutions[@]}&quot;; do
        if ! sed -i.bak -e &quot;$substitution&quot; &quot;$temp_file&quot;; then
            log_error &quot;Failed to perform substitution: $substitution&quot;
            rm -f &quot;$temp_file&quot; &quot;$temp_file.bak&quot;
            return 1
        fi
    done
    
    # Convert \n sequences to actual newlines
    newline=$(printf &apos;\n&apos;)
    sed -i.bak2 &quot;s/\\\\n/${newline}/g&quot; &quot;$temp_file&quot;
    
    # Clean up backup files
    rm -f &quot;$temp_file.bak&quot; &quot;$temp_file.bak2&quot;
    
    return 0
}




update_existing_agent_file() {
    local target_file=&quot;$1&quot;
    local current_date=&quot;$2&quot;
    
    log_info &quot;Updating existing agent context file...&quot;
    
    # Use a single temporary file for atomic update
    local temp_file
    temp_file=$(mktemp) || {
        log_error &quot;Failed to create temporary file&quot;
        return 1
    }
    
    # Process the file in one pass
    local tech_stack=$(format_technology_stack &quot;$NEW_LANG&quot; &quot;$NEW_FRAMEWORK&quot;)
    local new_tech_entries=()
    local new_change_entry=&quot;&quot;
    
    # Prepare new technology entries
    if [[ -n &quot;$tech_stack&quot; ]] &amp;&amp; ! grep -q &quot;$tech_stack&quot; &quot;$target_file&quot;; then
        new_tech_entries+=(&quot;- $tech_stack ($CURRENT_BRANCH)&quot;)
    fi
    
    if [[ -n &quot;$NEW_DB&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;N/A&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;NEEDS CLARIFICATION&quot; ]] &amp;&amp; ! grep -q &quot;$NEW_DB&quot; &quot;$target_file&quot;; then
        new_tech_entries+=(&quot;- $NEW_DB ($CURRENT_BRANCH)&quot;)
    fi
    
    # Prepare new change entry
    if [[ -n &quot;$tech_stack&quot; ]]; then
        new_change_entry=&quot;- $CURRENT_BRANCH: Added $tech_stack&quot;
    elif [[ -n &quot;$NEW_DB&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;N/A&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;NEEDS CLARIFICATION&quot; ]]; then
        new_change_entry=&quot;- $CURRENT_BRANCH: Added $NEW_DB&quot;
    fi
    
    # Process file line by line
    local in_tech_section=false
    local in_changes_section=false
    local tech_entries_added=false
    local changes_entries_added=false
    local existing_changes_count=0
    
    while IFS= read -r line || [[ -n &quot;$line&quot; ]]; do
        # Handle Active Technologies section
        if [[ &quot;$line&quot; == &quot;## Active Technologies&quot; ]]; then
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
            in_tech_section=true
            continue
        elif [[ $in_tech_section == true ]] &amp;&amp; [[ &quot;$line&quot; =~ ^##[[:space:]] ]]; then
            # Add new tech entries before closing the section
            if [[ $tech_entries_added == false ]] &amp;&amp; [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf &apos;%s\n&apos; &quot;${new_tech_entries[@]}&quot; &gt;&gt; &quot;$temp_file&quot;
                tech_entries_added=true
            fi
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
            in_tech_section=false
            continue
        elif [[ $in_tech_section == true ]] &amp;&amp; [[ -z &quot;$line&quot; ]]; then
            # Add new tech entries before empty line in tech section
            if [[ $tech_entries_added == false ]] &amp;&amp; [[ ${#new_tech_entries[@]} -gt 0 ]]; then
                printf &apos;%s\n&apos; &quot;${new_tech_entries[@]}&quot; &gt;&gt; &quot;$temp_file&quot;
                tech_entries_added=true
            fi
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
            continue
        fi
        
        # Handle Recent Changes section
        if [[ &quot;$line&quot; == &quot;## Recent Changes&quot; ]]; then
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
            # Add new change entry right after the heading
            if [[ -n &quot;$new_change_entry&quot; ]]; then
                echo &quot;$new_change_entry&quot; &gt;&gt; &quot;$temp_file&quot;
            fi
            in_changes_section=true
            changes_entries_added=true
            continue
        elif [[ $in_changes_section == true ]] &amp;&amp; [[ &quot;$line&quot; =~ ^##[[:space:]] ]]; then
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
            in_changes_section=false
            continue
        elif [[ $in_changes_section == true ]] &amp;&amp; [[ &quot;$line&quot; == &quot;- &quot;* ]]; then
            # Keep only first 2 existing changes
            if [[ $existing_changes_count -lt 2 ]]; then
                echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
                ((existing_changes_count++))
            fi
            continue
        fi
        
        # Update timestamp
        if [[ &quot;$line&quot; =~ \*\*Last\ updated\*\*:.*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9] ]]; then
            echo &quot;$line&quot; | sed &quot;s/[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]/$current_date/&quot; &gt;&gt; &quot;$temp_file&quot;
        else
            echo &quot;$line&quot; &gt;&gt; &quot;$temp_file&quot;
        fi
    done &lt; &quot;$target_file&quot;
    
    # Post-loop check: if we&apos;re still in the Active Technologies section and haven&apos;t added new entries
    if [[ $in_tech_section == true ]] &amp;&amp; [[ $tech_entries_added == false ]] &amp;&amp; [[ ${#new_tech_entries[@]} -gt 0 ]]; then
        printf &apos;%s\n&apos; &quot;${new_tech_entries[@]}&quot; &gt;&gt; &quot;$temp_file&quot;
    fi
    
    # Move temp file to target atomically
    if ! mv &quot;$temp_file&quot; &quot;$target_file&quot;; then
        log_error &quot;Failed to update target file&quot;
        rm -f &quot;$temp_file&quot;
        return 1
    fi
    
    return 0
}
#==============================================================================
# Main Agent File Update Function
#==============================================================================

update_agent_file() {
    local target_file=&quot;$1&quot;
    local agent_name=&quot;$2&quot;
    
    if [[ -z &quot;$target_file&quot; ]] || [[ -z &quot;$agent_name&quot; ]]; then
        log_error &quot;update_agent_file requires target_file and agent_name parameters&quot;
        return 1
    fi
    
    log_info &quot;Updating $agent_name context file: $target_file&quot;
    
    local project_name
    project_name=$(basename &quot;$REPO_ROOT&quot;)
    local current_date
    current_date=$(date +%Y-%m-%d)
    
    # Create directory if it doesn&apos;t exist
    local target_dir
    target_dir=$(dirname &quot;$target_file&quot;)
    if [[ ! -d &quot;$target_dir&quot; ]]; then
        if ! mkdir -p &quot;$target_dir&quot;; then
            log_error &quot;Failed to create directory: $target_dir&quot;
            return 1
        fi
    fi
    
    if [[ ! -f &quot;$target_file&quot; ]]; then
        # Create new file from template
        local temp_file
        temp_file=$(mktemp) || {
            log_error &quot;Failed to create temporary file&quot;
            return 1
        }
        
        if create_new_agent_file &quot;$target_file&quot; &quot;$temp_file&quot; &quot;$project_name&quot; &quot;$current_date&quot;; then
            if mv &quot;$temp_file&quot; &quot;$target_file&quot;; then
                log_success &quot;Created new $agent_name context file&quot;
            else
                log_error &quot;Failed to move temporary file to $target_file&quot;
                rm -f &quot;$temp_file&quot;
                return 1
            fi
        else
            log_error &quot;Failed to create new agent file&quot;
            rm -f &quot;$temp_file&quot;
            return 1
        fi
    else
        # Update existing file
        if [[ ! -r &quot;$target_file&quot; ]]; then
            log_error &quot;Cannot read existing file: $target_file&quot;
            return 1
        fi
        
        if [[ ! -w &quot;$target_file&quot; ]]; then
            log_error &quot;Cannot write to existing file: $target_file&quot;
            return 1
        fi
        
        if update_existing_agent_file &quot;$target_file&quot; &quot;$current_date&quot;; then
            log_success &quot;Updated existing $agent_name context file&quot;
        else
            log_error &quot;Failed to update existing agent file&quot;
            return 1
        fi
    fi
    
    return 0
}

#==============================================================================
# Agent Selection and Processing
#==============================================================================

update_specific_agent() {
    local agent_type=&quot;$1&quot;
    
    case &quot;$agent_type&quot; in
        claude)
            update_agent_file &quot;$CLAUDE_FILE&quot; &quot;Claude Code&quot;
            ;;
        gemini)
            update_agent_file &quot;$GEMINI_FILE&quot; &quot;Gemini CLI&quot;
            ;;
        copilot)
            update_agent_file &quot;$COPILOT_FILE&quot; &quot;GitHub Copilot&quot;
            ;;
        cursor)
            update_agent_file &quot;$CURSOR_FILE&quot; &quot;Cursor IDE&quot;
            ;;
        qwen)
            update_agent_file &quot;$QWEN_FILE&quot; &quot;Qwen Code&quot;
            ;;
        opencode)
            update_agent_file &quot;$AGENTS_FILE&quot; &quot;opencode&quot;
            ;;
        codex)
            update_agent_file &quot;$AGENTS_FILE&quot; &quot;Codex CLI&quot;
            ;;
        windsurf)
            update_agent_file &quot;$WINDSURF_FILE&quot; &quot;Windsurf&quot;
            ;;
        kilocode)
            update_agent_file &quot;$KILOCODE_FILE&quot; &quot;Kilo Code&quot;
            ;;
        auggie)
            update_agent_file &quot;$AUGGIE_FILE&quot; &quot;Auggie CLI&quot;
            ;;
        roo)
            update_agent_file &quot;$ROO_FILE&quot; &quot;Roo Code&quot;
            ;;
        *)
            log_error &quot;Unknown agent type &apos;$agent_type&apos;&quot;
            log_error &quot;Expected: claude|gemini|copilot|cursor|qwen|opencode|codex|windsurf|kilocode|auggie|roo&quot;
            exit 1
            ;;
    esac
}

update_all_existing_agents() {
    local found_agent=false
    
    # Check each possible agent file and update if it exists
    if [[ -f &quot;$CLAUDE_FILE&quot; ]]; then
        update_agent_file &quot;$CLAUDE_FILE&quot; &quot;Claude Code&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$GEMINI_FILE&quot; ]]; then
        update_agent_file &quot;$GEMINI_FILE&quot; &quot;Gemini CLI&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$COPILOT_FILE&quot; ]]; then
        update_agent_file &quot;$COPILOT_FILE&quot; &quot;GitHub Copilot&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$CURSOR_FILE&quot; ]]; then
        update_agent_file &quot;$CURSOR_FILE&quot; &quot;Cursor IDE&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$QWEN_FILE&quot; ]]; then
        update_agent_file &quot;$QWEN_FILE&quot; &quot;Qwen Code&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$AGENTS_FILE&quot; ]]; then
        update_agent_file &quot;$AGENTS_FILE&quot; &quot;Codex/opencode&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$WINDSURF_FILE&quot; ]]; then
        update_agent_file &quot;$WINDSURF_FILE&quot; &quot;Windsurf&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$KILOCODE_FILE&quot; ]]; then
        update_agent_file &quot;$KILOCODE_FILE&quot; &quot;Kilo Code&quot;
        found_agent=true
    fi

    if [[ -f &quot;$AUGGIE_FILE&quot; ]]; then
        update_agent_file &quot;$AUGGIE_FILE&quot; &quot;Auggie CLI&quot;
        found_agent=true
    fi
    
    if [[ -f &quot;$ROO_FILE&quot; ]]; then
        update_agent_file &quot;$ROO_FILE&quot; &quot;Roo Code&quot;
        found_agent=true
    fi
    
    # If no agent files exist, create a default Claude file
    if [[ &quot;$found_agent&quot; == false ]]; then
        log_info &quot;No existing agent files found, creating default Claude file...&quot;
        update_agent_file &quot;$CLAUDE_FILE&quot; &quot;Claude Code&quot;
    fi
}
print_summary() {
    echo
    log_info &quot;Summary of changes:&quot;
    
    if [[ -n &quot;$NEW_LANG&quot; ]]; then
        echo &quot;  - Added language: $NEW_LANG&quot;
    fi
    
    if [[ -n &quot;$NEW_FRAMEWORK&quot; ]]; then
        echo &quot;  - Added framework: $NEW_FRAMEWORK&quot;
    fi
    
    if [[ -n &quot;$NEW_DB&quot; ]] &amp;&amp; [[ &quot;$NEW_DB&quot; != &quot;N/A&quot; ]]; then
        echo &quot;  - Added database: $NEW_DB&quot;
    fi
    
    echo
    log_info &quot;Usage: $0 [claude|gemini|copilot|cursor|qwen|opencode|codex|windsurf|kilocode|auggie|roo]&quot;
}

#==============================================================================
# Main Execution
#==============================================================================

main() {
    # Validate environment before proceeding
    validate_environment
    
    log_info &quot;=== Updating agent context files for feature $CURRENT_BRANCH ===&quot;
    
    # Parse the plan file to extract project information
    if ! parse_plan_data &quot;$NEW_PLAN&quot;; then
        log_error &quot;Failed to parse plan data&quot;
        exit 1
    fi
    
    # Process based on agent type argument
    local success=true
    
    if [[ -z &quot;$AGENT_TYPE&quot; ]]; then
        # No specific agent provided - update all existing agent files
        log_info &quot;No agent specified, updating all existing agent files...&quot;
        if ! update_all_existing_agents; then
            success=false
        fi
    else
        # Specific agent provided - update only that agent
        log_info &quot;Updating specific agent: $AGENT_TYPE&quot;
        if ! update_specific_agent &quot;$AGENT_TYPE&quot;; then
            success=false
        fi
    fi
    
    # Print summary
    print_summary
    
    if [[ &quot;$success&quot; == true ]]; then
        log_success &quot;Agent context update completed successfully&quot;
        exit 0
    else
        log_error &quot;Agent context update completed with errors&quot;
        exit 1
    fi
}

# Execute main function if script is run directly
if [[ &quot;${BASH_SOURCE[0]}&quot; == &quot;${0}&quot; ]]; then
    main &quot;$@&quot;
fi</file><file path="scripts/powershell/check-prerequisites.ps1">#!/usr/bin/env pwsh

# Consolidated prerequisite checking script (PowerShell)
#
# This script provides unified prerequisite checking for Spec-Driven Development workflow.
# It replaces the functionality previously spread across multiple scripts.
#
# Usage: ./check-prerequisites.ps1 [OPTIONS]
#
# OPTIONS:
#   -Json               Output in JSON format
#   -RequireTasks       Require tasks.md to exist (for implementation phase)
#   -IncludeTasks       Include tasks.md in AVAILABLE_DOCS list
#   -PathsOnly          Only output path variables (no validation)
#   -Help, -h           Show help message

[CmdletBinding()]
param(
    [switch]$Json,
    [switch]$RequireTasks,
    [switch]$IncludeTasks,
    [switch]$PathsOnly,
    [switch]$Help
)

$ErrorActionPreference = &apos;Stop&apos;

# Show help if requested
if ($Help) {
    Write-Output @&quot;
Usage: check-prerequisites.ps1 [OPTIONS]

Consolidated prerequisite checking for Spec-Driven Development workflow.

OPTIONS:
  -Json               Output in JSON format
  -RequireTasks       Require tasks.md to exist (for implementation phase)
  -IncludeTasks       Include tasks.md in AVAILABLE_DOCS list
  -PathsOnly          Only output path variables (no prerequisite validation)
  -Help, -h           Show this help message

EXAMPLES:
  # Check task prerequisites (plan.md required)
  .\check-prerequisites.ps1 -Json
  
  # Check implementation prerequisites (plan.md + tasks.md required)
  .\check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks
  
  # Get feature paths only (no validation)
  .\check-prerequisites.ps1 -PathsOnly

&quot;@
    exit 0
}

# Source common functions
. &quot;$PSScriptRoot/common.ps1&quot;

# Get feature paths and validate branch
$paths = Get-FeaturePathsEnv

if (-not (Test-FeatureBranch -Branch $paths.CURRENT_BRANCH -HasGit:$paths.HAS_GIT)) { 
    exit 1 
}

# If paths-only mode, output paths and exit (support combined -Json -PathsOnly)
if ($PathsOnly) {
    if ($Json) {
        [PSCustomObject]@{
            REPO_ROOT    = $paths.REPO_ROOT
            BRANCH       = $paths.CURRENT_BRANCH
            FEATURE_DIR  = $paths.FEATURE_DIR
            FEATURE_SPEC = $paths.FEATURE_SPEC
            IMPL_PLAN    = $paths.IMPL_PLAN
            TASKS        = $paths.TASKS
        } | ConvertTo-Json -Compress
    } else {
        Write-Output &quot;REPO_ROOT: $($paths.REPO_ROOT)&quot;
        Write-Output &quot;BRANCH: $($paths.CURRENT_BRANCH)&quot;
        Write-Output &quot;FEATURE_DIR: $($paths.FEATURE_DIR)&quot;
        Write-Output &quot;FEATURE_SPEC: $($paths.FEATURE_SPEC)&quot;
        Write-Output &quot;IMPL_PLAN: $($paths.IMPL_PLAN)&quot;
        Write-Output &quot;TASKS: $($paths.TASKS)&quot;
    }
    exit 0
}

# Validate required directories and files
if (-not (Test-Path $paths.FEATURE_DIR -PathType Container)) {
    Write-Output &quot;ERROR: Feature directory not found: $($paths.FEATURE_DIR)&quot;
    Write-Output &quot;Run /specify first to create the feature structure.&quot;
    exit 1
}

if (-not (Test-Path $paths.IMPL_PLAN -PathType Leaf)) {
    Write-Output &quot;ERROR: plan.md not found in $($paths.FEATURE_DIR)&quot;
    Write-Output &quot;Run /plan first to create the implementation plan.&quot;
    exit 1
}

# Check for tasks.md if required
if ($RequireTasks -and -not (Test-Path $paths.TASKS -PathType Leaf)) {
    Write-Output &quot;ERROR: tasks.md not found in $($paths.FEATURE_DIR)&quot;
    Write-Output &quot;Run /tasks first to create the task list.&quot;
    exit 1
}

# Build list of available documents
$docs = @()

# Always check these optional docs
if (Test-Path $paths.RESEARCH) { $docs += &apos;research.md&apos; }
if (Test-Path $paths.DATA_MODEL) { $docs += &apos;data-model.md&apos; }

# Check contracts directory (only if it exists and has files)
if ((Test-Path $paths.CONTRACTS_DIR) -and (Get-ChildItem -Path $paths.CONTRACTS_DIR -ErrorAction SilentlyContinue | Select-Object -First 1)) { 
    $docs += &apos;contracts/&apos; 
}

if (Test-Path $paths.QUICKSTART) { $docs += &apos;quickstart.md&apos; }

# Include tasks.md if requested and it exists
if ($IncludeTasks -and (Test-Path $paths.TASKS)) { 
    $docs += &apos;tasks.md&apos; 
}

# Output results
if ($Json) {
    # JSON output
    [PSCustomObject]@{ 
        FEATURE_DIR = $paths.FEATURE_DIR
        AVAILABLE_DOCS = $docs 
    } | ConvertTo-Json -Compress
} else {
    # Text output
    Write-Output &quot;FEATURE_DIR:$($paths.FEATURE_DIR)&quot;
    Write-Output &quot;AVAILABLE_DOCS:&quot;
    
    # Show status of each potential document
    Test-FileExists -Path $paths.RESEARCH -Description &apos;research.md&apos; | Out-Null
    Test-FileExists -Path $paths.DATA_MODEL -Description &apos;data-model.md&apos; | Out-Null
    Test-DirHasFiles -Path $paths.CONTRACTS_DIR -Description &apos;contracts/&apos; | Out-Null
    Test-FileExists -Path $paths.QUICKSTART -Description &apos;quickstart.md&apos; | Out-Null
    
    if ($IncludeTasks) {
        Test-FileExists -Path $paths.TASKS -Description &apos;tasks.md&apos; | Out-Null
    }
}</file><file path="scripts/powershell/common.ps1">#!/usr/bin/env pwsh
# Common PowerShell functions analogous to common.sh

function Get-RepoRoot {
    try {
        $result = git rev-parse --show-toplevel 2&gt;$null
        if ($LASTEXITCODE -eq 0) {
            return $result
        }
    } catch {
        # Git command failed
    }
    
    # Fall back to script location for non-git repos
    return (Resolve-Path (Join-Path $PSScriptRoot &quot;../../..&quot;)).Path
}

function Get-CurrentBranch {
    # First check if SPECIFY_FEATURE environment variable is set
    if ($env:SPECIFY_FEATURE) {
        return $env:SPECIFY_FEATURE
    }
    
    # Then check git if available
    try {
        $result = git rev-parse --abbrev-ref HEAD 2&gt;$null
        if ($LASTEXITCODE -eq 0) {
            return $result
        }
    } catch {
        # Git command failed
    }
    
    # For non-git repos, try to find the latest feature directory
    $repoRoot = Get-RepoRoot
    $specsDir = Join-Path $repoRoot &quot;specs&quot;
    
    if (Test-Path $specsDir) {
        $latestFeature = &quot;&quot;
        $highest = 0
        
        Get-ChildItem -Path $specsDir -Directory | ForEach-Object {
            if ($_.Name -match &apos;^(\d{3})-&apos;) {
                $num = [int]$matches[1]
                if ($num -gt $highest) {
                    $highest = $num
                    $latestFeature = $_.Name
                }
            }
        }
        
        if ($latestFeature) {
            return $latestFeature
        }
    }
    
    # Final fallback
    return &quot;main&quot;
}

function Test-HasGit {
    try {
        git rev-parse --show-toplevel 2&gt;$null | Out-Null
        return ($LASTEXITCODE -eq 0)
    } catch {
        return $false
    }
}

function Test-FeatureBranch {
    param(
        [string]$Branch,
        [bool]$HasGit = $true
    )
    
    # For non-git repos, we can&apos;t enforce branch naming but still provide output
    if (-not $HasGit) {
        Write-Warning &quot;[specify] Warning: Git repository not detected; skipped branch validation&quot;
        return $true
    }
    
    if ($Branch -notmatch &apos;^[0-9]{3}-&apos;) {
        Write-Output &quot;ERROR: Not on a feature branch. Current branch: $Branch&quot;
        Write-Output &quot;Feature branches should be named like: 001-feature-name&quot;
        return $false
    }
    return $true
}

function Get-FeatureDir {
    param([string]$RepoRoot, [string]$Branch)
    Join-Path $RepoRoot &quot;specs/$Branch&quot;
}

function Get-FeaturePathsEnv {
    $repoRoot = Get-RepoRoot
    $currentBranch = Get-CurrentBranch
    $hasGit = Test-HasGit
    $featureDir = Get-FeatureDir -RepoRoot $repoRoot -Branch $currentBranch
    
    [PSCustomObject]@{
        REPO_ROOT     = $repoRoot
        CURRENT_BRANCH = $currentBranch
        HAS_GIT       = $hasGit
        FEATURE_DIR   = $featureDir
        FEATURE_SPEC  = Join-Path $featureDir &apos;spec.md&apos;
        IMPL_PLAN     = Join-Path $featureDir &apos;plan.md&apos;
        TASKS         = Join-Path $featureDir &apos;tasks.md&apos;
        RESEARCH      = Join-Path $featureDir &apos;research.md&apos;
        DATA_MODEL    = Join-Path $featureDir &apos;data-model.md&apos;
        QUICKSTART    = Join-Path $featureDir &apos;quickstart.md&apos;
        CONTRACTS_DIR = Join-Path $featureDir &apos;contracts&apos;
    }
}

function Test-FileExists {
    param([string]$Path, [string]$Description)
    if (Test-Path -Path $Path -PathType Leaf) {
        Write-Output &quot;  ✓ $Description&quot;
        return $true
    } else {
        Write-Output &quot;  ✗ $Description&quot;
        return $false
    }
}

function Test-DirHasFiles {
    param([string]$Path, [string]$Description)
    if ((Test-Path -Path $Path -PathType Container) -and (Get-ChildItem -Path $Path -ErrorAction SilentlyContinue | Where-Object { -not $_.PSIsContainer } | Select-Object -First 1)) {
        Write-Output &quot;  ✓ $Description&quot;
        return $true
    } else {
        Write-Output &quot;  ✗ $Description&quot;
        return $false
    }
}</file><file path="scripts/powershell/create-new-feature.ps1">#!/usr/bin/env pwsh
# Create a new feature
[CmdletBinding()]
param(
    [switch]$Json,
    [Parameter(ValueFromRemainingArguments = $true)]
    [string[]]$FeatureDescription
)
$ErrorActionPreference = &apos;Stop&apos;

if (-not $FeatureDescription -or $FeatureDescription.Count -eq 0) {
    Write-Error &quot;Usage: ./create-new-feature.ps1 [-Json] &lt;feature description&gt;&quot;
    exit 1
}
$featureDesc = ($FeatureDescription -join &apos; &apos;).Trim()

# Resolve repository root. Prefer git information when available, but fall back
# to searching for repository markers so the workflow still functions in repositories that
# were initialised with --no-git.
function Find-RepositoryRoot {
    param(
        [string]$StartDir,
        [string[]]$Markers = @(&apos;.git&apos;, &apos;.specify&apos;)
    )
    $current = Resolve-Path $StartDir
    while ($true) {
        foreach ($marker in $Markers) {
            if (Test-Path (Join-Path $current $marker)) {
                return $current
            }
        }
        $parent = Split-Path $current -Parent
        if ($parent -eq $current) {
            # Reached filesystem root without finding markers
            return $null
        }
        $current = $parent
    }
}
$fallbackRoot = (Find-RepositoryRoot -StartDir $PSScriptRoot)
if (-not $fallbackRoot) {
    Write-Error &quot;Error: Could not determine repository root. Please run this script from within the repository.&quot;
    exit 1
}

try {
    $repoRoot = git rev-parse --show-toplevel 2&gt;$null
    if ($LASTEXITCODE -eq 0) {
        $hasGit = $true
    } else {
        throw &quot;Git not available&quot;
    }
} catch {
    $repoRoot = $fallbackRoot
    $hasGit = $false
}

Set-Location $repoRoot

$specsDir = Join-Path $repoRoot &apos;specs&apos;
New-Item -ItemType Directory -Path $specsDir -Force | Out-Null

$highest = 0
if (Test-Path $specsDir) {
    Get-ChildItem -Path $specsDir -Directory | ForEach-Object {
        if ($_.Name -match &apos;^(\d{3})&apos;) {
            $num = [int]$matches[1]
            if ($num -gt $highest) { $highest = $num }
        }
    }
}
$next = $highest + 1
$featureNum = (&apos;{0:000}&apos; -f $next)

$branchName = $featureDesc.ToLower() -replace &apos;[^a-z0-9]&apos;, &apos;-&apos; -replace &apos;-{2,}&apos;, &apos;-&apos; -replace &apos;^-&apos;, &apos;&apos; -replace &apos;-$&apos;, &apos;&apos;
$words = ($branchName -split &apos;-&apos;) | Where-Object { $_ } | Select-Object -First 3
$branchName = &quot;$featureNum-$([string]::Join(&apos;-&apos;, $words))&quot;

if ($hasGit) {
    try {
        git checkout -b $branchName | Out-Null
    } catch {
        Write-Warning &quot;Failed to create git branch: $branchName&quot;
    }
} else {
    Write-Warning &quot;[specify] Warning: Git repository not detected; skipped branch creation for $branchName&quot;
}

$featureDir = Join-Path $specsDir $branchName
New-Item -ItemType Directory -Path $featureDir -Force | Out-Null

$template = Join-Path $repoRoot &apos;.specify/templates/spec-template.md&apos;
$specFile = Join-Path $featureDir &apos;spec.md&apos;
if (Test-Path $template) { 
    Copy-Item $template $specFile -Force 
} else { 
    New-Item -ItemType File -Path $specFile | Out-Null 
}

# Set the SPECIFY_FEATURE environment variable for the current session
$env:SPECIFY_FEATURE = $branchName

if ($Json) {
    $obj = [PSCustomObject]@{ 
        BRANCH_NAME = $branchName
        SPEC_FILE = $specFile
        FEATURE_NUM = $featureNum
        HAS_GIT = $hasGit
    }
    $obj | ConvertTo-Json -Compress
} else {
    Write-Output &quot;BRANCH_NAME: $branchName&quot;
    Write-Output &quot;SPEC_FILE: $specFile&quot;
    Write-Output &quot;FEATURE_NUM: $featureNum&quot;
    Write-Output &quot;HAS_GIT: $hasGit&quot;
    Write-Output &quot;SPECIFY_FEATURE environment variable set to: $branchName&quot;
}</file><file path="scripts/powershell/setup-plan.ps1">#!/usr/bin/env pwsh
# Setup implementation plan for a feature

[CmdletBinding()]
param(
    [switch]$Json,
    [switch]$Help
)

$ErrorActionPreference = &apos;Stop&apos;

# Show help if requested
if ($Help) {
    Write-Output &quot;Usage: ./setup-plan.ps1 [-Json] [-Help]&quot;
    Write-Output &quot;  -Json     Output results in JSON format&quot;
    Write-Output &quot;  -Help     Show this help message&quot;
    exit 0
}

# Load common functions
. &quot;$PSScriptRoot/common.ps1&quot;

# Get all paths and variables from common functions
$paths = Get-FeaturePathsEnv

# Check if we&apos;re on a proper feature branch (only for git repos)
if (-not (Test-FeatureBranch -Branch $paths.CURRENT_BRANCH -HasGit $paths.HAS_GIT)) { 
    exit 1 
}

# Ensure the feature directory exists
New-Item -ItemType Directory -Path $paths.FEATURE_DIR -Force | Out-Null

# Copy plan template if it exists, otherwise note it or create empty file
$template = Join-Path $paths.REPO_ROOT &apos;.specify/templates/plan-template.md&apos;
if (Test-Path $template) { 
    Copy-Item $template $paths.IMPL_PLAN -Force
    Write-Output &quot;Copied plan template to $($paths.IMPL_PLAN)&quot;
} else {
    Write-Warning &quot;Plan template not found at $template&quot;
    # Create a basic plan file if template doesn&apos;t exist
    New-Item -ItemType File -Path $paths.IMPL_PLAN -Force | Out-Null
}

# Output results
if ($Json) {
    $result = [PSCustomObject]@{ 
        FEATURE_SPEC = $paths.FEATURE_SPEC
        IMPL_PLAN = $paths.IMPL_PLAN
        SPECS_DIR = $paths.FEATURE_DIR
        BRANCH = $paths.CURRENT_BRANCH
        HAS_GIT = $paths.HAS_GIT
    }
    $result | ConvertTo-Json -Compress
} else {
    Write-Output &quot;FEATURE_SPEC: $($paths.FEATURE_SPEC)&quot;
    Write-Output &quot;IMPL_PLAN: $($paths.IMPL_PLAN)&quot;
    Write-Output &quot;SPECS_DIR: $($paths.FEATURE_DIR)&quot;
    Write-Output &quot;BRANCH: $($paths.CURRENT_BRANCH)&quot;
    Write-Output &quot;HAS_GIT: $($paths.HAS_GIT)&quot;
}</file><file path="scripts/powershell/update-agent-context.ps1">#!/usr/bin/env pwsh
&lt;#!
.SYNOPSIS
Update agent context files with information from plan.md (PowerShell version)

.DESCRIPTION
Mirrors the behavior of scripts/bash/update-agent-context.sh:
 1. Environment Validation
 2. Plan Data Extraction
 3. Agent File Management (create from template or update existing)
 4. Content Generation (technology stack, recent changes, timestamp)
 5. Multi-Agent Support (claude, gemini, copilot, cursor, qwen, opencode, codex, windsurf)

.PARAMETER AgentType
Optional agent key to update a single agent. If omitted, updates all existing agent files (creating a default Claude file if none exist).

.EXAMPLE
./update-agent-context.ps1 -AgentType claude

.EXAMPLE
./update-agent-context.ps1   # Updates all existing agent files

.NOTES
Relies on common helper functions in common.ps1
#&gt;
param(
    [Parameter(Position=0)]
    [ValidateSet(&apos;claude&apos;,&apos;gemini&apos;,&apos;copilot&apos;,&apos;cursor&apos;,&apos;qwen&apos;,&apos;opencode&apos;,&apos;codex&apos;,&apos;windsurf&apos;,&apos;kilocode&apos;,&apos;auggie&apos;,&apos;roo&apos;)]
    [string]$AgentType
)

$ErrorActionPreference = &apos;Stop&apos;

# Import common helpers
$ScriptDir = Split-Path -Parent $MyInvocation.MyCommand.Path
. (Join-Path $ScriptDir &apos;common.ps1&apos;)

# Acquire environment paths
$envData = Get-FeaturePathsEnv
$REPO_ROOT     = $envData.REPO_ROOT
$CURRENT_BRANCH = $envData.CURRENT_BRANCH
$HAS_GIT       = $envData.HAS_GIT
$IMPL_PLAN     = $envData.IMPL_PLAN
$NEW_PLAN = $IMPL_PLAN

# Agent file paths
$CLAUDE_FILE   = Join-Path $REPO_ROOT &apos;CLAUDE.md&apos;
$GEMINI_FILE   = Join-Path $REPO_ROOT &apos;GEMINI.md&apos;
$COPILOT_FILE  = Join-Path $REPO_ROOT &apos;.github/copilot-instructions.md&apos;
$CURSOR_FILE   = Join-Path $REPO_ROOT &apos;.cursor/rules/specify-rules.mdc&apos;
$QWEN_FILE     = Join-Path $REPO_ROOT &apos;QWEN.md&apos;
$AGENTS_FILE   = Join-Path $REPO_ROOT &apos;AGENTS.md&apos;
$WINDSURF_FILE = Join-Path $REPO_ROOT &apos;.windsurf/rules/specify-rules.md&apos;
$KILOCODE_FILE = Join-Path $REPO_ROOT &apos;.kilocode/rules/specify-rules.md&apos;
$AUGGIE_FILE   = Join-Path $REPO_ROOT &apos;.augment/rules/specify-rules.md&apos;
$ROO_FILE      = Join-Path $REPO_ROOT &apos;.roo/rules/specify-rules.md&apos;

$TEMPLATE_FILE = Join-Path $REPO_ROOT &apos;.specify/templates/agent-file-template.md&apos;

# Parsed plan data placeholders
$script:NEW_LANG = &apos;&apos;
$script:NEW_FRAMEWORK = &apos;&apos;
$script:NEW_DB = &apos;&apos;
$script:NEW_PROJECT_TYPE = &apos;&apos;

function Write-Info { 
    param(
        [Parameter(Mandatory=$true)]
        [string]$Message
    )
    Write-Host &quot;INFO: $Message&quot; 
}

function Write-Success { 
    param(
        [Parameter(Mandatory=$true)]
        [string]$Message
    )
    Write-Host &quot;$([char]0x2713) $Message&quot; 
}

function Write-WarningMsg { 
    param(
        [Parameter(Mandatory=$true)]
        [string]$Message
    )
    Write-Warning $Message 
}

function Write-Err { 
    param(
        [Parameter(Mandatory=$true)]
        [string]$Message
    )
    Write-Host &quot;ERROR: $Message&quot; -ForegroundColor Red 
}

function Validate-Environment {
    if (-not $CURRENT_BRANCH) {
        Write-Err &apos;Unable to determine current feature&apos;
        if ($HAS_GIT) { Write-Info &quot;Make sure you&apos;re on a feature branch&quot; } else { Write-Info &apos;Set SPECIFY_FEATURE environment variable or create a feature first&apos; }
        exit 1
    }
    if (-not (Test-Path $NEW_PLAN)) {
        Write-Err &quot;No plan.md found at $NEW_PLAN&quot;
        Write-Info &apos;Ensure you are working on a feature with a corresponding spec directory&apos;
        if (-not $HAS_GIT) { Write-Info &apos;Use: $env:SPECIFY_FEATURE=your-feature-name or create a new feature first&apos; }
        exit 1
    }
    if (-not (Test-Path $TEMPLATE_FILE)) {
        Write-Err &quot;Template file not found at $TEMPLATE_FILE&quot;
        Write-Info &apos;Run specify init to scaffold .specify/templates, or add agent-file-template.md there.&apos;
        exit 1
    }
}

function Extract-PlanField {
    param(
        [Parameter(Mandatory=$true)]
        [string]$FieldPattern,
        [Parameter(Mandatory=$true)]
        [string]$PlanFile
    )
    if (-not (Test-Path $PlanFile)) { return &apos;&apos; }
    # Lines like **Language/Version**: Python 3.12
    $regex = &quot;^\*\*$([Regex]::Escape($FieldPattern))\*\*: (.+)$&quot;
    Get-Content -LiteralPath $PlanFile -Encoding utf8 | ForEach-Object {
        if ($_ -match $regex) { 
            $val = $Matches[1].Trim()
            if ($val -notin @(&apos;NEEDS CLARIFICATION&apos;,&apos;N/A&apos;)) { return $val }
        }
    } | Select-Object -First 1
}

function Parse-PlanData {
    param(
        [Parameter(Mandatory=$true)]
        [string]$PlanFile
    )
    if (-not (Test-Path $PlanFile)) { Write-Err &quot;Plan file not found: $PlanFile&quot;; return $false }
    Write-Info &quot;Parsing plan data from $PlanFile&quot;
    $script:NEW_LANG        = Extract-PlanField -FieldPattern &apos;Language/Version&apos; -PlanFile $PlanFile
    $script:NEW_FRAMEWORK   = Extract-PlanField -FieldPattern &apos;Primary Dependencies&apos; -PlanFile $PlanFile
    $script:NEW_DB          = Extract-PlanField -FieldPattern &apos;Storage&apos; -PlanFile $PlanFile
    $script:NEW_PROJECT_TYPE = Extract-PlanField -FieldPattern &apos;Project Type&apos; -PlanFile $PlanFile

    if ($NEW_LANG) { Write-Info &quot;Found language: $NEW_LANG&quot; } else { Write-WarningMsg &apos;No language information found in plan&apos; }
    if ($NEW_FRAMEWORK) { Write-Info &quot;Found framework: $NEW_FRAMEWORK&quot; }
    if ($NEW_DB -and $NEW_DB -ne &apos;N/A&apos;) { Write-Info &quot;Found database: $NEW_DB&quot; }
    if ($NEW_PROJECT_TYPE) { Write-Info &quot;Found project type: $NEW_PROJECT_TYPE&quot; }
    return $true
}

function Format-TechnologyStack {
    param(
        [Parameter(Mandatory=$false)]
        [string]$Lang,
        [Parameter(Mandatory=$false)]
        [string]$Framework
    )
    $parts = @()
    if ($Lang -and $Lang -ne &apos;NEEDS CLARIFICATION&apos;) { $parts += $Lang }
    if ($Framework -and $Framework -notin @(&apos;NEEDS CLARIFICATION&apos;,&apos;N/A&apos;)) { $parts += $Framework }
    if (-not $parts) { return &apos;&apos; }
    return ($parts -join &apos; + &apos;)
}

function Get-ProjectStructure { 
    param(
        [Parameter(Mandatory=$false)]
        [string]$ProjectType
    )
    if ($ProjectType -match &apos;web&apos;) { return &quot;backend/`nfrontend/`ntests/&quot; } else { return &quot;src/`ntests/&quot; } 
}

function Get-CommandsForLanguage { 
    param(
        [Parameter(Mandatory=$false)]
        [string]$Lang
    )
    switch -Regex ($Lang) {
        &apos;Python&apos; { return &quot;cd src; pytest; ruff check .&quot; }
        &apos;Rust&apos; { return &quot;cargo test; cargo clippy&quot; }
        &apos;JavaScript|TypeScript&apos; { return &quot;npm test; npm run lint&quot; }
        default { return &quot;# Add commands for $Lang&quot; }
    }
}

function Get-LanguageConventions { 
    param(
        [Parameter(Mandatory=$false)]
        [string]$Lang
    )
    if ($Lang) { &quot;${Lang}: Follow standard conventions&quot; } else { &apos;General: Follow standard conventions&apos; } 
}

function New-AgentFile {
    param(
        [Parameter(Mandatory=$true)]
        [string]$TargetFile,
        [Parameter(Mandatory=$true)]
        [string]$ProjectName,
        [Parameter(Mandatory=$true)]
        [datetime]$Date
    )
    if (-not (Test-Path $TEMPLATE_FILE)) { Write-Err &quot;Template not found at $TEMPLATE_FILE&quot;; return $false }
    $temp = New-TemporaryFile
    Copy-Item -LiteralPath $TEMPLATE_FILE -Destination $temp -Force

    $projectStructure = Get-ProjectStructure -ProjectType $NEW_PROJECT_TYPE
    $commands = Get-CommandsForLanguage -Lang $NEW_LANG
    $languageConventions = Get-LanguageConventions -Lang $NEW_LANG

    $escaped_lang = $NEW_LANG
    $escaped_framework = $NEW_FRAMEWORK
    $escaped_branch = $CURRENT_BRANCH

    $content = Get-Content -LiteralPath $temp -Raw -Encoding utf8
    $content = $content -replace &apos;\[PROJECT NAME\]&apos;,$ProjectName
    $content = $content -replace &apos;\[DATE\]&apos;,$Date.ToString(&apos;yyyy-MM-dd&apos;)
    
    # Build the technology stack string safely
    $techStackForTemplate = &quot;&quot;
    if ($escaped_lang -and $escaped_framework) {
        $techStackForTemplate = &quot;- $escaped_lang + $escaped_framework ($escaped_branch)&quot;
    } elseif ($escaped_lang) {
        $techStackForTemplate = &quot;- $escaped_lang ($escaped_branch)&quot;
    } elseif ($escaped_framework) {
        $techStackForTemplate = &quot;- $escaped_framework ($escaped_branch)&quot;
    }
    
    $content = $content -replace &apos;\[EXTRACTED FROM ALL PLAN.MD FILES\]&apos;,$techStackForTemplate
    # For project structure we manually embed (keep newlines)
    $escapedStructure = [Regex]::Escape($projectStructure)
    $content = $content -replace &apos;\[ACTUAL STRUCTURE FROM PLANS\]&apos;,$escapedStructure
    # Replace escaped newlines placeholder after all replacements
    $content = $content -replace &apos;\[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES\]&apos;,$commands
    $content = $content -replace &apos;\[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE\]&apos;,$languageConventions
    
    # Build the recent changes string safely
    $recentChangesForTemplate = &quot;&quot;
    if ($escaped_lang -and $escaped_framework) {
        $recentChangesForTemplate = &quot;- ${escaped_branch}: Added ${escaped_lang} + ${escaped_framework}&quot;
    } elseif ($escaped_lang) {
        $recentChangesForTemplate = &quot;- ${escaped_branch}: Added ${escaped_lang}&quot;
    } elseif ($escaped_framework) {
        $recentChangesForTemplate = &quot;- ${escaped_branch}: Added ${escaped_framework}&quot;
    }
    
    $content = $content -replace &apos;\[LAST 3 FEATURES AND WHAT THEY ADDED\]&apos;,$recentChangesForTemplate
    # Convert literal \n sequences introduced by Escape to real newlines
    $content = $content -replace &apos;\\n&apos;,[Environment]::NewLine

    $parent = Split-Path -Parent $TargetFile
    if (-not (Test-Path $parent)) { New-Item -ItemType Directory -Path $parent | Out-Null }
    Set-Content -LiteralPath $TargetFile -Value $content -NoNewline -Encoding utf8
    Remove-Item $temp -Force
    return $true
}

function Update-ExistingAgentFile {
    param(
        [Parameter(Mandatory=$true)]
        [string]$TargetFile,
        [Parameter(Mandatory=$true)]
        [datetime]$Date
    )
    if (-not (Test-Path $TargetFile)) { return (New-AgentFile -TargetFile $TargetFile -ProjectName (Split-Path $REPO_ROOT -Leaf) -Date $Date) }

    $techStack = Format-TechnologyStack -Lang $NEW_LANG -Framework $NEW_FRAMEWORK
    $newTechEntries = @()
    if ($techStack) {
        $escapedTechStack = [Regex]::Escape($techStack)
        if (-not (Select-String -Pattern $escapedTechStack -Path $TargetFile -Quiet)) { 
            $newTechEntries += &quot;- $techStack ($CURRENT_BRANCH)&quot; 
        }
    }
    if ($NEW_DB -and $NEW_DB -notin @(&apos;N/A&apos;,&apos;NEEDS CLARIFICATION&apos;)) {
        $escapedDB = [Regex]::Escape($NEW_DB)
        if (-not (Select-String -Pattern $escapedDB -Path $TargetFile -Quiet)) { 
            $newTechEntries += &quot;- $NEW_DB ($CURRENT_BRANCH)&quot; 
        }
    }
    $newChangeEntry = &apos;&apos;
    if ($techStack) { $newChangeEntry = &quot;- ${CURRENT_BRANCH}: Added ${techStack}&quot; }
    elseif ($NEW_DB -and $NEW_DB -notin @(&apos;N/A&apos;,&apos;NEEDS CLARIFICATION&apos;)) { $newChangeEntry = &quot;- ${CURRENT_BRANCH}: Added ${NEW_DB}&quot; }

    $lines = Get-Content -LiteralPath $TargetFile -Encoding utf8
    $output = New-Object System.Collections.Generic.List[string]
    $inTech = $false; $inChanges = $false; $techAdded = $false; $changeAdded = $false; $existingChanges = 0

    for ($i=0; $i -lt $lines.Count; $i++) {
        $line = $lines[$i]
        if ($line -eq &apos;## Active Technologies&apos;) {
            $output.Add($line)
            $inTech = $true
            continue
        }
        if ($inTech -and $line -match &apos;^##\s&apos;) {
            if (-not $techAdded -and $newTechEntries.Count -gt 0) { $newTechEntries | ForEach-Object { $output.Add($_) }; $techAdded = $true }
            $output.Add($line); $inTech = $false; continue
        }
        if ($inTech -and [string]::IsNullOrWhiteSpace($line)) {
            if (-not $techAdded -and $newTechEntries.Count -gt 0) { $newTechEntries | ForEach-Object { $output.Add($_) }; $techAdded = $true }
            $output.Add($line); continue
        }
        if ($line -eq &apos;## Recent Changes&apos;) {
            $output.Add($line)
            if ($newChangeEntry) { $output.Add($newChangeEntry); $changeAdded = $true }
            $inChanges = $true
            continue
        }
        if ($inChanges -and $line -match &apos;^##\s&apos;) { $output.Add($line); $inChanges = $false; continue }
        if ($inChanges -and $line -match &apos;^- &apos;) {
            if ($existingChanges -lt 2) { $output.Add($line); $existingChanges++ }
            continue
        }
        if ($line -match &apos;\*\*Last updated\*\*: .*\d{4}-\d{2}-\d{2}&apos;) {
            $output.Add(($line -replace &apos;\d{4}-\d{2}-\d{2}&apos;,$Date.ToString(&apos;yyyy-MM-dd&apos;)))
            continue
        }
        $output.Add($line)
    }

    # Post-loop check: if we&apos;re still in the Active Technologies section and haven&apos;t added new entries
    if ($inTech -and -not $techAdded -and $newTechEntries.Count -gt 0) {
        $newTechEntries | ForEach-Object { $output.Add($_) }
    }

    Set-Content -LiteralPath $TargetFile -Value ($output -join [Environment]::NewLine) -Encoding utf8
    return $true
}

function Update-AgentFile {
    param(
        [Parameter(Mandatory=$true)]
        [string]$TargetFile,
        [Parameter(Mandatory=$true)]
        [string]$AgentName
    )
    if (-not $TargetFile -or -not $AgentName) { Write-Err &apos;Update-AgentFile requires TargetFile and AgentName&apos;; return $false }
    Write-Info &quot;Updating $AgentName context file: $TargetFile&quot;
    $projectName = Split-Path $REPO_ROOT -Leaf
    $date = Get-Date

    $dir = Split-Path -Parent $TargetFile
    if (-not (Test-Path $dir)) { New-Item -ItemType Directory -Path $dir | Out-Null }

    if (-not (Test-Path $TargetFile)) {
        if (New-AgentFile -TargetFile $TargetFile -ProjectName $projectName -Date $date) { Write-Success &quot;Created new $AgentName context file&quot; } else { Write-Err &apos;Failed to create new agent file&apos;; return $false }
    } else {
        try {
            if (Update-ExistingAgentFile -TargetFile $TargetFile -Date $date) { Write-Success &quot;Updated existing $AgentName context file&quot; } else { Write-Err &apos;Failed to update agent file&apos;; return $false }
        } catch {
            Write-Err &quot;Cannot access or update existing file: $TargetFile. $_&quot;
            return $false
        }
    }
    return $true
}

function Update-SpecificAgent {
    param(
        [Parameter(Mandatory=$true)]
        [string]$Type
    )
    switch ($Type) {
        &apos;claude&apos;   { Update-AgentFile -TargetFile $CLAUDE_FILE   -AgentName &apos;Claude Code&apos; }
        &apos;gemini&apos;   { Update-AgentFile -TargetFile $GEMINI_FILE   -AgentName &apos;Gemini CLI&apos; }
        &apos;copilot&apos;  { Update-AgentFile -TargetFile $COPILOT_FILE  -AgentName &apos;GitHub Copilot&apos; }
        &apos;cursor&apos;   { Update-AgentFile -TargetFile $CURSOR_FILE   -AgentName &apos;Cursor IDE&apos; }
        &apos;qwen&apos;     { Update-AgentFile -TargetFile $QWEN_FILE     -AgentName &apos;Qwen Code&apos; }
        &apos;opencode&apos; { Update-AgentFile -TargetFile $AGENTS_FILE   -AgentName &apos;opencode&apos; }
        &apos;codex&apos;    { Update-AgentFile -TargetFile $AGENTS_FILE   -AgentName &apos;Codex CLI&apos; }
        &apos;windsurf&apos; { Update-AgentFile -TargetFile $WINDSURF_FILE -AgentName &apos;Windsurf&apos; }
        &apos;kilocode&apos; { Update-AgentFile -TargetFile $KILOCODE_FILE -AgentName &apos;Kilo Code&apos; }
        &apos;auggie&apos;   { Update-AgentFile -TargetFile $AUGGIE_FILE   -AgentName &apos;Auggie CLI&apos; }
        &apos;roo&apos;      { Update-AgentFile -TargetFile $ROO_FILE      -AgentName &apos;Roo Code&apos; }
        default { Write-Err &quot;Unknown agent type &apos;$Type&apos;&quot;; Write-Err &apos;Expected: claude|gemini|copilot|cursor|qwen|opencode|codex|windsurf|kilocode|auggie|roo&apos;; return $false }
    }
}

function Update-AllExistingAgents {
    $found = $false
    $ok = $true
    if (Test-Path $CLAUDE_FILE)   { if (-not (Update-AgentFile -TargetFile $CLAUDE_FILE   -AgentName &apos;Claude Code&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $GEMINI_FILE)   { if (-not (Update-AgentFile -TargetFile $GEMINI_FILE   -AgentName &apos;Gemini CLI&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $COPILOT_FILE)  { if (-not (Update-AgentFile -TargetFile $COPILOT_FILE  -AgentName &apos;GitHub Copilot&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $CURSOR_FILE)   { if (-not (Update-AgentFile -TargetFile $CURSOR_FILE   -AgentName &apos;Cursor IDE&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $QWEN_FILE)     { if (-not (Update-AgentFile -TargetFile $QWEN_FILE     -AgentName &apos;Qwen Code&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $AGENTS_FILE)   { if (-not (Update-AgentFile -TargetFile $AGENTS_FILE   -AgentName &apos;Codex/opencode&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $WINDSURF_FILE) { if (-not (Update-AgentFile -TargetFile $WINDSURF_FILE -AgentName &apos;Windsurf&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $KILOCODE_FILE) { if (-not (Update-AgentFile -TargetFile $KILOCODE_FILE -AgentName &apos;Kilo Code&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $AUGGIE_FILE)   { if (-not (Update-AgentFile -TargetFile $AUGGIE_FILE   -AgentName &apos;Auggie CLI&apos;)) { $ok = $false }; $found = $true }
    if (Test-Path $ROO_FILE)      { if (-not (Update-AgentFile -TargetFile $ROO_FILE      -AgentName &apos;Roo Code&apos;)) { $ok = $false }; $found = $true }
    if (-not $found) {
        Write-Info &apos;No existing agent files found, creating default Claude file...&apos;
        if (-not (Update-AgentFile -TargetFile $CLAUDE_FILE -AgentName &apos;Claude Code&apos;)) { $ok = $false }
    }
    return $ok
}

function Print-Summary {
    Write-Host &apos;&apos;
    Write-Info &apos;Summary of changes:&apos;
    if ($NEW_LANG) { Write-Host &quot;  - Added language: $NEW_LANG&quot; }
    if ($NEW_FRAMEWORK) { Write-Host &quot;  - Added framework: $NEW_FRAMEWORK&quot; }
    if ($NEW_DB -and $NEW_DB -ne &apos;N/A&apos;) { Write-Host &quot;  - Added database: $NEW_DB&quot; }
    Write-Host &apos;&apos;
    Write-Info &apos;Usage: ./update-agent-context.ps1 [-AgentType claude|gemini|copilot|cursor|qwen|opencode|codex|windsurf|kilocode|auggie|roo]&apos;
}

function Main {
    Validate-Environment
    Write-Info &quot;=== Updating agent context files for feature $CURRENT_BRANCH ===&quot;
    if (-not (Parse-PlanData -PlanFile $NEW_PLAN)) { Write-Err &apos;Failed to parse plan data&apos;; exit 1 }
    $success = $true
    if ($AgentType) {
        Write-Info &quot;Updating specific agent: $AgentType&quot;
        if (-not (Update-SpecificAgent -Type $AgentType)) { $success = $false }
    }
    else {
        Write-Info &apos;No agent specified, updating all existing agent files...&apos;
        if (-not (Update-AllExistingAgents)) { $success = $false }
    }
    Print-Summary
    if ($success) { Write-Success &apos;Agent context update completed successfully&apos;; exit 0 } else { Write-Err &apos;Agent context update completed with errors&apos;; exit 1 }
}

Main</file><file path="src/specify_cli/__init__.py">#!/usr/bin/env python3
# /// script
# requires-python = &quot;&gt;=3.11&quot;
# dependencies = [
#     &quot;typer&quot;,
#     &quot;rich&quot;,
#     &quot;platformdirs&quot;,
#     &quot;readchar&quot;,
#     &quot;httpx&quot;,
# ]
# ///
&quot;&quot;&quot;
Specify CLI - Setup tool for Specify projects

Usage:
    uvx specify-cli.py init &lt;project-name&gt;
    uvx specify-cli.py init .
    uvx specify-cli.py init --here

Or install globally:
    uv tool install --from specify-cli.py specify-cli
    specify init &lt;project-name&gt;
    specify init .
    specify init --here
&quot;&quot;&quot;

import os
import subprocess
import sys
import zipfile
import tempfile
import shutil
import shlex
import json
from pathlib import Path
from typing import Optional, Tuple

import typer
import httpx
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.text import Text
from rich.live import Live
from rich.align import Align
from rich.table import Table
from rich.tree import Tree
from typer.core import TyperGroup

# For cross-platform keyboard input
import readchar
import ssl
import truststore

ssl_context = truststore.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
client = httpx.Client(verify=ssl_context)

def _github_token(cli_token: str | None = None) -&gt; str | None:
    &quot;&quot;&quot;Return sanitized GitHub token (cli arg takes precedence) or None.&quot;&quot;&quot;
    return ((cli_token or os.getenv(&quot;GH_TOKEN&quot;) or os.getenv(&quot;GITHUB_TOKEN&quot;) or &quot;&quot;).strip()) or None

def _github_auth_headers(cli_token: str | None = None) -&gt; dict:
    &quot;&quot;&quot;Return Authorization header dict only when a non-empty token exists.&quot;&quot;&quot;
    token = _github_token(cli_token)
    return {&quot;Authorization&quot;: f&quot;Bearer {token}&quot;} if token else {}

# Constants
AI_CHOICES = {
    &quot;copilot&quot;: &quot;GitHub Copilot&quot;,
    &quot;claude&quot;: &quot;Claude Code&quot;,
    &quot;gemini&quot;: &quot;Gemini CLI&quot;,
    &quot;cursor&quot;: &quot;Cursor&quot;,
    &quot;qwen&quot;: &quot;Qwen Code&quot;,
    &quot;opencode&quot;: &quot;opencode&quot;,
    &quot;codex&quot;: &quot;Codex CLI&quot;,
    &quot;windsurf&quot;: &quot;Windsurf&quot;,
    &quot;kilocode&quot;: &quot;Kilo Code&quot;,
    &quot;auggie&quot;: &quot;Auggie CLI&quot;,
    &quot;roo&quot;: &quot;Roo Code&quot;,
}
# Add script type choices
SCRIPT_TYPE_CHOICES = {&quot;sh&quot;: &quot;POSIX Shell (bash/zsh)&quot;, &quot;ps&quot;: &quot;PowerShell&quot;}

# Claude CLI local installation path after migrate-installer
CLAUDE_LOCAL_PATH = Path.home() / &quot;.claude&quot; / &quot;local&quot; / &quot;claude&quot;

# ASCII Art Banner
BANNER = &quot;&quot;&quot;
███████╗██████╗ ███████╗ ██████╗██╗███████╗██╗   ██╗
██╔════╝██╔══██╗██╔════╝██╔════╝██║██╔════╝╚██╗ ██╔╝
███████╗██████╔╝█████╗  ██║     ██║█████╗   ╚████╔╝ 
╚════██║██╔═══╝ ██╔══╝  ██║     ██║██╔══╝    ╚██╔╝  
███████║██║     ███████╗╚██████╗██║██║        ██║   
╚══════╝╚═╝     ╚══════╝ ╚═════╝╚═╝╚═╝        ╚═╝   
&quot;&quot;&quot;

TAGLINE = &quot;GitHub Spec Kit - Spec-Driven Development Toolkit&quot;
class StepTracker:
    &quot;&quot;&quot;Track and render hierarchical steps without emojis, similar to Claude Code tree output.
    Supports live auto-refresh via an attached refresh callback.
    &quot;&quot;&quot;
    def __init__(self, title: str):
        self.title = title
        self.steps = []  # list of dicts: {key, label, status, detail}
        self.status_order = {&quot;pending&quot;: 0, &quot;running&quot;: 1, &quot;done&quot;: 2, &quot;error&quot;: 3, &quot;skipped&quot;: 4}
        self._refresh_cb = None  # callable to trigger UI refresh

    def attach_refresh(self, cb):
        self._refresh_cb = cb

    def add(self, key: str, label: str):
        if key not in [s[&quot;key&quot;] for s in self.steps]:
            self.steps.append({&quot;key&quot;: key, &quot;label&quot;: label, &quot;status&quot;: &quot;pending&quot;, &quot;detail&quot;: &quot;&quot;})
            self._maybe_refresh()

    def start(self, key: str, detail: str = &quot;&quot;):
        self._update(key, status=&quot;running&quot;, detail=detail)

    def complete(self, key: str, detail: str = &quot;&quot;):
        self._update(key, status=&quot;done&quot;, detail=detail)

    def error(self, key: str, detail: str = &quot;&quot;):
        self._update(key, status=&quot;error&quot;, detail=detail)

    def skip(self, key: str, detail: str = &quot;&quot;):
        self._update(key, status=&quot;skipped&quot;, detail=detail)

    def _update(self, key: str, status: str, detail: str):
        for s in self.steps:
            if s[&quot;key&quot;] == key:
                s[&quot;status&quot;] = status
                if detail:
                    s[&quot;detail&quot;] = detail
                self._maybe_refresh()
                return
        # If not present, add it
        self.steps.append({&quot;key&quot;: key, &quot;label&quot;: key, &quot;status&quot;: status, &quot;detail&quot;: detail})
        self._maybe_refresh()

    def _maybe_refresh(self):
        if self._refresh_cb:
            try:
                self._refresh_cb()
            except Exception:
                pass

    def render(self):
        tree = Tree(f&quot;[cyan]{self.title}[/cyan]&quot;, guide_style=&quot;grey50&quot;)
        for step in self.steps:
            label = step[&quot;label&quot;]
            detail_text = step[&quot;detail&quot;].strip() if step[&quot;detail&quot;] else &quot;&quot;

            # Circles (unchanged styling)
            status = step[&quot;status&quot;]
            if status == &quot;done&quot;:
                symbol = &quot;[green]●[/green]&quot;
            elif status == &quot;pending&quot;:
                symbol = &quot;[green dim]○[/green dim]&quot;
            elif status == &quot;running&quot;:
                symbol = &quot;[cyan]○[/cyan]&quot;
            elif status == &quot;error&quot;:
                symbol = &quot;[red]●[/red]&quot;
            elif status == &quot;skipped&quot;:
                symbol = &quot;[yellow]○[/yellow]&quot;
            else:
                symbol = &quot; &quot;

            if status == &quot;pending&quot;:
                # Entire line light gray (pending)
                if detail_text:
                    line = f&quot;{symbol} [bright_black]{label} ({detail_text})[/bright_black]&quot;
                else:
                    line = f&quot;{symbol} [bright_black]{label}[/bright_black]&quot;
            else:
                # Label white, detail (if any) light gray in parentheses
                if detail_text:
                    line = f&quot;{symbol} [white]{label}[/white] [bright_black]({detail_text})[/bright_black]&quot;
                else:
                    line = f&quot;{symbol} [white]{label}[/white]&quot;

            tree.add(line)
        return tree



MINI_BANNER = &quot;&quot;&quot;
╔═╗╔═╗╔═╗╔═╗╦╔═╗╦ ╦
╚═╗╠═╝║╣ ║  ║╠╣ ╚╦╝
╚═╝╩  ╚═╝╚═╝╩╚   ╩ 
&quot;&quot;&quot;

def get_key():
    &quot;&quot;&quot;Get a single keypress in a cross-platform way using readchar.&quot;&quot;&quot;
    key = readchar.readkey()
    
    # Arrow keys
    if key == readchar.key.UP or key == readchar.key.CTRL_P:
        return &apos;up&apos;
    if key == readchar.key.DOWN or key == readchar.key.CTRL_N:
        return &apos;down&apos;
    
    # Enter/Return
    if key == readchar.key.ENTER:
        return &apos;enter&apos;
    
    # Escape
    if key == readchar.key.ESC:
        return &apos;escape&apos;
        
    # Ctrl+C
    if key == readchar.key.CTRL_C:
        raise KeyboardInterrupt

    return key



def select_with_arrows(options: dict, prompt_text: str = &quot;Select an option&quot;, default_key: str = None) -&gt; str:
    &quot;&quot;&quot;
    Interactive selection using arrow keys with Rich Live display.
    
    Args:
        options: Dict with keys as option keys and values as descriptions
        prompt_text: Text to show above the options
        default_key: Default option key to start with
        
    Returns:
        Selected option key
    &quot;&quot;&quot;
    option_keys = list(options.keys())
    if default_key and default_key in option_keys:
        selected_index = option_keys.index(default_key)
    else:
        selected_index = 0
    
    selected_key = None

    def create_selection_panel():
        &quot;&quot;&quot;Create the selection panel with current selection highlighted.&quot;&quot;&quot;
        table = Table.grid(padding=(0, 2))
        table.add_column(style=&quot;cyan&quot;, justify=&quot;left&quot;, width=3)
        table.add_column(style=&quot;white&quot;, justify=&quot;left&quot;)
        
        for i, key in enumerate(option_keys):
            if i == selected_index:
                table.add_row(&quot;▶&quot;, f&quot;[cyan]{key}[/cyan] [dim]({options[key]})[/dim]&quot;)
            else:
                table.add_row(&quot; &quot;, f&quot;[cyan]{key}[/cyan] [dim]({options[key]})[/dim]&quot;)
        
        table.add_row(&quot;&quot;, &quot;&quot;)
        table.add_row(&quot;&quot;, &quot;[dim]Use ↑/↓ to navigate, Enter to select, Esc to cancel[/dim]&quot;)
        
        return Panel(
            table,
            title=f&quot;[bold]{prompt_text}[/bold]&quot;,
            border_style=&quot;cyan&quot;,
            padding=(1, 2)
        )
    
    console.print()

    def run_selection_loop():
        nonlocal selected_key, selected_index
        with Live(create_selection_panel(), console=console, transient=True, auto_refresh=False) as live:
            while True:
                try:
                    key = get_key()
                    if key == &apos;up&apos;:
                        selected_index = (selected_index - 1) % len(option_keys)
                    elif key == &apos;down&apos;:
                        selected_index = (selected_index + 1) % len(option_keys)
                    elif key == &apos;enter&apos;:
                        selected_key = option_keys[selected_index]
                        break
                    elif key == &apos;escape&apos;:
                        console.print(&quot;\n[yellow]Selection cancelled[/yellow]&quot;)
                        raise typer.Exit(1)
                    
                    live.update(create_selection_panel(), refresh=True)

                except KeyboardInterrupt:
                    console.print(&quot;\n[yellow]Selection cancelled[/yellow]&quot;)
                    raise typer.Exit(1)

    run_selection_loop()

    if selected_key is None:
        console.print(&quot;\n[red]Selection failed.[/red]&quot;)
        raise typer.Exit(1)

    # Suppress explicit selection print; tracker / later logic will report consolidated status
    return selected_key



console = Console()


class BannerGroup(TyperGroup):
    &quot;&quot;&quot;Custom group that shows banner before help.&quot;&quot;&quot;
    
    def format_help(self, ctx, formatter):
        # Show banner before help
        show_banner()
        super().format_help(ctx, formatter)


app = typer.Typer(
    name=&quot;specify&quot;,
    help=&quot;Setup tool for Specify spec-driven development projects&quot;,
    add_completion=False,
    invoke_without_command=True,
    cls=BannerGroup,
)


def show_banner():
    &quot;&quot;&quot;Display the ASCII art banner.&quot;&quot;&quot;
    # Create gradient effect with different colors
    banner_lines = BANNER.strip().split(&apos;\n&apos;)
    colors = [&quot;bright_blue&quot;, &quot;blue&quot;, &quot;cyan&quot;, &quot;bright_cyan&quot;, &quot;white&quot;, &quot;bright_white&quot;]
    
    styled_banner = Text()
    for i, line in enumerate(banner_lines):
        color = colors[i % len(colors)]
        styled_banner.append(line + &quot;\n&quot;, style=color)
    
    console.print(Align.center(styled_banner))
    console.print(Align.center(Text(TAGLINE, style=&quot;italic bright_yellow&quot;)))
    console.print()


@app.callback()
def callback(ctx: typer.Context):
    &quot;&quot;&quot;Show banner when no subcommand is provided.&quot;&quot;&quot;
    # Show banner only when no subcommand and no help flag
    # (help is handled by BannerGroup)
    if ctx.invoked_subcommand is None and &quot;--help&quot; not in sys.argv and &quot;-h&quot; not in sys.argv:
        show_banner()
        console.print(Align.center(&quot;[dim]Run &apos;specify --help&apos; for usage information[/dim]&quot;))
        console.print()


def run_command(cmd: list[str], check_return: bool = True, capture: bool = False, shell: bool = False) -&gt; Optional[str]:
    &quot;&quot;&quot;Run a shell command and optionally capture output.&quot;&quot;&quot;
    try:
        if capture:
            result = subprocess.run(cmd, check=check_return, capture_output=True, text=True, shell=shell)
            return result.stdout.strip()
        else:
            subprocess.run(cmd, check=check_return, shell=shell)
            return None
    except subprocess.CalledProcessError as e:
        if check_return:
            console.print(f&quot;[red]Error running command:[/red] {&apos; &apos;.join(cmd)}&quot;)
            console.print(f&quot;[red]Exit code:[/red] {e.returncode}&quot;)
            if hasattr(e, &apos;stderr&apos;) and e.stderr:
                console.print(f&quot;[red]Error output:[/red] {e.stderr}&quot;)
            raise
        return None


def check_tool_for_tracker(tool: str, tracker: StepTracker) -&gt; bool:
    &quot;&quot;&quot;Check if a tool is installed and update tracker.&quot;&quot;&quot;
    if shutil.which(tool):
        tracker.complete(tool, &quot;available&quot;)
        return True
    else:
        tracker.error(tool, &quot;not found&quot;)
        return False


def check_tool(tool: str, install_hint: str) -&gt; bool:
    &quot;&quot;&quot;Check if a tool is installed.&quot;&quot;&quot;
    
    # Special handling for Claude CLI after `claude migrate-installer`
    # See: https://github.com/github/spec-kit/issues/123
    # The migrate-installer command REMOVES the original executable from PATH
    # and creates an alias at ~/.claude/local/claude instead
    # This path should be prioritized over other claude executables in PATH
    if tool == &quot;claude&quot;:
        if CLAUDE_LOCAL_PATH.exists() and CLAUDE_LOCAL_PATH.is_file():
            return True
    
    if shutil.which(tool):
        return True
    else:
        return False


def is_git_repo(path: Path = None) -&gt; bool:
    &quot;&quot;&quot;Check if the specified path is inside a git repository.&quot;&quot;&quot;
    if path is None:
        path = Path.cwd()
    
    if not path.is_dir():
        return False

    try:
        # Use git command to check if inside a work tree
        subprocess.run(
            [&quot;git&quot;, &quot;rev-parse&quot;, &quot;--is-inside-work-tree&quot;],
            check=True,
            capture_output=True,
            cwd=path,
        )
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def init_git_repo(project_path: Path, quiet: bool = False) -&gt; bool:
    &quot;&quot;&quot;Initialize a git repository in the specified path.
    quiet: if True suppress console output (tracker handles status)
    &quot;&quot;&quot;
    try:
        original_cwd = Path.cwd()
        os.chdir(project_path)
        if not quiet:
            console.print(&quot;[cyan]Initializing git repository...[/cyan]&quot;)
        subprocess.run([&quot;git&quot;, &quot;init&quot;], check=True, capture_output=True)
        subprocess.run([&quot;git&quot;, &quot;add&quot;, &quot;.&quot;], check=True, capture_output=True)
        subprocess.run([&quot;git&quot;, &quot;commit&quot;, &quot;-m&quot;, &quot;Initial commit from Specify template&quot;], check=True, capture_output=True)
        if not quiet:
            console.print(&quot;[green]✓[/green] Git repository initialized&quot;)
        return True
        
    except subprocess.CalledProcessError as e:
        if not quiet:
            console.print(f&quot;[red]Error initializing git repository:[/red] {e}&quot;)
        return False
    finally:
        os.chdir(original_cwd)


def download_template_from_github(ai_assistant: str, download_dir: Path, *, script_type: str = &quot;sh&quot;, verbose: bool = True, show_progress: bool = True, client: httpx.Client = None, debug: bool = False, github_token: str = None) -&gt; Tuple[Path, dict]:
    repo_owner = &quot;github&quot;
    repo_name = &quot;spec-kit&quot;
    if client is None:
        client = httpx.Client(verify=ssl_context)
    
    if verbose:
        console.print(&quot;[cyan]Fetching latest release information...[/cyan]&quot;)
    api_url = f&quot;https://api.github.com/repos/{repo_owner}/{repo_name}/releases/latest&quot;
    
    try:
        response = client.get(
            api_url,
            timeout=30,
            follow_redirects=True,
            headers=_github_auth_headers(github_token),
        )
        status = response.status_code
        if status != 200:
            msg = f&quot;GitHub API returned {status} for {api_url}&quot;
            if debug:
                msg += f&quot;\nResponse headers: {response.headers}\nBody (truncated 500): {response.text[:500]}&quot;
            raise RuntimeError(msg)
        try:
            release_data = response.json()
        except ValueError as je:
            raise RuntimeError(f&quot;Failed to parse release JSON: {je}\nRaw (truncated 400): {response.text[:400]}&quot;)
    except Exception as e:
        console.print(f&quot;[red]Error fetching release information[/red]&quot;)
        console.print(Panel(str(e), title=&quot;Fetch Error&quot;, border_style=&quot;red&quot;))
        raise typer.Exit(1)
    
    # Find the template asset for the specified AI assistant
    assets = release_data.get(&quot;assets&quot;, [])
    pattern = f&quot;spec-kit-template-{ai_assistant}-{script_type}&quot;
    matching_assets = [
        asset for asset in assets
        if pattern in asset[&quot;name&quot;] and asset[&quot;name&quot;].endswith(&quot;.zip&quot;)
    ]

    asset = matching_assets[0] if matching_assets else None

    if asset is None:
        console.print(f&quot;[red]No matching release asset found[/red] for [bold]{ai_assistant}[/bold] (expected pattern: [bold]{pattern}[/bold])&quot;)
        asset_names = [a.get(&apos;name&apos;, &apos;?&apos;) for a in assets]
        console.print(Panel(&quot;\n&quot;.join(asset_names) or &quot;(no assets)&quot;, title=&quot;Available Assets&quot;, border_style=&quot;yellow&quot;))
        raise typer.Exit(1)

    download_url = asset[&quot;browser_download_url&quot;]
    filename = asset[&quot;name&quot;]
    file_size = asset[&quot;size&quot;]
    
    if verbose:
        console.print(f&quot;[cyan]Found template:[/cyan] {filename}&quot;)
        console.print(f&quot;[cyan]Size:[/cyan] {file_size:,} bytes&quot;)
        console.print(f&quot;[cyan]Release:[/cyan] {release_data[&apos;tag_name&apos;]}&quot;)

    zip_path = download_dir / filename
    if verbose:
        console.print(f&quot;[cyan]Downloading template...[/cyan]&quot;)
    
    try:
        with client.stream(
            &quot;GET&quot;,
            download_url,
            timeout=60,
            follow_redirects=True,
            headers=_github_auth_headers(github_token),
        ) as response:
            if response.status_code != 200:
                body_sample = response.text[:400]
                raise RuntimeError(f&quot;Download failed with {response.status_code}\nHeaders: {response.headers}\nBody (truncated): {body_sample}&quot;)
            total_size = int(response.headers.get(&apos;content-length&apos;, 0))
            with open(zip_path, &apos;wb&apos;) as f:
                if total_size == 0:
                    for chunk in response.iter_bytes(chunk_size=8192):
                        f.write(chunk)
                else:
                    if show_progress:
                        with Progress(
                            SpinnerColumn(),
                            TextColumn(&quot;[progress.description]{task.description}&quot;),
                            TextColumn(&quot;[progress.percentage]{task.percentage:&gt;3.0f}%&quot;),
                            console=console,
                        ) as progress:
                            task = progress.add_task(&quot;Downloading...&quot;, total=total_size)
                            downloaded = 0
                            for chunk in response.iter_bytes(chunk_size=8192):
                                f.write(chunk)
                                downloaded += len(chunk)
                                progress.update(task, completed=downloaded)
                    else:
                        for chunk in response.iter_bytes(chunk_size=8192):
                            f.write(chunk)
    except Exception as e:
        console.print(f&quot;[red]Error downloading template[/red]&quot;)
        detail = str(e)
        if zip_path.exists():
            zip_path.unlink()
        console.print(Panel(detail, title=&quot;Download Error&quot;, border_style=&quot;red&quot;))
        raise typer.Exit(1)
    if verbose:
        console.print(f&quot;Downloaded: {filename}&quot;)
    metadata = {
        &quot;filename&quot;: filename,
        &quot;size&quot;: file_size,
        &quot;release&quot;: release_data[&quot;tag_name&quot;],
        &quot;asset_url&quot;: download_url
    }
    return zip_path, metadata


def download_and_extract_template(project_path: Path, ai_assistant: str, script_type: str, is_current_dir: bool = False, *, verbose: bool = True, tracker: StepTracker | None = None, client: httpx.Client = None, debug: bool = False, github_token: str = None) -&gt; Path:
    &quot;&quot;&quot;Download the latest release and extract it to create a new project.
    Returns project_path. Uses tracker if provided (with keys: fetch, download, extract, cleanup)
    &quot;&quot;&quot;
    current_dir = Path.cwd()
    
    # Step: fetch + download combined
    if tracker:
        tracker.start(&quot;fetch&quot;, &quot;contacting GitHub API&quot;)
    try:
        zip_path, meta = download_template_from_github(
            ai_assistant,
            current_dir,
            script_type=script_type,
            verbose=verbose and tracker is None,
            show_progress=(tracker is None),
            client=client,
            debug=debug,
            github_token=github_token
        )
        if tracker:
            tracker.complete(&quot;fetch&quot;, f&quot;release {meta[&apos;release&apos;]} ({meta[&apos;size&apos;]:,} bytes)&quot;)
            tracker.add(&quot;download&quot;, &quot;Download template&quot;)
            tracker.complete(&quot;download&quot;, meta[&apos;filename&apos;])
    except Exception as e:
        if tracker:
            tracker.error(&quot;fetch&quot;, str(e))
        else:
            if verbose:
                console.print(f&quot;[red]Error downloading template:[/red] {e}&quot;)
        raise
    
    if tracker:
        tracker.add(&quot;extract&quot;, &quot;Extract template&quot;)
        tracker.start(&quot;extract&quot;)
    elif verbose:
        console.print(&quot;Extracting template...&quot;)
    
    try:
        # Create project directory only if not using current directory
        if not is_current_dir:
            project_path.mkdir(parents=True)
        
        with zipfile.ZipFile(zip_path, &apos;r&apos;) as zip_ref:
            # List all files in the ZIP for debugging
            zip_contents = zip_ref.namelist()
            if tracker:
                tracker.start(&quot;zip-list&quot;)
                tracker.complete(&quot;zip-list&quot;, f&quot;{len(zip_contents)} entries&quot;)
            elif verbose:
                console.print(f&quot;[cyan]ZIP contains {len(zip_contents)} items[/cyan]&quot;)
            
            # For current directory, extract to a temp location first
            if is_current_dir:
                with tempfile.TemporaryDirectory() as temp_dir:
                    temp_path = Path(temp_dir)
                    zip_ref.extractall(temp_path)
                    
                    # Check what was extracted
                    extracted_items = list(temp_path.iterdir())
                    if tracker:
                        tracker.start(&quot;extracted-summary&quot;)
                        tracker.complete(&quot;extracted-summary&quot;, f&quot;temp {len(extracted_items)} items&quot;)
                    elif verbose:
                        console.print(f&quot;[cyan]Extracted {len(extracted_items)} items to temp location[/cyan]&quot;)
                    
                    # Handle GitHub-style ZIP with a single root directory
                    source_dir = temp_path
                    if len(extracted_items) == 1 and extracted_items[0].is_dir():
                        source_dir = extracted_items[0]
                        if tracker:
                            tracker.add(&quot;flatten&quot;, &quot;Flatten nested directory&quot;)
                            tracker.complete(&quot;flatten&quot;)
                        elif verbose:
                            console.print(f&quot;[cyan]Found nested directory structure[/cyan]&quot;)
                    
                    # Copy contents to current directory
                    for item in source_dir.iterdir():
                        dest_path = project_path / item.name
                        if item.is_dir():
                            if dest_path.exists():
                                if verbose and not tracker:
                                    console.print(f&quot;[yellow]Merging directory:[/yellow] {item.name}&quot;)
                                # Recursively copy directory contents
                                for sub_item in item.rglob(&apos;*&apos;):
                                    if sub_item.is_file():
                                        rel_path = sub_item.relative_to(item)
                                        dest_file = dest_path / rel_path
                                        dest_file.parent.mkdir(parents=True, exist_ok=True)
                                        shutil.copy2(sub_item, dest_file)
                            else:
                                shutil.copytree(item, dest_path)
                        else:
                            if dest_path.exists() and verbose and not tracker:
                                console.print(f&quot;[yellow]Overwriting file:[/yellow] {item.name}&quot;)
                            shutil.copy2(item, dest_path)
                    if verbose and not tracker:
                        console.print(f&quot;[cyan]Template files merged into current directory[/cyan]&quot;)
            else:
                # Extract directly to project directory (original behavior)
                zip_ref.extractall(project_path)
                
                # Check what was extracted
                extracted_items = list(project_path.iterdir())
                if tracker:
                    tracker.start(&quot;extracted-summary&quot;)
                    tracker.complete(&quot;extracted-summary&quot;, f&quot;{len(extracted_items)} top-level items&quot;)
                elif verbose:
                    console.print(f&quot;[cyan]Extracted {len(extracted_items)} items to {project_path}:[/cyan]&quot;)
                    for item in extracted_items:
                        console.print(f&quot;  - {item.name} ({&apos;dir&apos; if item.is_dir() else &apos;file&apos;})&quot;)
                
                # Handle GitHub-style ZIP with a single root directory
                if len(extracted_items) == 1 and extracted_items[0].is_dir():
                    # Move contents up one level
                    nested_dir = extracted_items[0]
                    temp_move_dir = project_path.parent / f&quot;{project_path.name}_temp&quot;
                    # Move the nested directory contents to temp location
                    shutil.move(str(nested_dir), str(temp_move_dir))
                    # Remove the now-empty project directory
                    project_path.rmdir()
                    # Rename temp directory to project directory
                    shutil.move(str(temp_move_dir), str(project_path))
                    if tracker:
                        tracker.add(&quot;flatten&quot;, &quot;Flatten nested directory&quot;)
                        tracker.complete(&quot;flatten&quot;)
                    elif verbose:
                        console.print(f&quot;[cyan]Flattened nested directory structure[/cyan]&quot;)
                    
    except Exception as e:
        if tracker:
            tracker.error(&quot;extract&quot;, str(e))
        else:
            if verbose:
                console.print(f&quot;[red]Error extracting template:[/red] {e}&quot;)
                if debug:
                    console.print(Panel(str(e), title=&quot;Extraction Error&quot;, border_style=&quot;red&quot;))
        # Clean up project directory if created and not current directory
        if not is_current_dir and project_path.exists():
            shutil.rmtree(project_path)
        raise typer.Exit(1)
    else:
        if tracker:
            tracker.complete(&quot;extract&quot;)
    finally:
        if tracker:
            tracker.add(&quot;cleanup&quot;, &quot;Remove temporary archive&quot;)
        # Clean up downloaded ZIP file
        if zip_path.exists():
            zip_path.unlink()
            if tracker:
                tracker.complete(&quot;cleanup&quot;)
            elif verbose:
                console.print(f&quot;Cleaned up: {zip_path.name}&quot;)
    
    return project_path


def ensure_executable_scripts(project_path: Path, tracker: StepTracker | None = None) -&gt; None:
    &quot;&quot;&quot;Ensure POSIX .sh scripts under .specify/scripts (recursively) have execute bits (no-op on Windows).&quot;&quot;&quot;
    if os.name == &quot;nt&quot;:
        return  # Windows: skip silently
    scripts_root = project_path / &quot;.specify&quot; / &quot;scripts&quot;
    if not scripts_root.is_dir():
        return
    failures: list[str] = []
    updated = 0
    for script in scripts_root.rglob(&quot;*.sh&quot;):
        try:
            if script.is_symlink() or not script.is_file():
                continue
            try:
                with script.open(&quot;rb&quot;) as f:
                    if f.read(2) != b&quot;#!&quot;:
                        continue
            except Exception:
                continue
            st = script.stat(); mode = st.st_mode
            if mode &amp; 0o111:
                continue
            new_mode = mode
            if mode &amp; 0o400: new_mode |= 0o100
            if mode &amp; 0o040: new_mode |= 0o010
            if mode &amp; 0o004: new_mode |= 0o001
            if not (new_mode &amp; 0o100):
                new_mode |= 0o100
            os.chmod(script, new_mode)
            updated += 1
        except Exception as e:
            failures.append(f&quot;{script.relative_to(scripts_root)}: {e}&quot;)
    if tracker:
        detail = f&quot;{updated} updated&quot; + (f&quot;, {len(failures)} failed&quot; if failures else &quot;&quot;)
        tracker.add(&quot;chmod&quot;, &quot;Set script permissions recursively&quot;)
        (tracker.error if failures else tracker.complete)(&quot;chmod&quot;, detail)
    else:
        if updated:
            console.print(f&quot;[cyan]Updated execute permissions on {updated} script(s) recursively[/cyan]&quot;)
        if failures:
            console.print(&quot;[yellow]Some scripts could not be updated:[/yellow]&quot;)
            for f in failures:
                console.print(f&quot;  - {f}&quot;)

@app.command()
def init(
    project_name: str = typer.Argument(None, help=&quot;Name for your new project directory (optional if using --here, or use &apos;.&apos; for current directory)&quot;),
    ai_assistant: str = typer.Option(None, &quot;--ai&quot;, help=&quot;AI assistant to use: claude, gemini, copilot, cursor, qwen, opencode, codex, windsurf, kilocode, or auggie&quot;),
    script_type: str = typer.Option(None, &quot;--script&quot;, help=&quot;Script type to use: sh or ps&quot;),
    ignore_agent_tools: bool = typer.Option(False, &quot;--ignore-agent-tools&quot;, help=&quot;Skip checks for AI agent tools like Claude Code&quot;),
    no_git: bool = typer.Option(False, &quot;--no-git&quot;, help=&quot;Skip git repository initialization&quot;),
    here: bool = typer.Option(False, &quot;--here&quot;, help=&quot;Initialize project in the current directory instead of creating a new one&quot;),
    force: bool = typer.Option(False, &quot;--force&quot;, help=&quot;Force merge/overwrite when using --here (skip confirmation)&quot;),
    skip_tls: bool = typer.Option(False, &quot;--skip-tls&quot;, help=&quot;Skip SSL/TLS verification (not recommended)&quot;),
    debug: bool = typer.Option(False, &quot;--debug&quot;, help=&quot;Show verbose diagnostic output for network and extraction failures&quot;),
    github_token: str = typer.Option(None, &quot;--github-token&quot;, help=&quot;GitHub token to use for API requests (or set GH_TOKEN or GITHUB_TOKEN environment variable)&quot;),
):
    &quot;&quot;&quot;
    Initialize a new Specify project from the latest template.
    
    This command will:
    1. Check that required tools are installed (git is optional)
    2. Let you choose your AI assistant (Claude Code, Gemini CLI, GitHub Copilot, Cursor, Qwen Code, opencode, Codex CLI, Windsurf, Kilo Code, or Auggie CLI)
    3. Download the appropriate template from GitHub
    4. Extract the template to a new project directory or current directory
    5. Initialize a fresh git repository (if not --no-git and no existing repo)
    6. Optionally set up AI assistant commands
    
    Examples:
        specify init my-project
        specify init my-project --ai claude
        specify init my-project --ai gemini
        specify init my-project --ai copilot --no-git
        specify init my-project --ai cursor
        specify init my-project --ai qwen
        specify init my-project --ai opencode
        specify init my-project --ai codex
        specify init my-project --ai windsurf
        specify init my-project --ai auggie
        specify init --ignore-agent-tools my-project
        specify init . --ai claude         # Initialize in current directory
        specify init .                     # Initialize in current directory (interactive AI selection)
        specify init --here --ai claude    # Alternative syntax for current directory
        specify init --here --ai codex
        specify init --here
        specify init --here --force  # Skip confirmation when current directory not empty
    &quot;&quot;&quot;
    # Show banner first
    show_banner()
    
    # Handle &apos;.&apos; as shorthand for current directory (equivalent to --here)
    if project_name == &quot;.&quot;:
        here = True
        project_name = None  # Clear project_name to use existing validation logic
    
    # Validate arguments
    if here and project_name:
        console.print(&quot;[red]Error:[/red] Cannot specify both project name and --here flag&quot;)
        raise typer.Exit(1)
    
    if not here and not project_name:
        console.print(&quot;[red]Error:[/red] Must specify either a project name, use &apos;.&apos; for current directory, or use --here flag&quot;)
        raise typer.Exit(1)
    
    # Determine project directory
    if here:
        project_name = Path.cwd().name
        project_path = Path.cwd()
        
        # Check if current directory has any files
        existing_items = list(project_path.iterdir())
        if existing_items:
            console.print(f&quot;[yellow]Warning:[/yellow] Current directory is not empty ({len(existing_items)} items)&quot;)
            console.print(&quot;[yellow]Template files will be merged with existing content and may overwrite existing files[/yellow]&quot;)
            if force:
                console.print(&quot;[cyan]--force supplied: skipping confirmation and proceeding with merge[/cyan]&quot;)
            else:
                # Ask for confirmation
                response = typer.confirm(&quot;Do you want to continue?&quot;)
                if not response:
                    console.print(&quot;[yellow]Operation cancelled[/yellow]&quot;)
                    raise typer.Exit(0)
    else:
        project_path = Path(project_name).resolve()
        # Check if project directory already exists
        if project_path.exists():
            error_panel = Panel(
                f&quot;Directory &apos;[cyan]{project_name}[/cyan]&apos; already exists\n&quot;
                &quot;Please choose a different project name or remove the existing directory.&quot;,
                title=&quot;[red]Directory Conflict[/red]&quot;,
                border_style=&quot;red&quot;,
                padding=(1, 2)
            )
            console.print()
            console.print(error_panel)
            raise typer.Exit(1)
    
    # Create formatted setup info with column alignment
    current_dir = Path.cwd()
    
    setup_lines = [
        &quot;[cyan]Specify Project Setup[/cyan]&quot;,
        &quot;&quot;,
        f&quot;{&apos;Project&apos;:&lt;15} [green]{project_path.name}[/green]&quot;,
        f&quot;{&apos;Working Path&apos;:&lt;15} [dim]{current_dir}[/dim]&quot;,
    ]
    
    # Add target path only if different from working dir
    if not here:
        setup_lines.append(f&quot;{&apos;Target Path&apos;:&lt;15} [dim]{project_path}[/dim]&quot;)
    
    console.print(Panel(&quot;\n&quot;.join(setup_lines), border_style=&quot;cyan&quot;, padding=(1, 2)))
    
    # Check git only if we might need it (not --no-git)
    # Only set to True if the user wants it and the tool is available
    should_init_git = False
    if not no_git:
        should_init_git = check_tool(&quot;git&quot;, &quot;https://git-scm.com/downloads&quot;)
        if not should_init_git:
            console.print(&quot;[yellow]Git not found - will skip repository initialization[/yellow]&quot;)

    # AI assistant selection
    if ai_assistant:
        if ai_assistant not in AI_CHOICES:
            console.print(f&quot;[red]Error:[/red] Invalid AI assistant &apos;{ai_assistant}&apos;. Choose from: {&apos;, &apos;.join(AI_CHOICES.keys())}&quot;)
            raise typer.Exit(1)
        selected_ai = ai_assistant
    else:
        # Use arrow-key selection interface
        selected_ai = select_with_arrows(
            AI_CHOICES, 
            &quot;Choose your AI assistant:&quot;, 
            &quot;copilot&quot;
        )
    
    # Check agent tools unless ignored
    if not ignore_agent_tools:
        agent_tool_missing = False
        install_url = &quot;&quot;
        if selected_ai == &quot;claude&quot;:
            if not check_tool(&quot;claude&quot;, &quot;https://docs.anthropic.com/en/docs/claude-code/setup&quot;):
                install_url = &quot;https://docs.anthropic.com/en/docs/claude-code/setup&quot;
                agent_tool_missing = True
        elif selected_ai == &quot;gemini&quot;:
            if not check_tool(&quot;gemini&quot;, &quot;https://github.com/google-gemini/gemini-cli&quot;):
                install_url = &quot;https://github.com/google-gemini/gemini-cli&quot;
                agent_tool_missing = True
        elif selected_ai == &quot;qwen&quot;:
            if not check_tool(&quot;qwen&quot;, &quot;https://github.com/QwenLM/qwen-code&quot;):
                install_url = &quot;https://github.com/QwenLM/qwen-code&quot;
                agent_tool_missing = True
        elif selected_ai == &quot;opencode&quot;:
            if not check_tool(&quot;opencode&quot;, &quot;https://opencode.ai&quot;):
                install_url = &quot;https://opencode.ai&quot;
                agent_tool_missing = True
        elif selected_ai == &quot;codex&quot;:
            if not check_tool(&quot;codex&quot;, &quot;https://github.com/openai/codex&quot;):
                install_url = &quot;https://github.com/openai/codex&quot;
                agent_tool_missing = True
        elif selected_ai == &quot;auggie&quot;:
            if not check_tool(&quot;auggie&quot;, &quot;https://docs.augmentcode.com/cli/setup-auggie/install-auggie-cli&quot;):
                install_url = &quot;https://docs.augmentcode.com/cli/setup-auggie/install-auggie-cli&quot;
                agent_tool_missing = True
        # GitHub Copilot and Cursor checks are not needed as they&apos;re typically available in supported IDEs

        if agent_tool_missing:
            error_panel = Panel(
                f&quot;[cyan]{selected_ai}[/cyan] not found\n&quot;
                f&quot;Install with: [cyan]{install_url}[/cyan]\n&quot;
                f&quot;{AI_CHOICES[selected_ai]} is required to continue with this project type.\n\n&quot;
                &quot;Tip: Use [cyan]--ignore-agent-tools[/cyan] to skip this check&quot;,
                title=&quot;[red]Agent Detection Error[/red]&quot;,
                border_style=&quot;red&quot;,
                padding=(1, 2)
            )
            console.print()
            console.print(error_panel)
            raise typer.Exit(1)
    
    # Determine script type (explicit, interactive, or OS default)
    if script_type:
        if script_type not in SCRIPT_TYPE_CHOICES:
            console.print(f&quot;[red]Error:[/red] Invalid script type &apos;{script_type}&apos;. Choose from: {&apos;, &apos;.join(SCRIPT_TYPE_CHOICES.keys())}&quot;)
            raise typer.Exit(1)
        selected_script = script_type
    else:
        # Auto-detect default
        default_script = &quot;ps&quot; if os.name == &quot;nt&quot; else &quot;sh&quot;
        # Provide interactive selection similar to AI if stdin is a TTY
        if sys.stdin.isatty():
            selected_script = select_with_arrows(SCRIPT_TYPE_CHOICES, &quot;Choose script type (or press Enter)&quot;, default_script)
        else:
            selected_script = default_script
    
    console.print(f&quot;[cyan]Selected AI assistant:[/cyan] {selected_ai}&quot;)
    console.print(f&quot;[cyan]Selected script type:[/cyan] {selected_script}&quot;)
    
    # Download and set up project
    # New tree-based progress (no emojis); include earlier substeps
    tracker = StepTracker(&quot;Initialize Specify Project&quot;)
    # Flag to allow suppressing legacy headings
    sys._specify_tracker_active = True
    # Pre steps recorded as completed before live rendering
    tracker.add(&quot;precheck&quot;, &quot;Check required tools&quot;)
    tracker.complete(&quot;precheck&quot;, &quot;ok&quot;)
    tracker.add(&quot;ai-select&quot;, &quot;Select AI assistant&quot;)
    tracker.complete(&quot;ai-select&quot;, f&quot;{selected_ai}&quot;)
    tracker.add(&quot;script-select&quot;, &quot;Select script type&quot;)
    tracker.complete(&quot;script-select&quot;, selected_script)
    for key, label in [
        (&quot;fetch&quot;, &quot;Fetch latest release&quot;),
        (&quot;download&quot;, &quot;Download template&quot;),
        (&quot;extract&quot;, &quot;Extract template&quot;),
        (&quot;zip-list&quot;, &quot;Archive contents&quot;),
        (&quot;extracted-summary&quot;, &quot;Extraction summary&quot;),
        (&quot;chmod&quot;, &quot;Ensure scripts executable&quot;),
        (&quot;cleanup&quot;, &quot;Cleanup&quot;),
        (&quot;git&quot;, &quot;Initialize git repository&quot;),
        (&quot;final&quot;, &quot;Finalize&quot;)
    ]:
        tracker.add(key, label)

    # Use transient so live tree is replaced by the final static render (avoids duplicate output)
    with Live(tracker.render(), console=console, refresh_per_second=8, transient=True) as live:
        tracker.attach_refresh(lambda: live.update(tracker.render()))
        try:
            # Create a httpx client with verify based on skip_tls
            verify = not skip_tls
            local_ssl_context = ssl_context if verify else False
            local_client = httpx.Client(verify=local_ssl_context)

            download_and_extract_template(project_path, selected_ai, selected_script, here, verbose=False, tracker=tracker, client=local_client, debug=debug, github_token=github_token)

            # Ensure scripts are executable (POSIX)
            ensure_executable_scripts(project_path, tracker=tracker)

            # Git step
            if not no_git:
                tracker.start(&quot;git&quot;)
                if is_git_repo(project_path):
                    tracker.complete(&quot;git&quot;, &quot;existing repo detected&quot;)
                elif should_init_git:
                    if init_git_repo(project_path, quiet=True):
                        tracker.complete(&quot;git&quot;, &quot;initialized&quot;)
                    else:
                        tracker.error(&quot;git&quot;, &quot;init failed&quot;)
                else:
                    tracker.skip(&quot;git&quot;, &quot;git not available&quot;)
            else:
                tracker.skip(&quot;git&quot;, &quot;--no-git flag&quot;)

            tracker.complete(&quot;final&quot;, &quot;project ready&quot;)
        except Exception as e:
            tracker.error(&quot;final&quot;, str(e))
            console.print(Panel(f&quot;Initialization failed: {e}&quot;, title=&quot;Failure&quot;, border_style=&quot;red&quot;))
            if debug:
                _env_pairs = [
                    (&quot;Python&quot;, sys.version.split()[0]),
                    (&quot;Platform&quot;, sys.platform),
                    (&quot;CWD&quot;, str(Path.cwd())),
                ]
                _label_width = max(len(k) for k, _ in _env_pairs)
                env_lines = [f&quot;{k.ljust(_label_width)} → [bright_black]{v}[/bright_black]&quot; for k, v in _env_pairs]
                console.print(Panel(&quot;\n&quot;.join(env_lines), title=&quot;Debug Environment&quot;, border_style=&quot;magenta&quot;))
            if not here and project_path.exists():
                shutil.rmtree(project_path)
            raise typer.Exit(1)
        finally:
            # Force final render
            pass

    # Final static tree (ensures finished state visible after Live context ends)
    console.print(tracker.render())
    console.print(&quot;\n[bold green]Project ready.[/bold green]&quot;)
    
    # Agent folder security notice
    agent_folder_map = {
        &quot;claude&quot;: &quot;.claude/&quot;,
        &quot;gemini&quot;: &quot;.gemini/&quot;,
        &quot;cursor&quot;: &quot;.cursor/&quot;,
        &quot;qwen&quot;: &quot;.qwen/&quot;,
        &quot;opencode&quot;: &quot;.opencode/&quot;,
        &quot;codex&quot;: &quot;.codex/&quot;,
        &quot;windsurf&quot;: &quot;.windsurf/&quot;,
        &quot;kilocode&quot;: &quot;.kilocode/&quot;,
        &quot;auggie&quot;: &quot;.augment/&quot;,
        &quot;copilot&quot;: &quot;.github/&quot;,
        &quot;roo&quot;: &quot;.roo/&quot;
    }
    
    if selected_ai in agent_folder_map:
        agent_folder = agent_folder_map[selected_ai]
        security_notice = Panel(
            f&quot;Some agents may store credentials, auth tokens, or other identifying and private artifacts in the agent folder within your project.\n&quot;
            f&quot;Consider adding [cyan]{agent_folder}[/cyan] (or parts of it) to [cyan].gitignore[/cyan] to prevent accidental credential leakage.&quot;,
            title=&quot;[yellow]Agent Folder Security[/yellow]&quot;,
            border_style=&quot;yellow&quot;,
            padding=(1, 2)
        )
        console.print()
        console.print(security_notice)
    
    # Boxed &quot;Next steps&quot; section
    steps_lines = []
    if not here:
        steps_lines.append(f&quot;1. Go to the project folder: [cyan]cd {project_name}[/cyan]&quot;)
        step_num = 2
    else:
        steps_lines.append(&quot;1. You&apos;re already in the project directory!&quot;)
        step_num = 2

    # Add Codex-specific setup step if needed
    if selected_ai == &quot;codex&quot;:
        codex_path = project_path / &quot;.codex&quot;
        quoted_path = shlex.quote(str(codex_path))
        if os.name == &quot;nt&quot;:  # Windows
            cmd = f&quot;setx CODEX_HOME {quoted_path}&quot;
        else:  # Unix-like systems
            cmd = f&quot;export CODEX_HOME={quoted_path}&quot;
        
        steps_lines.append(f&quot;{step_num}. Set [cyan]CODEX_HOME[/cyan] environment variable before running Codex: [cyan]{cmd}[/cyan]&quot;)
        step_num += 1

    steps_lines.append(f&quot;{step_num}. Start using slash commands with your AI agent:&quot;)

    steps_lines.append(&quot;   2.1 [cyan]/constitution[/] - Establish project principles&quot;)
    steps_lines.append(&quot;   2.2 [cyan]/specify[/] - Create baseline specification&quot;)
    steps_lines.append(&quot;   2.3 [cyan]/plan[/] - Create implementation plan&quot;)
    steps_lines.append(&quot;   2.4 [cyan]/tasks[/] - Generate actionable tasks&quot;)
    steps_lines.append(&quot;   2.5 [cyan]/implement[/] - Execute implementation&quot;)

    steps_panel = Panel(&quot;\n&quot;.join(steps_lines), title=&quot;Next Steps&quot;, border_style=&quot;cyan&quot;, padding=(1,2))
    console.print()
    console.print(steps_panel)

    enhancement_lines = [
        &quot;Optional commands that you can use for your specs [bright_black](improve quality &amp; confidence)[/bright_black]&quot;,
        &quot;&quot;,
        f&quot;○ [cyan]/clarify[/] [bright_black](optional)[/bright_black] - Ask structured questions to de-risk ambiguous areas before planning (run before [cyan]/plan[/] if used)&quot;,
        f&quot;○ [cyan]/analyze[/] [bright_black](optional)[/bright_black] - Cross-artifact consistency &amp; alignment report (after [cyan]/tasks[/], before [cyan]/implement[/])&quot;
    ]
    enhancements_panel = Panel(&quot;\n&quot;.join(enhancement_lines), title=&quot;Enhancement Commands&quot;, border_style=&quot;cyan&quot;, padding=(1,2))
    console.print()
    console.print(enhancements_panel)

    if selected_ai == &quot;codex&quot;:
        warning_text = &quot;&quot;&quot;[bold yellow]Important Note:[/bold yellow]

Custom prompts do not yet support arguments in Codex. You may need to manually specify additional project instructions directly in prompt files located in [cyan].codex/prompts/[/cyan].

For more information, see: [cyan]https://github.com/openai/codex/issues/2890[/cyan]&quot;&quot;&quot;
        
        warning_panel = Panel(warning_text, title=&quot;Slash Commands in Codex&quot;, border_style=&quot;yellow&quot;, padding=(1,2))
        console.print()
        console.print(warning_panel)

@app.command()
def check():
    &quot;&quot;&quot;Check that all required tools are installed.&quot;&quot;&quot;
    show_banner()
    console.print(&quot;[bold]Checking for installed tools...[/bold]\n&quot;)

    tracker = StepTracker(&quot;Check Available Tools&quot;)
    
    tracker.add(&quot;git&quot;, &quot;Git version control&quot;)
    tracker.add(&quot;claude&quot;, &quot;Claude Code CLI&quot;)
    tracker.add(&quot;gemini&quot;, &quot;Gemini CLI&quot;)
    tracker.add(&quot;qwen&quot;, &quot;Qwen Code CLI&quot;)
    tracker.add(&quot;code&quot;, &quot;Visual Studio Code&quot;)
    tracker.add(&quot;code-insiders&quot;, &quot;Visual Studio Code Insiders&quot;)
    tracker.add(&quot;cursor-agent&quot;, &quot;Cursor IDE agent&quot;)
    tracker.add(&quot;windsurf&quot;, &quot;Windsurf IDE&quot;)
    tracker.add(&quot;kilocode&quot;, &quot;Kilo Code IDE&quot;)
    tracker.add(&quot;opencode&quot;, &quot;opencode&quot;)
    tracker.add(&quot;codex&quot;, &quot;Codex CLI&quot;)
    tracker.add(&quot;auggie&quot;, &quot;Auggie CLI&quot;)
    
    git_ok = check_tool_for_tracker(&quot;git&quot;, tracker)
    claude_ok = check_tool_for_tracker(&quot;claude&quot;, tracker)  
    gemini_ok = check_tool_for_tracker(&quot;gemini&quot;, tracker)
    qwen_ok = check_tool_for_tracker(&quot;qwen&quot;, tracker)
    code_ok = check_tool_for_tracker(&quot;code&quot;, tracker)
    code_insiders_ok = check_tool_for_tracker(&quot;code-insiders&quot;, tracker)
    cursor_ok = check_tool_for_tracker(&quot;cursor-agent&quot;, tracker)
    windsurf_ok = check_tool_for_tracker(&quot;windsurf&quot;, tracker)
    kilocode_ok = check_tool_for_tracker(&quot;kilocode&quot;, tracker)
    opencode_ok = check_tool_for_tracker(&quot;opencode&quot;, tracker)
    codex_ok = check_tool_for_tracker(&quot;codex&quot;, tracker)
    auggie_ok = check_tool_for_tracker(&quot;auggie&quot;, tracker)

    console.print(tracker.render())

    console.print(&quot;\n[bold green]Specify CLI is ready to use![/bold green]&quot;)

    if not git_ok:
        console.print(&quot;[dim]Tip: Install git for repository management[/dim]&quot;)
    if not (claude_ok or gemini_ok or cursor_ok or qwen_ok or windsurf_ok or kilocode_ok or opencode_ok or codex_ok or auggie_ok):
        console.print(&quot;[dim]Tip: Install an AI assistant for the best experience[/dim]&quot;)


def main():
    app()


if __name__ == &quot;__main__&quot;:
    main()</file><file path="templates/commands/analyze.md">---
description: Perform a non-destructive cross-artifact consistency and quality analysis across spec.md, plan.md, and tasks.md after task generation.
scripts:
  sh: scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks
  ps: scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

Goal: Identify inconsistencies, duplications, ambiguities, and underspecified items across the three core artifacts (`spec.md`, `plan.md`, `tasks.md`) before implementation. This command MUST run only after `/tasks` has successfully produced a complete `tasks.md`.

STRICTLY READ-ONLY: Do **not** modify any files. Output a structured analysis report. Offer an optional remediation plan (user must explicitly approve before any follow-up editing commands would be invoked manually).

Constitution Authority: The project constitution (`/memory/constitution.md`) is **non-negotiable** within this analysis scope. Constitution conflicts are automatically CRITICAL and require adjustment of the spec, plan, or tasks—not dilution, reinterpretation, or silent ignoring of the principle. If a principle itself needs to change, that must occur in a separate, explicit constitution update outside `/analyze`.

Execution steps:

1. Run `{SCRIPT}` once from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS. Derive absolute paths:
   - SPEC = FEATURE_DIR/spec.md
   - PLAN = FEATURE_DIR/plan.md
   - TASKS = FEATURE_DIR/tasks.md
   Abort with an error message if any required file is missing (instruct the user to run missing prerequisite command).

2. Load artifacts:
   - Parse spec.md sections: Overview/Context, Functional Requirements, Non-Functional Requirements, User Stories, Edge Cases (if present).
   - Parse plan.md: Architecture/stack choices, Data Model references, Phases, Technical constraints.
   - Parse tasks.md: Task IDs, descriptions, phase grouping, parallel markers [P], referenced file paths.
   - Load constitution `/memory/constitution.md` for principle validation.

3. Build internal semantic models:
   - Requirements inventory: Each functional + non-functional requirement with a stable key (derive slug based on imperative phrase; e.g., &quot;User can upload file&quot; -&gt; `user-can-upload-file`).
   - User story/action inventory.
   - Task coverage mapping: Map each task to one or more requirements or stories (inference by keyword / explicit reference patterns like IDs or key phrases).
   - Constitution rule set: Extract principle names and any MUST/SHOULD normative statements.

4. Detection passes:
   A. Duplication detection:
      - Identify near-duplicate requirements. Mark lower-quality phrasing for consolidation.
   B. Ambiguity detection:
      - Flag vague adjectives (fast, scalable, secure, intuitive, robust) lacking measurable criteria.
      - Flag unresolved placeholders (TODO, TKTK, ???, &lt;placeholder&gt;, etc.).
   C. Underspecification:
      - Requirements with verbs but missing object or measurable outcome.
      - User stories missing acceptance criteria alignment.
      - Tasks referencing files or components not defined in spec/plan.
   D. Constitution alignment:
      - Any requirement or plan element conflicting with a MUST principle.
      - Missing mandated sections or quality gates from constitution.
   E. Coverage gaps:
      - Requirements with zero associated tasks.
      - Tasks with no mapped requirement/story.
      - Non-functional requirements not reflected in tasks (e.g., performance, security).
   F. Inconsistency:
      - Terminology drift (same concept named differently across files).
      - Data entities referenced in plan but absent in spec (or vice versa).
      - Task ordering contradictions (e.g., integration tasks before foundational setup tasks without dependency note).
      - Conflicting requirements (e.g., one requires to use Next.js while other says to use Vue as the framework).

5. Severity assignment heuristic:
   - CRITICAL: Violates constitution MUST, missing core spec artifact, or requirement with zero coverage that blocks baseline functionality.
   - HIGH: Duplicate or conflicting requirement, ambiguous security/performance attribute, untestable acceptance criterion.
   - MEDIUM: Terminology drift, missing non-functional task coverage, underspecified edge case.
   - LOW: Style/wording improvements, minor redundancy not affecting execution order.

6. Produce a Markdown report (no file writes) with sections:

   ### Specification Analysis Report
   | ID | Category | Severity | Location(s) | Summary | Recommendation |
   |----|----------|----------|-------------|---------|----------------|
   | A1 | Duplication | HIGH | spec.md:L120-134 | Two similar requirements ... | Merge phrasing; keep clearer version |
   (Add one row per finding; generate stable IDs prefixed by category initial.)

   Additional subsections:
   - Coverage Summary Table:
     | Requirement Key | Has Task? | Task IDs | Notes |
   - Constitution Alignment Issues (if any)
   - Unmapped Tasks (if any)
   - Metrics:
     * Total Requirements
     * Total Tasks
     * Coverage % (requirements with &gt;=1 task)
     * Ambiguity Count
     * Duplication Count
     * Critical Issues Count

7. At end of report, output a concise Next Actions block:
   - If CRITICAL issues exist: Recommend resolving before `/implement`.
   - If only LOW/MEDIUM: User may proceed, but provide improvement suggestions.
   - Provide explicit command suggestions: e.g., &quot;Run /specify with refinement&quot;, &quot;Run /plan to adjust architecture&quot;, &quot;Manually edit tasks.md to add coverage for &apos;performance-metrics&apos;&quot;.

8. Ask the user: &quot;Would you like me to suggest concrete remediation edits for the top N issues?&quot; (Do NOT apply them automatically.)

Behavior rules:
- NEVER modify files.
- NEVER hallucinate missing sections—if absent, report them.
- KEEP findings deterministic: if rerun without changes, produce consistent IDs and counts.
- LIMIT total findings in the main table to 50; aggregate remainder in a summarized overflow note.
- If zero issues found, emit a success report with coverage statistics and proceed recommendation.

Context: {ARGS}</file><file path="templates/commands/clarify.md">---
description: Identify underspecified areas in the current feature spec by asking up to 5 highly targeted clarification questions and encoding answers back into the spec.
scripts:
   sh: scripts/bash/check-prerequisites.sh --json --paths-only
   ps: scripts/powershell/check-prerequisites.ps1 -Json -PathsOnly
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

Goal: Detect and reduce ambiguity or missing decision points in the active feature specification and record the clarifications directly in the spec file.

Note: This clarification workflow is expected to run (and be completed) BEFORE invoking `/plan`. If the user explicitly states they are skipping clarification (e.g., exploratory spike), you may proceed, but must warn that downstream rework risk increases.

Execution steps:

1. Run `{SCRIPT}` from repo root **once** (combined `--json --paths-only` mode / `-Json -PathsOnly`). Parse minimal JSON payload fields:
   - `FEATURE_DIR`
   - `FEATURE_SPEC`
   - (Optionally capture `IMPL_PLAN`, `TASKS` for future chained flows.)
   - If JSON parsing fails, abort and instruct user to re-run `/specify` or verify feature branch environment.

2. Load the current spec file. Perform a structured ambiguity &amp; coverage scan using this taxonomy. For each category, mark status: Clear / Partial / Missing. Produce an internal coverage map used for prioritization (do not output raw map unless no questions will be asked).

   Functional Scope &amp; Behavior:
   - Core user goals &amp; success criteria
   - Explicit out-of-scope declarations
   - User roles / personas differentiation

   Domain &amp; Data Model:
   - Entities, attributes, relationships
   - Identity &amp; uniqueness rules
   - Lifecycle/state transitions
   - Data volume / scale assumptions

   Interaction &amp; UX Flow:
   - Critical user journeys / sequences
   - Error/empty/loading states
   - Accessibility or localization notes

   Non-Functional Quality Attributes:
   - Performance (latency, throughput targets)
   - Scalability (horizontal/vertical, limits)
   - Reliability &amp; availability (uptime, recovery expectations)
   - Observability (logging, metrics, tracing signals)
   - Security &amp; privacy (authN/Z, data protection, threat assumptions)
   - Compliance / regulatory constraints (if any)

   Integration &amp; External Dependencies:
   - External services/APIs and failure modes
   - Data import/export formats
   - Protocol/versioning assumptions

   Edge Cases &amp; Failure Handling:
   - Negative scenarios
   - Rate limiting / throttling
   - Conflict resolution (e.g., concurrent edits)

   Constraints &amp; Tradeoffs:
   - Technical constraints (language, storage, hosting)
   - Explicit tradeoffs or rejected alternatives

   Terminology &amp; Consistency:
   - Canonical glossary terms
   - Avoided synonyms / deprecated terms

   Completion Signals:
   - Acceptance criteria testability
   - Measurable Definition of Done style indicators

   Misc / Placeholders:
   - TODO markers / unresolved decisions
   - Ambiguous adjectives (&quot;robust&quot;, &quot;intuitive&quot;) lacking quantification

   For each category with Partial or Missing status, add a candidate question opportunity unless:
   - Clarification would not materially change implementation or validation strategy
   - Information is better deferred to planning phase (note internally)

3. Generate (internally) a prioritized queue of candidate clarification questions (maximum 5). Do NOT output them all at once. Apply these constraints:
    - Maximum of 5 total questions across the whole session.
    - Each question must be answerable with EITHER:
       * A short multiple‑choice selection (2–5 distinct, mutually exclusive options), OR
       * A one-word / short‑phrase answer (explicitly constrain: &quot;Answer in &lt;=5 words&quot;).
   - Only include questions whose answers materially impact architecture, data modeling, task decomposition, test design, UX behavior, operational readiness, or compliance validation.
   - Ensure category coverage balance: attempt to cover the highest impact unresolved categories first; avoid asking two low-impact questions when a single high-impact area (e.g., security posture) is unresolved.
   - Exclude questions already answered, trivial stylistic preferences, or plan-level execution details (unless blocking correctness).
   - Favor clarifications that reduce downstream rework risk or prevent misaligned acceptance tests.
   - If more than 5 categories remain unresolved, select the top 5 by (Impact * Uncertainty) heuristic.

4. Sequential questioning loop (interactive):
    - Present EXACTLY ONE question at a time.
    - For multiple‑choice questions render options as a Markdown table:

       | Option | Description |
       |--------|-------------|
       | A | &lt;Option A description&gt; |
       | B | &lt;Option B description&gt; |
       | C | &lt;Option C description&gt; | (add D/E as needed up to 5)
       | Short | Provide a different short answer (&lt;=5 words) | (Include only if free-form alternative is appropriate)

    - For short‑answer style (no meaningful discrete options), output a single line after the question: `Format: Short answer (&lt;=5 words)`.
    - After the user answers:
       * Validate the answer maps to one option or fits the &lt;=5 word constraint.
       * If ambiguous, ask for a quick disambiguation (count still belongs to same question; do not advance).
       * Once satisfactory, record it in working memory (do not yet write to disk) and move to the next queued question.
    - Stop asking further questions when:
       * All critical ambiguities resolved early (remaining queued items become unnecessary), OR
       * User signals completion (&quot;done&quot;, &quot;good&quot;, &quot;no more&quot;), OR
       * You reach 5 asked questions.
    - Never reveal future queued questions in advance.
    - If no valid questions exist at start, immediately report no critical ambiguities.

5. Integration after EACH accepted answer (incremental update approach):
    - Maintain in-memory representation of the spec (loaded once at start) plus the raw file contents.
    - For the first integrated answer in this session:
       * Ensure a `## Clarifications` section exists (create it just after the highest-level contextual/overview section per the spec template if missing).
       * Under it, create (if not present) a `### Session YYYY-MM-DD` subheading for today.
    - Append a bullet line immediately after acceptance: `- Q: &lt;question&gt; → A: &lt;final answer&gt;`.
    - Then immediately apply the clarification to the most appropriate section(s):
       * Functional ambiguity → Update or add a bullet in Functional Requirements.
       * User interaction / actor distinction → Update User Stories or Actors subsection (if present) with clarified role, constraint, or scenario.
       * Data shape / entities → Update Data Model (add fields, types, relationships) preserving ordering; note added constraints succinctly.
       * Non-functional constraint → Add/modify measurable criteria in Non-Functional / Quality Attributes section (convert vague adjective to metric or explicit target).
       * Edge case / negative flow → Add a new bullet under Edge Cases / Error Handling (or create such subsection if template provides placeholder for it).
       * Terminology conflict → Normalize term across spec; retain original only if necessary by adding `(formerly referred to as &quot;X&quot;)` once.
    - If the clarification invalidates an earlier ambiguous statement, replace that statement instead of duplicating; leave no obsolete contradictory text.
    - Save the spec file AFTER each integration to minimize risk of context loss (atomic overwrite).
    - Preserve formatting: do not reorder unrelated sections; keep heading hierarchy intact.
    - Keep each inserted clarification minimal and testable (avoid narrative drift).

6. Validation (performed after EACH write plus final pass):
   - Clarifications session contains exactly one bullet per accepted answer (no duplicates).
   - Total asked (accepted) questions ≤ 5.
   - Updated sections contain no lingering vague placeholders the new answer was meant to resolve.
   - No contradictory earlier statement remains (scan for now-invalid alternative choices removed).
   - Markdown structure valid; only allowed new headings: `## Clarifications`, `### Session YYYY-MM-DD`.
   - Terminology consistency: same canonical term used across all updated sections.

7. Write the updated spec back to `FEATURE_SPEC`.

8. Report completion (after questioning loop ends or early termination):
   - Number of questions asked &amp; answered.
   - Path to updated spec.
   - Sections touched (list names).
   - Coverage summary table listing each taxonomy category with Status: Resolved (was Partial/Missing and addressed), Deferred (exceeds question quota or better suited for planning), Clear (already sufficient), Outstanding (still Partial/Missing but low impact).
   - If any Outstanding or Deferred remain, recommend whether to proceed to `/plan` or run `/clarify` again later post-plan.
   - Suggested next command.

Behavior rules:
- If no meaningful ambiguities found (or all potential questions would be low-impact), respond: &quot;No critical ambiguities detected worth formal clarification.&quot; and suggest proceeding.
- If spec file missing, instruct user to run `/specify` first (do not create a new spec here).
- Never exceed 5 total asked questions (clarification retries for a single question do not count as new questions).
- Avoid speculative tech stack questions unless the absence blocks functional clarity.
- Respect user early termination signals (&quot;stop&quot;, &quot;done&quot;, &quot;proceed&quot;).
 - If no questions asked due to full coverage, output a compact coverage summary (all categories Clear) then suggest advancing.
 - If quota reached with unresolved high-impact categories remaining, explicitly flag them under Deferred with rationale.

Context for prioritization: {ARGS}</file><file path="templates/commands/constitution.md">---
description: Create or update the project constitution from interactive or provided principle inputs, ensuring all dependent templates stay in sync.
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

You are updating the project constitution at `/memory/constitution.md`. This file is a TEMPLATE containing placeholder tokens in square brackets (e.g. `[PROJECT_NAME]`, `[PRINCIPLE_1_NAME]`). Your job is to (a) collect/derive concrete values, (b) fill the template precisely, and (c) propagate any amendments across dependent artifacts.

Follow this execution flow:

1. Load the existing constitution template at `/memory/constitution.md`.
   - Identify every placeholder token of the form `[ALL_CAPS_IDENTIFIER]`.
   **IMPORTANT**: The user might require less or more principles than the ones used in the template. If a number is specified, respect that - follow the general template. You will update the doc accordingly.

2. Collect/derive values for placeholders:
   - If user input (conversation) supplies a value, use it.
   - Otherwise infer from existing repo context (README, docs, prior constitution versions if embedded).
   - For governance dates: `RATIFICATION_DATE` is the original adoption date (if unknown ask or mark TODO), `LAST_AMENDED_DATE` is today if changes are made, otherwise keep previous.
   - `CONSTITUTION_VERSION` must increment according to semantic versioning rules:
     * MAJOR: Backward incompatible governance/principle removals or redefinitions.
     * MINOR: New principle/section added or materially expanded guidance.
     * PATCH: Clarifications, wording, typo fixes, non-semantic refinements.
   - If version bump type ambiguous, propose reasoning before finalizing.

3. Draft the updated constitution content:
   - Replace every placeholder with concrete text (no bracketed tokens left except intentionally retained template slots that the project has chosen not to define yet—explicitly justify any left).
   - Preserve heading hierarchy and comments can be removed once replaced unless they still add clarifying guidance.
   - Ensure each Principle section: succinct name line, paragraph (or bullet list) capturing non‑negotiable rules, explicit rationale if not obvious.
   - Ensure Governance section lists amendment procedure, versioning policy, and compliance review expectations.

4. Consistency propagation checklist (convert prior checklist into active validations):
   - Read `/templates/plan-template.md` and ensure any &quot;Constitution Check&quot; or rules align with updated principles.
   - Read `/templates/spec-template.md` for scope/requirements alignment—update if constitution adds/removes mandatory sections or constraints.
   - Read `/templates/tasks-template.md` and ensure task categorization reflects new or removed principle-driven task types (e.g., observability, versioning, testing discipline).
   - Read each command file in `/templates/commands/*.md` (including this one) to verify no outdated references (agent-specific names like CLAUDE only) remain when generic guidance is required.
   - Read any runtime guidance docs (e.g., `README.md`, `docs/quickstart.md`, or agent-specific guidance files if present). Update references to principles changed.

5. Produce a Sync Impact Report (prepend as an HTML comment at top of the constitution file after update):
   - Version change: old → new
   - List of modified principles (old title → new title if renamed)
   - Added sections
   - Removed sections
   - Templates requiring updates (✅ updated / ⚠ pending) with file paths
   - Follow-up TODOs if any placeholders intentionally deferred.

6. Validation before final output:
   - No remaining unexplained bracket tokens.
   - Version line matches report.
   - Dates ISO format YYYY-MM-DD.
   - Principles are declarative, testable, and free of vague language (&quot;should&quot; → replace with MUST/SHOULD rationale where appropriate).

7. Write the completed constitution back to `/memory/constitution.md` (overwrite).

8. Output a final summary to the user with:
   - New version and bump rationale.
   - Any files flagged for manual follow-up.
   - Suggested commit message (e.g., `docs: amend constitution to vX.Y.Z (principle additions + governance update)`).

Formatting &amp; Style Requirements:
- Use Markdown headings exactly as in the template (do not demote/promote levels).
- Wrap long rationale lines to keep readability (&lt;100 chars ideally) but do not hard enforce with awkward breaks.
- Keep a single blank line between sections.
- Avoid trailing whitespace.

If the user supplies partial updates (e.g., only one principle revision), still perform validation and version decision steps.

If critical info missing (e.g., ratification date truly unknown), insert `TODO(&lt;FIELD_NAME&gt;): explanation` and include in the Sync Impact Report under deferred items.

Do not create a new template; always operate on the existing `/memory/constitution.md` file.</file><file path="templates/commands/implement.md">---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
scripts:
  sh: scripts/bash/check-prerequisites.sh --json --require-tasks --include-tasks
  ps: scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks
---

The user input can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

1. Run `{SCRIPT}` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute.

2. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

3. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

4. Execute implementation following the task plan:
   - **Phase-by-phase execution**: Complete each phase before moving to the next
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together  
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **Validation checkpoints**: Verify each phase completion before proceeding

5. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

6. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

7. Completion validation:
   - Verify all required tasks are completed
   - Check that implemented features match the original specification
   - Validate that tests pass and coverage meets requirements
   - Confirm the implementation follows the technical plan
   - Report final status with summary of completed work

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/tasks` first to regenerate the task list.</file><file path="templates/commands/plan.md">---
description: Execute the implementation planning workflow using the plan template to generate design artifacts.
scripts:
  sh: scripts/bash/setup-plan.sh --json
  ps: scripts/powershell/setup-plan.ps1 -Json
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

Given the implementation details provided as an argument, do this:

1. Run `{SCRIPT}` from the repo root and parse JSON for FEATURE_SPEC, IMPL_PLAN, SPECS_DIR, BRANCH. All future file paths must be absolute.
   - BEFORE proceeding, inspect FEATURE_SPEC for a `## Clarifications` section with at least one `Session` subheading. If missing or clearly ambiguous areas remain (vague adjectives, unresolved critical choices), PAUSE and instruct the user to run `/clarify` first to reduce rework. Only continue if: (a) Clarifications exist OR (b) an explicit user override is provided (e.g., &quot;proceed without clarification&quot;). Do not attempt to fabricate clarifications yourself.
2. Read and analyze the feature specification to understand:
   - The feature requirements and user stories
   - Functional and non-functional requirements
   - Success criteria and acceptance criteria
   - Any technical constraints or dependencies mentioned

3. Read the constitution at `/memory/constitution.md` to understand constitutional requirements.

4. Execute the implementation plan template:
   - Load `/templates/plan-template.md` (already copied to IMPL_PLAN path)
   - Set Input path to FEATURE_SPEC
   - Run the Execution Flow (main) function steps 1-9
   - The template is self-contained and executable
   - Follow error handling and gate checks as specified
   - Let the template guide artifact generation in $SPECS_DIR:
     * Phase 0 generates research.md
     * Phase 1 generates data-model.md, contracts/, quickstart.md
     * Phase 2 generates tasks.md
   - Incorporate user-provided details from arguments into Technical Context: {ARGS}
   - Update Progress Tracking as you complete each phase

5. Verify execution completed:
   - Check Progress Tracking shows all phases complete
   - Ensure all required artifacts were generated
   - Confirm no ERROR states in execution

6. Report results with branch name, file paths, and generated artifacts.

Use absolute paths with the repository root for all file operations to avoid path issues.</file><file path="templates/commands/specify.md">---
description: Create or update the feature specification from a natural language feature description.
scripts:
  sh: scripts/bash/create-new-feature.sh --json &quot;{ARGS}&quot;
  ps: scripts/powershell/create-new-feature.ps1 -Json &quot;{ARGS}&quot;
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

The text the user typed after `/specify` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `{ARGS}` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

Given that feature description, do this:

1. Run the script `{SCRIPT}` from repo root and parse its JSON output for BRANCH_NAME and SPEC_FILE. All file paths must be absolute.
  **IMPORTANT** You must only ever run this script once. The JSON is provided in the terminal as output - always refer to it to get the actual content you&apos;re looking for.
2. Load `templates/spec-template.md` to understand required sections.
3. Write the specification to SPEC_FILE using the template structure, replacing placeholders with concrete details derived from the feature description (arguments) while preserving section order and headings.
4. Report completion with branch name, spec file path, and readiness for the next phase.

Note: The script creates and checks out the new branch and initializes the spec file before writing.</file><file path="templates/commands/tasks.md">---
description: Generate an actionable, dependency-ordered tasks.md for the feature based on available design artifacts.
scripts:
  sh: scripts/bash/check-prerequisites.sh --json
  ps: scripts/powershell/check-prerequisites.ps1 -Json
---

The user input to you can be provided directly by the agent or as a command argument - you **MUST** consider it before proceeding with the prompt (if not empty).

User input:

$ARGUMENTS

1. Run `{SCRIPT}` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute.
2. Load and analyze available design documents:
   - Always read plan.md for tech stack and libraries
   - IF EXISTS: Read data-model.md for entities
   - IF EXISTS: Read contracts/ for API endpoints
   - IF EXISTS: Read research.md for technical decisions
   - IF EXISTS: Read quickstart.md for test scenarios

   Note: Not all projects have all documents. For example:
   - CLI tools might not have contracts/
   - Simple libraries might not need data-model.md
   - Generate tasks based on what&apos;s available

3. Generate tasks following the template:
   - Use `/templates/tasks-template.md` as the base
   - Replace example tasks with actual tasks based on:
     * **Setup tasks**: Project init, dependencies, linting
     * **Test tasks [P]**: One per contract, one per integration scenario
     * **Core tasks**: One per entity, service, CLI command, endpoint
     * **Integration tasks**: DB connections, middleware, logging
     * **Polish tasks [P]**: Unit tests, performance, docs

4. Task generation rules:
   - Each contract file → contract test task marked [P]
   - Each entity in data-model → model creation task marked [P]
   - Each endpoint → implementation task (not parallel if shared files)
   - Each user story → integration test marked [P]
   - Different files = can be parallel [P]
   - Same file = sequential (no [P])

5. Order tasks by dependencies:
   - Setup before everything
   - Tests before implementation (TDD)
   - Models before services
   - Services before endpoints
   - Core before integration
   - Everything before polish

6. Include parallel execution examples:
   - Group [P] tasks that can run together
   - Show actual Task agent commands

7. Create FEATURE_DIR/tasks.md with:
   - Correct feature name from implementation plan
   - Numbered tasks (T001, T002, etc.)
   - Clear file paths for each task
   - Dependency notes
   - Parallel execution guidance

Context for task generation: {ARGS}

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.</file><file path="templates/agent-file-template.md"># [PROJECT NAME] Development Guidelines

Auto-generated from all feature plans. Last updated: [DATE]

## Active Technologies
[EXTRACTED FROM ALL PLAN.MD FILES]

## Project Structure
```
[ACTUAL STRUCTURE FROM PLANS]
```

## Commands
[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES]

## Code Style
[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE]

## Recent Changes
[LAST 3 FEATURES AND WHAT THEY ADDED]

&lt;!-- MANUAL ADDITIONS START --&gt;
&lt;!-- MANUAL ADDITIONS END --&gt;</file><file path="templates/plan-template.md">---
description: &quot;Implementation plan template for feature development&quot;
scripts:
  sh: scripts/bash/update-agent-context.sh __AGENT__
  ps: scripts/powershell/update-agent-context.ps1 -AgentType __AGENT__
---

# Implementation Plan: [FEATURE]

**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

## Execution Flow (/plan command scope)
```
1. Load feature spec from Input path
   → If not found: ERROR &quot;No feature spec at {path}&quot;
2. Fill Technical Context (scan for NEEDS CLARIFICATION)
   → Detect Project Type from file system structure or context (web=frontend+backend, mobile=app+api)
   → Set Structure Decision based on project type
3. Fill the Constitution Check section based on the content of the constitution document.
4. Evaluate Constitution Check section below
   → If violations exist: Document in Complexity Tracking
   → If no justification possible: ERROR &quot;Simplify approach first&quot;
   → Update Progress Tracking: Initial Constitution Check
5. Execute Phase 0 → research.md
   → If NEEDS CLARIFICATION remain: ERROR &quot;Resolve unknowns&quot;
6. Execute Phase 1 → contracts, data-model.md, quickstart.md, agent-specific template file (e.g., `CLAUDE.md` for Claude Code, `.github/copilot-instructions.md` for GitHub Copilot, `GEMINI.md` for Gemini CLI, `QWEN.md` for Qwen Code or `AGENTS.md` for opencode).
7. Re-evaluate Constitution Check section
   → If new violations: Refactor design, return to Phase 1
   → Update Progress Tracking: Post-Design Constitution Check
8. Plan Phase 2 → Describe task generation approach (DO NOT create tasks.md)
9. STOP - Ready for /tasks command
```

**IMPORTANT**: The /plan command STOPS at step 7. Phases 2-4 are executed by other commands:
- Phase 2: /tasks command creates tasks.md
- Phase 3-4: Implementation execution (manual or via tools)

## Summary
[Extract from feature spec: primary requirement + technical approach from research]

## Technical Context
**Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  
**Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  
**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  
**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  
**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines source structure]  
**Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  
**Constraints**: [domain-specific, e.g., &lt;200ms p95, &lt;100MB memory, offline-capable or NEEDS CLARIFICATION]  
**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

## Constitution Check
*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

[Gates determined based on constitution file]

## Project Structure

### Documentation (this feature)
```
specs/[###-feature]/
├── plan.md              # This file (/plan command output)
├── research.md          # Phase 0 output (/plan command)
├── data-model.md        # Phase 1 output (/plan command)
├── quickstart.md        # Phase 1 output (/plan command)
├── contracts/           # Phase 1 output (/plan command)
└── tasks.md             # Phase 2 output (/tasks command - NOT created by /plan)
```

### Source Code (repository root)
&lt;!--
  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout
  for this feature. Delete unused options and expand the chosen structure with
  real paths (e.g., apps/admin, packages/something). The delivered plan must
  not include Option labels.
--&gt;
```
# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)
src/
├── models/
├── services/
├── cli/
└── lib/

tests/
├── contract/
├── integration/
└── unit/

# [REMOVE IF UNUSED] Option 2: Web application (when &quot;frontend&quot; + &quot;backend&quot; detected)
backend/
├── src/
│   ├── models/
│   ├── services/
│   └── api/
└── tests/

frontend/
├── src/
│   ├── components/
│   ├── pages/
│   └── services/
└── tests/

# [REMOVE IF UNUSED] Option 3: Mobile + API (when &quot;iOS/Android&quot; detected)
api/
└── [same as backend above]

ios/ or android/
└── [platform-specific structure: feature modules, UI flows, platform tests]
```

**Structure Decision**: [Document the selected structure and reference the real
directories captured above]

## Phase 0: Outline &amp; Research
1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION → research task
   - For each dependency → best practices task
   - For each integration → patterns task

2. **Generate and dispatch research agents**:
   ```
   For each unknown in Technical Context:
     Task: &quot;Research {unknown} for {feature context}&quot;
   For each technology choice:
     Task: &quot;Find best practices for {tech} in {domain}&quot;
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

## Phase 1: Design &amp; Contracts
*Prerequisites: research.md complete*

1. **Extract entities from feature spec** → `data-model.md`:
   - Entity name, fields, relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action → endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Generate contract tests** from contracts:
   - One test file per endpoint
   - Assert request/response schemas
   - Tests must fail (no implementation yet)

4. **Extract test scenarios** from user stories:
   - Each story → integration test scenario
   - Quickstart test = story validation steps

5. **Update agent file incrementally** (O(1) operation):
   - Run `{SCRIPT}`
     **IMPORTANT**: Execute it exactly as specified above. Do not add or remove any arguments.
   - If exists: Add only NEW tech from current plan
   - Preserve manual additions between markers
   - Update recent changes (keep last 3)
   - Keep under 150 lines for token efficiency
   - Output to repository root

**Output**: data-model.md, /contracts/*, failing tests, quickstart.md, agent-specific file

## Phase 2: Task Planning Approach
*This section describes what the /tasks command will do - DO NOT execute during /plan*

**Task Generation Strategy**:
- Load `.specify/templates/tasks-template.md` as base
- Generate tasks from Phase 1 design docs (contracts, data model, quickstart)
- Each contract → contract test task [P]
- Each entity → model creation task [P] 
- Each user story → integration test task
- Implementation tasks to make tests pass

**Ordering Strategy**:
- TDD order: Tests before implementation 
- Dependency order: Models before services before UI
- Mark [P] for parallel execution (independent files)

**Estimated Output**: 25-30 numbered, ordered tasks in tasks.md

**IMPORTANT**: This phase is executed by the /tasks command, NOT by /plan

## Phase 3+: Future Implementation
*These phases are beyond the scope of the /plan command*

**Phase 3**: Task execution (/tasks command creates tasks.md)  
**Phase 4**: Implementation (execute tasks.md following constitutional principles)  
**Phase 5**: Validation (run tests, execute quickstart.md, performance validation)

## Complexity Tracking
*Fill ONLY if Constitution Check has violations that must be justified*

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |


## Progress Tracking
*This checklist is updated during execution flow*

**Phase Status**:
- [ ] Phase 0: Research complete (/plan command)
- [ ] Phase 1: Design complete (/plan command)
- [ ] Phase 2: Task planning complete (/plan command - describe approach only)
- [ ] Phase 3: Tasks generated (/tasks command)
- [ ] Phase 4: Implementation complete
- [ ] Phase 5: Validation passed

**Gate Status**:
- [ ] Initial Constitution Check: PASS
- [ ] Post-Design Constitution Check: PASS
- [ ] All NEEDS CLARIFICATION resolved
- [ ] Complexity deviations documented

---
*Based on Constitution v2.1.1 - See `/memory/constitution.md`*</file><file path="templates/spec-template.md"># Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`  
**Created**: [DATE]  
**Status**: Draft  
**Input**: User description: &quot;$ARGUMENTS&quot;

## Execution Flow (main)
```
1. Parse user description from Input
   → If empty: ERROR &quot;No feature description provided&quot;
2. Extract key concepts from description
   → Identify: actors, actions, data, constraints
3. For each unclear aspect:
   → Mark with [NEEDS CLARIFICATION: specific question]
4. Fill User Scenarios &amp; Testing section
   → If no clear user flow: ERROR &quot;Cannot determine user scenarios&quot;
5. Generate Functional Requirements
   → Each requirement must be testable
   → Mark ambiguous requirements
6. Identify Key Entities (if data involved)
7. Run Review Checklist
   → If any [NEEDS CLARIFICATION]: WARN &quot;Spec has uncertainties&quot;
   → If implementation details found: ERROR &quot;Remove tech details&quot;
8. Return: SUCCESS (spec ready for planning)
```

---

## ⚡ Quick Guidelines
- ✅ Focus on WHAT users need and WHY
- ❌ Avoid HOW to implement (no tech stack, APIs, code structure)
- 👥 Written for business stakeholders, not developers

### Section Requirements
- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn&apos;t apply, remove it entirely (don&apos;t leave as &quot;N/A&quot;)

### For AI Generation
When creating this spec from a user prompt:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question] for any assumption you&apos;d need to make
2. **Don&apos;t guess**: If the prompt doesn&apos;t specify something (e.g., &quot;login system&quot; without auth method), mark it
3. **Think like a tester**: Every vague requirement should fail the &quot;testable and unambiguous&quot; checklist item
4. **Common underspecified areas**:
   - User types and permissions
   - Data retention/deletion policies  
   - Performance targets and scale
   - Error handling behaviors
   - Integration requirements
   - Security/compliance needs

---

## User Scenarios &amp; Testing *(mandatory)*

### Primary User Story
[Describe the main user journey in plain language]

### Acceptance Scenarios
1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

### Edge Cases
- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

### Functional Requirements
- **FR-001**: System MUST [specific capability, e.g., &quot;allow users to create accounts&quot;]
- **FR-002**: System MUST [specific capability, e.g., &quot;validate email addresses&quot;]  
- **FR-003**: Users MUST be able to [key interaction, e.g., &quot;reset their password&quot;]
- **FR-004**: System MUST [data requirement, e.g., &quot;persist user preferences&quot;]
- **FR-005**: System MUST [behavior, e.g., &quot;log all security events&quot;]

*Example of marking unclear requirements:*
- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*
- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

---

## Review &amp; Acceptance Checklist
*GATE: Automated checks run during main() execution*

### Content Quality
- [ ] No implementation details (languages, frameworks, APIs)
- [ ] Focused on user value and business needs
- [ ] Written for non-technical stakeholders
- [ ] All mandatory sections completed

### Requirement Completeness
- [ ] No [NEEDS CLARIFICATION] markers remain
- [ ] Requirements are testable and unambiguous  
- [ ] Success criteria are measurable
- [ ] Scope is clearly bounded
- [ ] Dependencies and assumptions identified

---

## Execution Status
*Updated by main() during processing*

- [ ] User description parsed
- [ ] Key concepts extracted
- [ ] Ambiguities marked
- [ ] User scenarios defined
- [ ] Requirements generated
- [ ] Entities identified
- [ ] Review checklist passed

---</file><file path="templates/tasks-template.md"># Tasks: [FEATURE NAME]

**Input**: Design documents from `/specs/[###-feature-name]/`
**Prerequisites**: plan.md (required), research.md, data-model.md, contracts/

## Execution Flow (main)
```
1. Load plan.md from feature directory
   → If not found: ERROR &quot;No implementation plan found&quot;
   → Extract: tech stack, libraries, structure
2. Load optional design documents:
   → data-model.md: Extract entities → model tasks
   → contracts/: Each file → contract test task
   → research.md: Extract decisions → setup tasks
3. Generate tasks by category:
   → Setup: project init, dependencies, linting
   → Tests: contract tests, integration tests
   → Core: models, services, CLI commands
   → Integration: DB, middleware, logging
   → Polish: unit tests, performance, docs
4. Apply task rules:
   → Different files = mark [P] for parallel
   → Same file = sequential (no [P])
   → Tests before implementation (TDD)
5. Number tasks sequentially (T001, T002...)
6. Generate dependency graph
7. Create parallel execution examples
8. Validate task completeness:
   → All contracts have tests?
   → All entities have models?
   → All endpoints implemented?
9. Return: SUCCESS (tasks ready for execution)
```

## Format: `[ID] [P?] Description`
- **[P]**: Can run in parallel (different files, no dependencies)
- Include exact file paths in descriptions

## Path Conventions
- **Single project**: `src/`, `tests/` at repository root
- **Web app**: `backend/src/`, `frontend/src/`
- **Mobile**: `api/src/`, `ios/src/` or `android/src/`
- Paths shown below assume single project - adjust based on plan.md structure

## Phase 3.1: Setup
- [ ] T001 Create project structure per implementation plan
- [ ] T002 Initialize [language] project with [framework] dependencies
- [ ] T003 [P] Configure linting and formatting tools

## Phase 3.2: Tests First (TDD) ⚠️ MUST COMPLETE BEFORE 3.3
**CRITICAL: These tests MUST be written and MUST FAIL before ANY implementation**
- [ ] T004 [P] Contract test POST /api/users in tests/contract/test_users_post.py
- [ ] T005 [P] Contract test GET /api/users/{id} in tests/contract/test_users_get.py
- [ ] T006 [P] Integration test user registration in tests/integration/test_registration.py
- [ ] T007 [P] Integration test auth flow in tests/integration/test_auth.py

## Phase 3.3: Core Implementation (ONLY after tests are failing)
- [ ] T008 [P] User model in src/models/user.py
- [ ] T009 [P] UserService CRUD in src/services/user_service.py
- [ ] T010 [P] CLI --create-user in src/cli/user_commands.py
- [ ] T011 POST /api/users endpoint
- [ ] T012 GET /api/users/{id} endpoint
- [ ] T013 Input validation
- [ ] T014 Error handling and logging

## Phase 3.4: Integration
- [ ] T015 Connect UserService to DB
- [ ] T016 Auth middleware
- [ ] T017 Request/response logging
- [ ] T018 CORS and security headers

## Phase 3.5: Polish
- [ ] T019 [P] Unit tests for validation in tests/unit/test_validation.py
- [ ] T020 Performance tests (&lt;200ms)
- [ ] T021 [P] Update docs/api.md
- [ ] T022 Remove duplication
- [ ] T023 Run manual-testing.md

## Dependencies
- Tests (T004-T007) before implementation (T008-T014)
- T008 blocks T009, T015
- T016 blocks T018
- Implementation before polish (T019-T023)

## Parallel Example
```
# Launch T004-T007 together:
Task: &quot;Contract test POST /api/users in tests/contract/test_users_post.py&quot;
Task: &quot;Contract test GET /api/users/{id} in tests/contract/test_users_get.py&quot;
Task: &quot;Integration test registration in tests/integration/test_registration.py&quot;
Task: &quot;Integration test auth in tests/integration/test_auth.py&quot;
```

## Notes
- [P] tasks = different files, no dependencies
- Verify tests fail before implementing
- Commit after each task
- Avoid: vague tasks, same file conflicts

## Task Generation Rules
*Applied during main() execution*

1. **From Contracts**:
   - Each contract file → contract test task [P]
   - Each endpoint → implementation task
   
2. **From Data Model**:
   - Each entity → model creation task [P]
   - Relationships → service layer tasks
   
3. **From User Stories**:
   - Each story → integration test [P]
   - Quickstart scenarios → validation tasks

4. **Ordering**:
   - Setup → Tests → Models → Services → Endpoints → Polish
   - Dependencies block parallel execution

## Validation Checklist
*GATE: Checked by main() before returning*

- [ ] All contracts have corresponding tests
- [ ] All entities have model tasks
- [ ] All tests come before implementation
- [ ] Parallel tasks truly independent
- [ ] Each task specifies exact file path
- [ ] No task modifies same file as another [P] task</file><file path=".gitignore"># Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
ENV/
env/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo
.DS_Store

# Project specific
*.log
.env
.env.local
*.lock

# Spec Kit-specific files
.genreleases/
*.zip
sdd-*/</file><file path="AGENTS.md"># AGENTS.md

## About Spec Kit and Specify

**GitHub Spec Kit** is a comprehensive toolkit for implementing Spec-Driven Development (SDD) - a methodology that emphasizes creating clear specifications before implementation. The toolkit includes templates, scripts, and workflows that guide development teams through a structured approach to building software.

**Specify CLI** is the command-line interface that bootstraps projects with the Spec Kit framework. It sets up the necessary directory structures, templates, and AI agent integrations to support the Spec-Driven Development workflow.

The toolkit supports multiple AI coding assistants, allowing teams to use their preferred tools while maintaining consistent project structure and development practices.

---

## General practices

- Any changes to `__init__.py` for the Specify CLI require a version rev in `pyproject.toml` and addition of entries to `CHANGELOG.md`.

## Adding New Agent Support

This section explains how to add support for new AI agents/assistants to the Specify CLI. Use this guide as a reference when integrating new AI tools into the Spec-Driven Development workflow.

### Overview

Specify supports multiple AI agents by generating agent-specific command files and directory structures when initializing projects. Each agent has its own conventions for:

- **Command file formats** (Markdown, TOML, etc.)
- **Directory structures** (`.claude/commands/`, `.windsurf/workflows/`, etc.)
- **Command invocation patterns** (slash commands, CLI tools, etc.)
- **Argument passing conventions** (`$ARGUMENTS`, `{{args}}`, etc.)

### Current Supported Agents

| Agent | Directory | Format | CLI Tool | Description |
|-------|-----------|---------|----------|-------------|
| **Claude Code** | `.claude/commands/` | Markdown | `claude` | Anthropic&apos;s Claude Code CLI |
| **Gemini CLI** | `.gemini/commands/` | TOML | `gemini` | Google&apos;s Gemini CLI |
| **GitHub Copilot** | `.github/prompts/` | Markdown | N/A (IDE-based) | GitHub Copilot in VS Code |
| **Cursor** | `.cursor/commands/` | Markdown | `cursor-agent` | Cursor CLI |
| **Qwen Code** | `.qwen/commands/` | TOML | `qwen` | Alibaba&apos;s Qwen Code CLI |
| **opencode** | `.opencode/command/` | Markdown | `opencode` | opencode CLI |
| **Windsurf** | `.windsurf/workflows/` | Markdown | N/A (IDE-based) | Windsurf IDE workflows |

### Step-by-Step Integration Guide

Follow these steps to add a new agent (using Windsurf as an example):

#### 1. Update AI_CHOICES Constant

Add the new agent to the `AI_CHOICES` dictionary in `src/specify_cli/__init__.py`:

```python
AI_CHOICES = {
    &quot;copilot&quot;: &quot;GitHub Copilot&quot;,
    &quot;claude&quot;: &quot;Claude Code&quot;, 
    &quot;gemini&quot;: &quot;Gemini CLI&quot;,
    &quot;cursor&quot;: &quot;Cursor&quot;,
    &quot;qwen&quot;: &quot;Qwen Code&quot;,
    &quot;opencode&quot;: &quot;opencode&quot;,
    &quot;windsurf&quot;: &quot;Windsurf&quot;  # Add new agent here
}
```

Also update the `agent_folder_map` in the same file to include the new agent&apos;s folder for the security notice:

```python
agent_folder_map = {
    &quot;claude&quot;: &quot;.claude/&quot;,
    &quot;gemini&quot;: &quot;.gemini/&quot;,
    &quot;cursor&quot;: &quot;.cursor/&quot;,
    &quot;qwen&quot;: &quot;.qwen/&quot;,
    &quot;opencode&quot;: &quot;.opencode/&quot;,
    &quot;codex&quot;: &quot;.codex/&quot;,
    &quot;windsurf&quot;: &quot;.windsurf/&quot;,  # Add new agent folder here
    &quot;kilocode&quot;: &quot;.kilocode/&quot;,
    &quot;auggie&quot;: &quot;.auggie/&quot;,
    &quot;copilot&quot;: &quot;.github/&quot;
}
```

#### 2. Update CLI Help Text

Update all help text and examples to include the new agent:

- Command option help: `--ai` parameter description
- Function docstrings and examples
- Error messages with agent lists

#### 3. Update README Documentation

Update the **Supported AI Agents** section in `README.md` to include the new agent:

- Add the new agent to the table with appropriate support level (Full/Partial)
- Include the agent&apos;s official website link
- Add any relevant notes about the agent&apos;s implementation
- Ensure the table formatting remains aligned and consistent

#### 4. Update Release Package Script

Modify `.github/workflows/scripts/create-release-packages.sh`:

##### Add to ALL_AGENTS array:
```bash
ALL_AGENTS=(claude gemini copilot cursor qwen opencode windsurf)
```

##### Add case statement for directory structure:
```bash
case $agent in
  # ... existing cases ...
  windsurf)
    mkdir -p &quot;$base_dir/.windsurf/workflows&quot;
    generate_commands windsurf md &quot;\$ARGUMENTS&quot; &quot;$base_dir/.windsurf/workflows&quot; &quot;$script&quot; ;;
esac
```

#### 4. Update GitHub Release Script

Modify `.github/workflows/scripts/create-github-release.sh` to include the new agent&apos;s packages:

```bash
gh release create &quot;$VERSION&quot; \
  # ... existing packages ...
  .genreleases/spec-kit-template-windsurf-sh-&quot;$VERSION&quot;.zip \
  .genreleases/spec-kit-template-windsurf-ps-&quot;$VERSION&quot;.zip \
  # Add new agent packages here
```

#### 5. Update Agent Context Scripts

##### Bash script (`scripts/bash/update-agent-context.sh`):

Add file variable:
```bash
WINDSURF_FILE=&quot;$REPO_ROOT/.windsurf/rules/specify-rules.md&quot;
```

Add to case statement:
```bash
case &quot;$AGENT_TYPE&quot; in
  # ... existing cases ...
  windsurf) update_agent_file &quot;$WINDSURF_FILE&quot; &quot;Windsurf&quot; ;;
  &quot;&quot;) 
    # ... existing checks ...
    [ -f &quot;$WINDSURF_FILE&quot; ] &amp;&amp; update_agent_file &quot;$WINDSURF_FILE&quot; &quot;Windsurf&quot;;
    # Update default creation condition
    ;;
esac
```

##### PowerShell script (`scripts/powershell/update-agent-context.ps1`):

Add file variable:
```powershell
$windsurfFile = Join-Path $repoRoot &apos;.windsurf/rules/specify-rules.md&apos;
```

Add to switch statement:
```powershell
switch ($AgentType) {
    # ... existing cases ...
    &apos;windsurf&apos; { Update-AgentFile $windsurfFile &apos;Windsurf&apos; }
    &apos;&apos; {
        foreach ($pair in @(
            # ... existing pairs ...
            @{file=$windsurfFile; name=&apos;Windsurf&apos;}
        )) {
            if (Test-Path $pair.file) { Update-AgentFile $pair.file $pair.name }
        }
        # Update default creation condition
    }
}
```

#### 6. Update CLI Tool Checks (Optional)

For agents that require CLI tools, add checks in the `check()` command and agent validation:

```python
# In check() command
tracker.add(&quot;windsurf&quot;, &quot;Windsurf IDE (optional)&quot;)
windsurf_ok = check_tool_for_tracker(&quot;windsurf&quot;, &quot;https://windsurf.com/&quot;, tracker)

# In init validation (only if CLI tool required)
elif selected_ai == &quot;windsurf&quot;:
    if not check_tool(&quot;windsurf&quot;, &quot;Install from: https://windsurf.com/&quot;):
        console.print(&quot;[red]Error:[/red] Windsurf CLI is required for Windsurf projects&quot;)
        agent_tool_missing = True
```

**Note**: Skip CLI checks for IDE-based agents (Copilot, Windsurf).

## Agent Categories

### CLI-Based Agents
Require a command-line tool to be installed:
- **Claude Code**: `claude` CLI
- **Gemini CLI**: `gemini` CLI  
- **Cursor**: `cursor-agent` CLI
- **Qwen Code**: `qwen` CLI
- **opencode**: `opencode` CLI

### IDE-Based Agents
Work within integrated development environments:
- **GitHub Copilot**: Built into VS Code/compatible editors
- **Windsurf**: Built into Windsurf IDE

## Command File Formats

### Markdown Format
Used by: Claude, Cursor, opencode, Windsurf

```markdown
---
description: &quot;Command description&quot;
---

Command content with {SCRIPT} and $ARGUMENTS placeholders.
```

### TOML Format
Used by: Gemini, Qwen

```toml
description = &quot;Command description&quot;

prompt = &quot;&quot;&quot;
Command content with {SCRIPT} and {{args}} placeholders.
&quot;&quot;&quot;
```

## Directory Conventions

- **CLI agents**: Usually `.&lt;agent-name&gt;/commands/`
- **IDE agents**: Follow IDE-specific patterns:
  - Copilot: `.github/prompts/`
  - Cursor: `.cursor/commands/`
  - Windsurf: `.windsurf/workflows/`

## Argument Patterns

Different agents use different argument placeholders:
- **Markdown/prompt-based**: `$ARGUMENTS`
- **TOML-based**: `{{args}}`
- **Script placeholders**: `{SCRIPT}` (replaced with actual script path)
- **Agent placeholders**: `__AGENT__` (replaced with agent name)

## Testing New Agent Integration

1. **Build test**: Run package creation script locally
2. **CLI test**: Test `specify init --ai &lt;agent&gt;` command
3. **File generation**: Verify correct directory structure and files
4. **Command validation**: Ensure generated commands work with the agent
5. **Context update**: Test agent context update scripts

## Common Pitfalls

1. **Forgetting update scripts**: Both bash and PowerShell scripts must be updated
2. **Missing CLI checks**: Only add for agents that actually have CLI tools
3. **Wrong argument format**: Use correct placeholder format for each agent type
4. **Directory naming**: Follow agent-specific conventions exactly
5. **Help text inconsistency**: Update all user-facing text consistently

## Future Considerations

When adding new agents:
- Consider the agent&apos;s native command/workflow patterns
- Ensure compatibility with the Spec-Driven Development process
- Document any special requirements or limitations
- Update this guide with lessons learned

---

*This documentation should be updated whenever new agents are added to maintain accuracy and completeness.*</file><file path="CHANGELOG.md"># Changelog

&lt;!-- markdownlint-disable MD024 --&gt;

All notable changes to the Specify CLI will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [LATEST_VERSION] - RELEASE_DATE

### Added

- Support for using `.` as a shorthand for current directory in `specify init .` command, equivalent to `--here` flag but more intuitive for users

## [0.0.17] - 2025-09-22

### Added

- New `/clarify` command template to surface up to 5 targeted clarification questions for an existing spec and persist answers into a Clarifications section in the spec.
- New `/analyze` command template providing a non-destructive cross-artifact discrepancy and alignment report (spec, clarifications, plan, tasks, constitution) inserted after `/tasks` and before `/implement`.
	- Note: Constitution rules are explicitly treated as non-negotiable; any conflict is a CRITICAL finding requiring artifact remediation, not weakening of principles.

## [0.0.16] - 2025-09-22

### Added

- `--force` flag for `init` command to bypass confirmation when using `--here` in a non-empty directory and proceed with merging/overwriting files.

## [0.0.15] - 2025-09-21

### Added

- Support for Roo Code.

## [0.0.14] - 2025-09-21

### Changed

- Error messages are now shown consistently.

## [0.0.13] - 2025-09-21

### Added

- Support for Kilo Code. Thank you [@shahrukhkhan489](https://github.com/shahrukhkhan489) with [#394](https://github.com/github/spec-kit/pull/394).
- Support for Auggie CLI. Thank you [@hungthai1401](https://github.com/hungthai1401) with [#137](https://github.com/github/spec-kit/pull/137).
- Agent folder security notice displayed after project provisioning completion, warning users that some agents may store credentials or auth tokens in their agent folders and recommending adding relevant folders to `.gitignore` to prevent accidental credential leakage.

### Changed

- Warning displayed to ensure that folks are aware that they might need to add their agent folder to `.gitignore`.
- Cleaned up the `check` command output.

## [0.0.12] - 2025-09-21

### Changed

- Added additional context for OpenAI Codex users - they need to set an additional environment variable, as described in [#417](https://github.com/github/spec-kit/issues/417).

## [0.0.11] - 2025-09-20

### Added

- Codex CLI support (thank you [@honjo-hiroaki-gtt](https://github.com/honjo-hiroaki-gtt) for the contribution in [#14](https://github.com/github/spec-kit/pull/14))
- Codex-aware context update tooling (Bash and PowerShell) so feature plans refresh `AGENTS.md` alongside existing assistants without manual edits.

## [0.0.10] - 2025-09-20

### Fixed

- Addressed [#378](https://github.com/github/spec-kit/issues/378) where a GitHub token may be attached to the request when it was empty.

## [0.0.9] - 2025-09-19

### Changed

- Improved agent selector UI with cyan highlighting for agent keys and gray parentheses for full names

## [0.0.8] - 2025-09-19

### Added

- Windsurf IDE support as additional AI assistant option (thank you [@raedkit](https://github.com/raedkit) for the work in [#151](https://github.com/github/spec-kit/pull/151))
- GitHub token support for API requests to handle corporate environments and rate limiting (contributed by [@zryfish](https://github.com/@zryfish) in [#243](https://github.com/github/spec-kit/pull/243))

### Changed

- Updated README with Windsurf examples and GitHub token usage
- Enhanced release workflow to include Windsurf templates

## [0.0.7] - 2025-09-18

### Changed

- Updated command instructions in the CLI.
- Cleaned up the code to not render agent-specific information when it&apos;s generic.


## [0.0.6] - 2025-09-17

### Added

- opencode support as additional AI assistant option

## [0.0.5] - 2025-09-17

### Added

- Qwen Code support as additional AI assistant option

## [0.0.4] - 2025-09-14

### Added

- SOCKS proxy support for corporate environments via `httpx[socks]` dependency

### Fixed

N/A

### Changed

N/A</file><file path="CODE_OF_CONDUCT.md"># Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of experience,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others&apos; private information, such as a physical or electronic
  address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at &lt;opensource@github.com&gt;. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project&apos;s leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at [http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/</file><file path="CONTRIBUTING.md">## Contributing to Spec Kit

Hi there! We&apos;re thrilled that you&apos;d like to contribute to Spec Kit. Contributions to this project are [released](https://help.github.com/articles/github-terms-of-service/#6-contributions-under-repository-license) to the public under the [project&apos;s open source license](LICENSE).

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## Prerequisites for running and testing code

These are one time installations required to be able to test your changes locally as part of the pull request (PR) submission process.

1. Install [Python 3.11+](https://www.python.org/downloads/)
1. Install [uv](https://docs.astral.sh/uv/) for package management
1. Install [Git](https://git-scm.com/downloads)
1. Have an [AI coding agent available](README.md#-supported-ai-agents)

## Submitting a pull request

&gt;[!NOTE]
&gt;If your pull request introduces a large change that materially impacts the work of the CLI or the rest of the repository (e.g., you&apos;re introducing new templates, arguments, or otherwise major changes), make sure that it was **discussed and agreed upon** by the project maintainers. Pull requests with large changes that did not have a prior conversation and agreement will be closed.

1. Fork and clone the repository
1. Configure and install the dependencies: `uv sync`
1. Make sure the CLI works on your machine: `uv run specify --help`
1. Create a new branch: `git checkout -b my-branch-name`
1. Make your change, add tests, and make sure everything still works
1. Test the CLI functionality with a sample project if relevant
1. Push to your fork and submit a pull request
1. Wait for your pull request to be reviewed and merged.

Here are a few things you can do that will increase the likelihood of your pull request being accepted:

- Follow the project&apos;s coding conventions.
- Write tests for new functionality.
- Update documentation (`README.md`, `spec-driven.md`) if your changes affect user-facing features.
- Keep your change as focused as possible. If there are multiple changes you would like to make that are not dependent upon each other, consider submitting them as separate pull requests.
- Write a [good commit message](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html).
- Test your changes with the Spec-Driven Development workflow to ensure compatibility.

## Development workflow

When working on spec-kit:

1. Test changes with the `specify` CLI commands (`/specify`, `/plan`, `/tasks`) in your coding agent of choice
2. Verify templates are working correctly in `templates/` directory
3. Test script functionality in the `scripts/` directory
4. Ensure memory files (`memory/constitution.md`) are updated if major process changes are made

## AI contributions in Spec Kit

&gt; [!IMPORTANT]
&gt;
&gt; If you are using **any kind of AI assistance** to contribute to Spec Kit,
&gt; it must be disclosed in the pull request or issue.

We welcome and encourage the use of AI tools to help improve Spec Kit! Many valuable contributions have been enhanced with AI assistance for code generation, issue detection, and feature definition.

That being said, if you are using any kind of AI assistance (e.g., agents, ChatGPT) while contributing to Spec Kit,
**this must be disclosed in the pull request or issue**, along with the extent to which AI assistance was used (e.g., documentation comments vs. code generation).

If your PR responses or comments are being generated by an AI, disclose that as well.

As an exception, trivial spacing or typo fixes don&apos;t need to be disclosed, so long as the changes are limited to small parts of the code or short phrases.

An example disclosure:

&gt; This PR was written primarily by GitHub Copilot.

Or a more detailed disclosure:

&gt; I consulted ChatGPT to understand the codebase but the solution
&gt; was fully authored manually by myself.

Failure to disclose this is first and foremost rude to the human operators on the other end of the pull request, but it also makes it difficult to
determine how much scrutiny to apply to the contribution.

In a perfect world, AI assistance would produce equal or higher quality work than any human. That isn&apos;t the world we live in today, and in most cases
where human supervision or expertise is not in the loop, it&apos;s generating code that cannot be reasonably maintained or evolved.

### What we&apos;re looking for

When submitting AI-assisted contributions, please ensure they include:

- **Clear disclosure of AI use** - You are transparent about AI use and degree to which you&apos;re using it for the contribution
- **Human understanding and testing** - You&apos;ve personally tested the changes and understand what they do
- **Clear rationale** - You can explain why the change is needed and how it fits within Spec Kit&apos;s goals  
- **Concrete evidence** - Include test cases, scenarios, or examples that demonstrate the improvement
- **Your own analysis** - Share your thoughts on the end-to-end developer experience

### What we&apos;ll close

We reserve the right to close contributions that appear to be:

- Untested changes submitted without verification
- Generic suggestions that don&apos;t address specific Spec Kit needs
- Bulk submissions that show no human review or understanding

### Guidelines for success

The key is demonstrating that you understand and have validated your proposed changes. If a maintainer can easily tell that a contribution was generated entirely by AI without human input or testing, it likely needs more work before submission.

Contributors who consistently submit low-effort AI-generated changes may be restricted from further contributions at the maintainers&apos; discretion.

Please be respectful to maintainers and disclose AI assistance.

## Resources

- [Spec-Driven Development Methodology](./spec-driven.md)
- [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)
- [Using Pull Requests](https://help.github.com/articles/about-pull-requests/)
- [GitHub Help](https://help.github.com)</file><file path="LICENSE">MIT License

Copyright GitHub, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</file><file path="pyproject.toml">[project]
name = &quot;specify-cli&quot;
version = &quot;0.0.17&quot;
description = &quot;Specify CLI, part of GitHub Spec Kit. A tool to bootstrap your projects for Spec-Driven Development (SDD).&quot;
requires-python = &quot;&gt;=3.11&quot;
dependencies = [
    &quot;typer&quot;,
    &quot;rich&quot;,
    &quot;httpx[socks]&quot;,
    &quot;platformdirs&quot;,
    &quot;readchar&quot;,
    &quot;truststore&gt;=0.10.4&quot;,
]

[project.scripts]
specify = &quot;specify_cli:main&quot;

[build-system]
requires = [&quot;hatchling&quot;]
build-backend = &quot;hatchling.build&quot;

[tool.hatch.build.targets.wheel]
packages = [&quot;src/specify_cli&quot;]</file><file path="README.md">&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;./media/logo_small.webp&quot;/&gt;
    &lt;h1&gt;🌱 Spec Kit&lt;/h1&gt;
    &lt;h3&gt;&lt;em&gt;Build high-quality software faster.&lt;/em&gt;&lt;/h3&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;strong&gt;An effort to allow organizations to focus on product scenarios rather than writing undifferentiated code with the help of Spec-Driven Development.&lt;/strong&gt;
&lt;/p&gt;

[![Release](https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg)](https://github.com/github/spec-kit/actions/workflows/release.yml)

---

## Table of Contents

- [🤔 What is Spec-Driven Development?](#-what-is-spec-driven-development)
- [⚡ Get started](#-get-started)
- [📽️ Video Overview](#️-video-overview)
- [🤖 Supported AI Agents](#-supported-ai-agents)
- [🔧 Specify CLI Reference](#-specify-cli-reference)
- [📚 Core philosophy](#-core-philosophy)
- [🌟 Development phases](#-development-phases)
- [🎯 Experimental goals](#-experimental-goals)
- [🔧 Prerequisites](#-prerequisites)
- [📖 Learn more](#-learn-more)
- [📋 Detailed process](#-detailed-process)
- [🔍 Troubleshooting](#-troubleshooting)
- [👥 Maintainers](#-maintainers)
- [💬 Support](#-support)
- [🙏 Acknowledgements](#-acknowledgements)
- [📄 License](#-license)

## 🤔 What is Spec-Driven Development?

Spec-Driven Development **flips the script** on traditional software development. For decades, code has been king — specifications were just scaffolding we built and discarded once the &quot;real work&quot; of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.

## ⚡ Get started

### 1. Install Specify

Choose your preferred installation method:

#### Option 1: Persistent Installation (Recommended)

Install once and use everywhere:

```bash
uv tool install specify-cli --from git+https://github.com/github/spec-kit.git
```

Then use the tool directly:

```bash
specify init &lt;PROJECT_NAME&gt;
specify check
```

#### Option 2: One-time Usage

Run directly without installing:

```bash
uvx --from git+https://github.com/github/spec-kit.git specify init &lt;PROJECT_NAME&gt;
```

**Benefits of persistent installation:**

- Tool stays installed and available in PATH
- No need to create shell aliases
- Better tool management with `uv tool list`, `uv tool upgrade`, `uv tool uninstall`
- Cleaner shell configuration

### 2. Establish project principles

Use the **`/constitution`** command to create your project&apos;s governing principles and development guidelines that will guide all subsequent development.

```bash
/constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements
```

### 3. Create the spec

Use the **`/specify`** command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.

```bash
/specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.
```

### 4. Create a technical implementation plan

Use the **`/plan`** command to provide your tech stack and architecture choices.

```bash
/plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.
```

### 5. Break down into tasks

Use **`/tasks`** to create an actionable task list from your implementation plan.

```bash
/tasks
```

### 6. Execute implementation

Use **`/implement`** to execute all tasks and build your feature according to the plan.

```bash
/implement
```

For detailed step-by-step instructions, see our [comprehensive guide](./spec-driven.md).

## 📽️ Video Overview

Want to see Spec Kit in action? Watch our [video overview](https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;pp=0gcJCckJAYcqIYzv)!

[![Spec Kit video header](/media/spec-kit-video-header.jpg)](https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;pp=0gcJCckJAYcqIYzv)

## 🤖 Supported AI Agents

| Agent                                                     | Support | Notes                                             |
|-----------------------------------------------------------|---------|---------------------------------------------------|
| [Claude Code](https://www.anthropic.com/claude-code)      | ✅ |                                                   |
| [GitHub Copilot](https://code.visualstudio.com/)          | ✅ |                                                   |
| [Gemini CLI](https://github.com/google-gemini/gemini-cli) | ✅ |                                                   |
| [Cursor](https://cursor.sh/)                              | ✅ |                                                   |
| [Qwen Code](https://github.com/QwenLM/qwen-code)          | ✅ |                                                   |
| [opencode](https://opencode.ai/)                          | ✅ |                                                   |
| [Windsurf](https://windsurf.com/)                         | ✅ |                                                   |
| [Kilo Code](https://github.com/Kilo-Org/kilocode)         | ✅ |                                                   |
| [Auggie CLI](https://docs.augmentcode.com/cli/overview)   | ✅ |                                                   |
| [Roo Code](https://roocode.com/)                          | ✅ |                                                   |
| [Codex CLI](https://github.com/openai/codex)              | ⚠️ | Codex [does not support](https://github.com/openai/codex/issues/2890) custom arguments for slash commands.  |

## 🔧 Specify CLI Reference

The `specify` command supports the following options:

### Commands

| Command     | Description                                                    |
|-------------|----------------------------------------------------------------|
| `init`      | Initialize a new Specify project from the latest template      |
| `check`     | Check for installed tools (`git`, `claude`, `gemini`, `code`/`code-insiders`, `cursor-agent`, `windsurf`, `qwen`, `opencode`, `codex`) |

### `specify init` Arguments &amp; Options

| Argument/Option        | Type     | Description                                                                  |
|------------------------|----------|------------------------------------------------------------------------------|
| `&lt;project-name&gt;`       | Argument | Name for your new project directory (optional if using `--here`, or use `.` for current directory) |
| `--ai`                 | Option   | AI assistant to use: `claude`, `gemini`, `copilot`, `cursor`, `qwen`, `opencode`, `codex`, `windsurf`, `kilocode`, `auggie`, or `roo` |
| `--script`             | Option   | Script variant to use: `sh` (bash/zsh) or `ps` (PowerShell)                 |
| `--ignore-agent-tools` | Flag     | Skip checks for AI agent tools like Claude Code                             |
| `--no-git`             | Flag     | Skip git repository initialization                                          |
| `--here`               | Flag     | Initialize project in the current directory instead of creating a new one   |
| `--force`              | Flag     | Force merge/overwrite when initializing in current directory (skip confirmation) |
| `--skip-tls`           | Flag     | Skip SSL/TLS verification (not recommended)                                 |
| `--debug`              | Flag     | Enable detailed debug output for troubleshooting                            |
| `--github-token`       | Option   | GitHub token for API requests (or set GH_TOKEN/GITHUB_TOKEN env variable)  |

### Examples

```bash
# Basic project initialization
specify init my-project

# Initialize with specific AI assistant
specify init my-project --ai claude

# Initialize with Cursor support
specify init my-project --ai cursor

# Initialize with Windsurf support
specify init my-project --ai windsurf

# Initialize with PowerShell scripts (Windows/cross-platform)
specify init my-project --ai copilot --script ps

# Initialize in current directory
specify init . --ai copilot
# or use the --here flag
specify init --here --ai copilot

# Force merge into current (non-empty) directory without confirmation
specify init . --force --ai copilot
# or 
specify init --here --force --ai copilot

# Skip git initialization
specify init my-project --ai gemini --no-git

# Enable debug output for troubleshooting
specify init my-project --ai claude --debug

# Use GitHub token for API requests (helpful for corporate environments)
specify init my-project --ai claude --github-token ghp_your_token_here

# Check system requirements
specify check
```

### Available Slash Commands

After running `specify init`, your AI coding agent will have access to these slash commands for structured development:

| Command         | Description                                                           |
|-----------------|-----------------------------------------------------------------------|
| `/constitution` | Create or update project governing principles and development guidelines |
| `/specify`      | Define what you want to build (requirements and user stories)        |
| `/clarify`      | Clarify underspecified areas (must be run before `/plan` unless explicitly skipped; formerly `/quizme`) |
| `/plan`         | Create technical implementation plans with your chosen tech stack     |
| `/tasks`        | Generate actionable task lists for implementation                     |
| `/analyze`      | Cross-artifact consistency &amp; coverage analysis (run after /tasks, before /implement) |
| `/implement`    | Execute all tasks to build the feature according to the plan         |

### Environment Variables

| Variable         | Description                                                                                    |
|------------------|------------------------------------------------------------------------------------------------|
| `SPECIFY_FEATURE` | Override feature detection for non-Git repositories. Set to the feature directory name (e.g., `001-photo-albums`) to work on a specific feature when not using Git branches.&lt;br/&gt;**Must be set in the context of the agent you&apos;re working with prior to using `/plan` or follow-up commands. |

## 📚 Core philosophy

Spec-Driven Development is a structured process that emphasizes:

- **Intent-driven development** where specifications define the &quot;_what_&quot; before the &quot;_how_&quot;
- **Rich specification creation** using guardrails and organizational principles
- **Multi-step refinement** rather than one-shot code generation from prompts
- **Heavy reliance** on advanced AI model capabilities for specification interpretation

## 🌟 Development phases

| Phase | Focus | Key Activities |
|-------|-------|----------------|
| **0-to-1 Development** (&quot;Greenfield&quot;) | Generate from scratch | &lt;ul&gt;&lt;li&gt;Start with high-level requirements&lt;/li&gt;&lt;li&gt;Generate specifications&lt;/li&gt;&lt;li&gt;Plan implementation steps&lt;/li&gt;&lt;li&gt;Build production-ready applications&lt;/li&gt;&lt;/ul&gt; |
| **Creative Exploration** | Parallel implementations | &lt;ul&gt;&lt;li&gt;Explore diverse solutions&lt;/li&gt;&lt;li&gt;Support multiple technology stacks &amp; architectures&lt;/li&gt;&lt;li&gt;Experiment with UX patterns&lt;/li&gt;&lt;/ul&gt; |
| **Iterative Enhancement** (&quot;Brownfield&quot;) | Brownfield modernization | &lt;ul&gt;&lt;li&gt;Add features iteratively&lt;/li&gt;&lt;li&gt;Modernize legacy systems&lt;/li&gt;&lt;li&gt;Adapt processes&lt;/li&gt;&lt;/ul&gt; |

## 🎯 Experimental goals

Our research and experimentation focus on:

### Technology independence

- Create applications using diverse technology stacks
- Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks

### Enterprise constraints

- Demonstrate mission-critical application development
- Incorporate organizational constraints (cloud providers, tech stacks, engineering practices)
- Support enterprise design systems and compliance requirements

### User-centric development

- Build applications for different user cohorts and preferences
- Support various development approaches (from vibe-coding to AI-native development)

### Creative &amp; iterative processes

- Validate the concept of parallel implementation exploration
- Provide robust iterative feature development workflows
- Extend processes to handle upgrades and modernization tasks

## 🔧 Prerequisites

- **Linux/macOS** (or WSL2 on Windows)
- AI coding agent: [Claude Code](https://www.anthropic.com/claude-code), [GitHub Copilot](https://code.visualstudio.com/), [Gemini CLI](https://github.com/google-gemini/gemini-cli), [Cursor](https://cursor.sh/), [Qwen CLI](https://github.com/QwenLM/qwen-code), [opencode](https://opencode.ai/), [Codex CLI](https://github.com/openai/codex), or [Windsurf](https://windsurf.com/)
- [uv](https://docs.astral.sh/uv/) for package management
- [Python 3.11+](https://www.python.org/downloads/)
- [Git](https://git-scm.com/downloads)

If you encounter issues with an agent, please open an issue so we can refine the integration.

## 📖 Learn more

- **[Complete Spec-Driven Development Methodology](./spec-driven.md)** - Deep dive into the full process
- **[Detailed Walkthrough](#-detailed-process)** - Step-by-step implementation guide

---

## 📋 Detailed process

&lt;details&gt;
&lt;summary&gt;Click to expand the detailed step-by-step walkthrough&lt;/summary&gt;

You can use the Specify CLI to bootstrap your project, which will bring in the required artifacts in your environment. Run:

```bash
specify init &lt;project_name&gt;
```

Or initialize in the current directory:

```bash
specify init .
# or use the --here flag
specify init --here
# Skip confirmation when the directory already has files
specify init . --force
# or
specify init --here --force
```

![Specify CLI bootstrapping a new project in the terminal](./media/specify_cli.gif)

You will be prompted to select the AI agent you are using. You can also proactively specify it directly in the terminal:

```bash
specify init &lt;project_name&gt; --ai claude
specify init &lt;project_name&gt; --ai gemini
specify init &lt;project_name&gt; --ai copilot
specify init &lt;project_name&gt; --ai cursor
specify init &lt;project_name&gt; --ai qwen
specify init &lt;project_name&gt; --ai opencode
specify init &lt;project_name&gt; --ai codex
specify init &lt;project_name&gt; --ai windsurf
# Or in current directory:
specify init . --ai claude
specify init . --ai codex
# or use --here flag
specify init --here --ai claude
specify init --here --ai codex
# Force merge into a non-empty current directory
specify init . --force --ai claude
# or
specify init --here --force --ai claude
```

The CLI will check if you have Claude Code, Gemini CLI, Cursor CLI, Qwen CLI, opencode, or Codex CLI installed. If you do not, or you prefer to get the templates without checking for the right tools, use `--ignore-agent-tools` with your command:

```bash
specify init &lt;project_name&gt; --ai claude --ignore-agent-tools
```

### **STEP 1:** Establish project principles

Go to the project folder and run your AI agent. In our example, we&apos;re using `claude`.

![Bootstrapping Claude Code environment](./media/bootstrap-claude-code.gif)

You will know that things are configured correctly if you see the `/constitution`, `/specify`, `/plan`, `/tasks`, and `/implement` commands available.

The first step should be establishing your project&apos;s governing principles using the `/constitution` command. This helps ensure consistent decision-making throughout all subsequent development phases:

```text
/constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements. Include governance for how these principles should guide technical decisions and implementation choices.
```

This step creates or updates the `.specify/memory/constitution.md` file with your project&apos;s foundational guidelines that the AI agent will reference during specification, planning, and implementation phases.

### **STEP 2:** Create project specifications

With your project principles established, you can now create the functional specifications. Use the `/specify` command and then provide the concrete requirements for the project you want to develop.

&gt;[!IMPORTANT]
&gt;Be as explicit as possible about _what_ you are trying to build and _why_. **Do not focus on the tech stack at this point**.

An example prompt:

```text
Develop Taskify, a team productivity platform. It should allow users to create projects, add team members,
assign tasks, comment and move tasks between boards in Kanban style. In this initial phase for this feature,
let&apos;s call it &quot;Create Taskify,&quot; let&apos;s have multiple users but the users will be declared ahead of time, predefined.
I want five users in two different categories, one product manager and four engineers. Let&apos;s create three
different sample projects. Let&apos;s have the standard Kanban columns for the status of each task, such as &quot;To Do,&quot;
&quot;In Progress,&quot; &quot;In Review,&quot; and &quot;Done.&quot; There will be no login for this application as this is just the very
first testing thing to ensure that our basic features are set up. For each task in the UI for a task card,
you should be able to change the current status of the task between the different columns in the Kanban work board.
You should be able to leave an unlimited number of comments for a particular card. You should be able to, from that task
card, assign one of the valid users. When you first launch Taskify, it&apos;s going to give you a list of the five users to pick
from. There will be no password required. When you click on a user, you go into the main view, which displays the list of
projects. When you click on a project, you open the Kanban board for that project. You&apos;re going to see the columns.
You&apos;ll be able to drag and drop cards back and forth between different columns. You will see any cards that are
assigned to you, the currently logged in user, in a different color from all the other ones, so you can quickly
see yours. You can edit any comments that you make, but you can&apos;t edit comments that other people made. You can
delete any comments that you made, but you can&apos;t delete comments anybody else made.
```

After this prompt is entered, you should see Claude Code kick off the planning and spec drafting process. Claude Code will also trigger some of the built-in scripts to set up the repository.

Once this step is completed, you should have a new branch created (e.g., `001-create-taskify`), as well as a new specification in the `specs/001-create-taskify` directory.

The produced specification should contain a set of user stories and functional requirements, as defined in the template.

At this stage, your project folder contents should resemble the following:

```text
└── .specify
    ├── memory
    │	 └── constitution.md
    ├── scripts
    │	 ├── check-prerequisites.sh
    │	 ├── common.sh
    │	 ├── create-new-feature.sh
    │	 ├── setup-plan.sh
    │	 └── update-claude-md.sh
    ├── specs
    │	 └── 001-create-taskify
    │	     └── spec.md
    └── templates
        ├── plan-template.md
        ├── spec-template.md
        └── tasks-template.md
```

### **STEP 3:** Functional specification clarification (required before planning)

With the baseline specification created, you can go ahead and clarify any of the requirements that were not captured properly within the first shot attempt.

You should run the structured clarification workflow **before** creating a technical plan to reduce rework downstream.

Preferred order:
1. Use `/clarify` (structured) – sequential, coverage-based questioning that records answers in a Clarifications section.
2. Optionally follow up with ad-hoc free-form refinement if something still feels vague.

If you intentionally want to skip clarification (e.g., spike or exploratory prototype), explicitly state that so the agent doesn&apos;t block on missing clarifications.

Example free-form refinement prompt (after `/clarify` if still needed):

```text
For each sample project or project that you create there should be a variable number of tasks between 5 and 15
tasks for each one randomly distributed into different states of completion. Make sure that there&apos;s at least
one task in each stage of completion.
```

You should also ask Claude Code to validate the **Review &amp; Acceptance Checklist**, checking off the things that are validated/pass the requirements, and leave the ones that are not unchecked. The following prompt can be used:

```text
Read the review and acceptance checklist, and check off each item in the checklist if the feature spec meets the criteria. Leave it empty if it does not.
```

It&apos;s important to use the interaction with Claude Code as an opportunity to clarify and ask questions around the specification - **do not treat its first attempt as final**.

### **STEP 4:** Generate a plan

You can now be specific about the tech stack and other technical requirements. You can use the `/plan` command that is built into the project template with a prompt like this:

```text
We are going to generate this using .NET Aspire, using Postgres as the database. The frontend should use
Blazor server with drag-and-drop task boards, real-time updates. There should be a REST API created with a projects API,
tasks API, and a notifications API.
```

The output of this step will include a number of implementation detail documents, with your directory tree resembling this:

```text
.
├── CLAUDE.md
├── memory
│	 └── constitution.md
├── scripts
│	 ├── check-prerequisites.sh
│	 ├── common.sh
│	 ├── create-new-feature.sh
│	 ├── setup-plan.sh
│	 └── update-claude-md.sh
├── specs
│	 └── 001-create-taskify
│	     ├── contracts
│	     │	 ├── api-spec.json
│	     │	 └── signalr-spec.md
│	     ├── data-model.md
│	     ├── plan.md
│	     ├── quickstart.md
│	     ├── research.md
│	     └── spec.md
└── templates
    ├── CLAUDE-template.md
    ├── plan-template.md
    ├── spec-template.md
    └── tasks-template.md
```

Check the `research.md` document to ensure that the right tech stack is used, based on your instructions. You can ask Claude Code to refine it if any of the components stand out, or even have it check the locally-installed version of the platform/framework you want to use (e.g., .NET).

Additionally, you might want to ask Claude Code to research details about the chosen tech stack if it&apos;s something that is rapidly changing (e.g., .NET Aspire, JS frameworks), with a prompt like this:

```text
I want you to go through the implementation plan and implementation details, looking for areas that could
benefit from additional research as .NET Aspire is a rapidly changing library. For those areas that you identify that
require further research, I want you to update the research document with additional details about the specific
versions that we are going to be using in this Taskify application and spawn parallel research tasks to clarify
any details using research from the web.
```

During this process, you might find that Claude Code gets stuck researching the wrong thing - you can help nudge it in the right direction with a prompt like this:

```text
I think we need to break this down into a series of steps. First, identify a list of tasks
that you would need to do during implementation that you&apos;re not sure of or would benefit
from further research. Write down a list of those tasks. And then for each one of these tasks,
I want you to spin up a separate research task so that the net results is we are researching
all of those very specific tasks in parallel. What I saw you doing was it looks like you were
researching .NET Aspire in general and I don&apos;t think that&apos;s gonna do much for us in this case.
That&apos;s way too untargeted research. The research needs to help you solve a specific targeted question.
```

&gt;[!NOTE]
&gt;Claude Code might be over-eager and add components that you did not ask for. Ask it to clarify the rationale and the source of the change.

### **STEP 5:** Have Claude Code validate the plan

With the plan in place, you should have Claude Code run through it to make sure that there are no missing pieces. You can use a prompt like this:

```text
Now I want you to go and audit the implementation plan and the implementation detail files.
Read through it with an eye on determining whether or not there is a sequence of tasks that you need
to be doing that are obvious from reading this. Because I don&apos;t know if there&apos;s enough here. For example,
when I look at the core implementation, it would be useful to reference the appropriate places in the implementation
details where it can find the information as it walks through each step in the core implementation or in the refinement.
```

This helps refine the implementation plan and helps you avoid potential blind spots that Claude Code missed in its planning cycle. Once the initial refinement pass is complete, ask Claude Code to go through the checklist once more before you can get to the implementation.

You can also ask Claude Code (if you have the [GitHub CLI](https://docs.github.com/en/github-cli/github-cli) installed) to go ahead and create a pull request from your current branch to `main` with a detailed description, to make sure that the effort is properly tracked.

&gt;[!NOTE]
&gt;Before you have the agent implement it, it&apos;s also worth prompting Claude Code to cross-check the details to see if there are any over-engineered pieces (remember - it can be over-eager). If over-engineered components or decisions exist, you can ask Claude Code to resolve them. Ensure that Claude Code follows the [constitution](base/memory/constitution.md) as the foundational piece that it must adhere to when establishing the plan.

### STEP 6: Implementation

Once ready, use the `/implement` command to execute your implementation plan:

```text
/implement
```

The `/implement` command will:
- Validate that all prerequisites are in place (constitution, spec, plan, and tasks)
- Parse the task breakdown from `tasks.md`
- Execute tasks in the correct order, respecting dependencies and parallel execution markers
- Follow the TDD approach defined in your task plan
- Provide progress updates and handle errors appropriately

&gt;[!IMPORTANT]
&gt;The AI agent will execute local CLI commands (such as `dotnet`, `npm`, etc.) - make sure you have the required tools installed on your machine.

Once the implementation is complete, test the application and resolve any runtime errors that may not be visible in CLI logs (e.g., browser console errors). You can copy and paste such errors back to your AI agent for resolution.

&lt;/details&gt;

---

## 🔍 Troubleshooting

### Git Credential Manager on Linux

If you&apos;re having issues with Git authentication on Linux, you can install Git Credential Manager:

```bash
#!/usr/bin/env bash
set -e
echo &quot;Downloading Git Credential Manager v2.6.1...&quot;
wget https://github.com/git-ecosystem/git-credential-manager/releases/download/v2.6.1/gcm-linux_amd64.2.6.1.deb
echo &quot;Installing Git Credential Manager...&quot;
sudo dpkg -i gcm-linux_amd64.2.6.1.deb
echo &quot;Configuring Git to use GCM...&quot;
git config --global credential.helper manager
echo &quot;Cleaning up...&quot;
rm gcm-linux_amd64.2.6.1.deb
```

## 👥 Maintainers

- Den Delimarsky ([@localden](https://github.com/localden))
- John Lam ([@jflam](https://github.com/jflam))

## 💬 Support

For support, please open a [GitHub issue](https://github.com/github/spec-kit/issues/new). We welcome bug reports, feature requests, and questions about using Spec-Driven Development.

## 🙏 Acknowledgements

This project is heavily influenced by and based on the work and research of [John Lam](https://github.com/jflam).

## 📄 License

This project is licensed under the terms of the MIT open source license. Please refer to the [LICENSE](./LICENSE) file for the full terms.</file><file path="SECURITY.md">Thanks for helping make GitHub safe for everyone.

# Security

GitHub takes the security of our software products and services seriously, including all of the open source code repositories managed through our GitHub organizations, such as [GitHub](https://github.com/GitHub).

Even though [open source repositories are outside of the scope of our bug bounty program](https://bounty.github.com/index.html#scope) and therefore not eligible for bounty rewards, we will ensure that your finding gets passed along to the appropriate maintainers for remediation. 

## Reporting Security Issues

If you believe you have found a security vulnerability in any GitHub-owned repository, please report it to us through coordinated disclosure.

**Please do not report security vulnerabilities through public GitHub issues, discussions, or pull requests.**

Instead, please send an email to opensource-security[@]github.com.

Please include as much of the information listed below as you can to help us better understand and resolve the issue:

  * The type of issue (e.g., buffer overflow, SQL injection, or cross-site scripting)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

## Policy

See [GitHub&apos;s Safe Harbor Policy](https://docs.github.com/en/site-policy/security-policies/github-bug-bounty-program-legal-safe-harbor#1-safe-harbor-terms)</file><file path="spec-driven.md"># Specification-Driven Development (SDD)

## The Power Inversion

For decades, code has been king. Specifications served code—they were the scaffolding we built and then discarded once the &quot;real work&quot; of coding began. We wrote PRDs to guide development, created design docs to inform implementation, drew diagrams to visualize architecture. But these were always subordinate to the code itself. Code was truth. Everything else was, at best, good intentions. Code was the source of truth, and as it moved forward, specs rarely kept pace. As the asset (code) and the implementation are one, it&apos;s not easy to have a parallel implementation without trying to build from the code.

Spec-Driven Development (SDD) inverts this power structure. Specifications don&apos;t serve code—code serves specifications. The Product Requirements Document (PRD) isn&apos;t a guide for implementation; it&apos;s the source that generates implementation. Technical plans aren&apos;t documents that inform coding; they&apos;re precise definitions that produce code. This isn&apos;t an incremental improvement to how we build software. It&apos;s a fundamental rethinking of what drives development.

The gap between specification and implementation has plagued software development since its inception. We&apos;ve tried to bridge it with better documentation, more detailed requirements, stricter processes. These approaches fail because they accept the gap as inevitable. They try to narrow it but never eliminate it. SDD eliminates the gap by making specifications and their concrete implementation plans born from the specification executable. When specifications and implementation plans generate code, there is no gap—only transformation.

This transformation is now possible because AI can understand and implement complex specifications, and create detailed implementation plans. But raw AI generation without structure produces chaos. SDD provides that structure through specifications and subsequent implementation plans that are precise, complete, and unambiguous enough to generate working systems. The specification becomes the primary artifact. Code becomes its expression (as an implementation from the implementation plan) in a particular language and framework.

In this new world, maintaining software means evolving specifications. The intent of the development team is expressed in natural language (&quot;**intent-driven development**&quot;), design assets, core principles and other guidelines. The **lingua franca** of development moves to a higher level, and code is the last-mile approach.

Debugging means fixing specifications and their implementation plans that generate incorrect code. Refactoring means restructuring for clarity. The entire development workflow reorganizes around specifications as the central source of truth, with implementation plans and code as the continuously regenerated output. Updating apps with new features or creating a new parallel implementation because we are creative beings, means revisiting the specification and creating new implementation plans. This process is therefore a 0 -&gt; 1, (1&apos;, ..), 2, 3, N.

The development team focuses in on their creativity, experimentation, their critical thinking.

## The SDD Workflow in Practice

The workflow begins with an idea—often vague and incomplete. Through iterative dialogue with AI, this idea becomes a comprehensive PRD. The AI asks clarifying questions, identifies edge cases, and helps define precise acceptance criteria. What might take days of meetings and documentation in traditional development happens in hours of focused specification work. This transforms the traditional SDLC—requirements and design become continuous activities rather than discrete phases. This is supportive of a **team process**, where team-reviewed specifications are expressed and versioned, created in branches, and merged.

When a product manager updates acceptance criteria, implementation plans automatically flag affected technical decisions. When an architect discovers a better pattern, the PRD updates to reflect new possibilities.

Throughout this specification process, research agents gather critical context. They investigate library compatibility, performance benchmarks, and security implications. Organizational constraints are discovered and applied automatically—your company&apos;s database standards, authentication requirements, and deployment policies seamlessly integrate into every specification.

From the PRD, AI generates implementation plans that map requirements to technical decisions. Every technology choice has documented rationale. Every architectural decision traces back to specific requirements. Throughout this process, consistency validation continuously improves quality. AI analyzes specifications for ambiguity, contradictions, and gaps—not as a one-time gate, but as an ongoing refinement.

Code generation begins as soon as specifications and their implementation plans are stable enough, but they do not have to be &quot;complete.&quot; Early generations might be exploratory—testing whether the specification makes sense in practice. Domain concepts become data models. User stories become API endpoints. Acceptance scenarios become tests. This merges development and testing through specification—test scenarios aren&apos;t written after code, they&apos;re part of the specification that generates both implementation and tests.

The feedback loop extends beyond initial development. Production metrics and incidents don&apos;t just trigger hotfixes—they update specifications for the next regeneration. Performance bottlenecks become new non-functional requirements. Security vulnerabilities become constraints that affect all future generations. This iterative dance between specification, implementation, and operational reality is where true understanding emerges and where the traditional SDLC transforms into a continuous evolution.

## Why SDD Matters Now

Three trends make SDD not just possible but necessary:

First, AI capabilities have reached a threshold where natural language specifications can reliably generate working code. This isn&apos;t about replacing developers—it&apos;s about amplifying their effectiveness by automating the mechanical translation from specification to implementation. It can amplify exploration and creativity, support &quot;start-over&quot; easily, and support addition, subtraction, and critical thinking.

Second, software complexity continues to grow exponentially. Modern systems integrate dozens of services, frameworks, and dependencies. Keeping all these pieces aligned with original intent through manual processes becomes increasingly difficult. SDD provides systematic alignment through specification-driven generation. Frameworks may evolve to provide AI-first support, not human-first support, or architect around reusable components.

Third, the pace of change accelerates. Requirements change far more rapidly today than ever before. Pivoting is no longer exceptional—it&apos;s expected. Modern product development demands rapid iteration based on user feedback, market conditions, and competitive pressures. Traditional development treats these changes as disruptions. Each pivot requires manually propagating changes through documentation, design, and code. The result is either slow, careful updates that limit velocity, or fast, reckless changes that accumulate technical debt.

SDD can support what-if/simulation experiments: &quot;If we need to re-implement or change the application to promote a business need to sell more T-shirts, how would we implement and experiment for that?&quot;

SDD transforms requirement changes from obstacles into normal workflow. When specifications drive implementation, pivots become systematic regenerations rather than manual rewrites. Change a core requirement in the PRD, and affected implementation plans update automatically. Modify a user story, and corresponding API endpoints regenerate. This isn&apos;t just about initial development—it&apos;s about maintaining engineering velocity through inevitable changes.

## Core Principles

**Specifications as the Lingua Franca**: The specification becomes the primary artifact. Code becomes its expression in a particular language and framework. Maintaining software means evolving specifications.

**Executable Specifications**: Specifications must be precise, complete, and unambiguous enough to generate working systems. This eliminates the gap between intent and implementation.

**Continuous Refinement**: Consistency validation happens continuously, not as a one-time gate. AI analyzes specifications for ambiguity, contradictions, and gaps as an ongoing process.

**Research-Driven Context**: Research agents gather critical context throughout the specification process, investigating technical options, performance implications, and organizational constraints.

**Bidirectional Feedback**: Production reality informs specification evolution. Metrics, incidents, and operational learnings become inputs for specification refinement.

**Branching for Exploration**: Generate multiple implementation approaches from the same specification to explore different optimization targets—performance, maintainability, user experience, cost.

## Implementation Approaches

Today, practicing SDD requires assembling existing tools and maintaining discipline throughout the process. The methodology can be practiced with:

- AI assistants for iterative specification development
- Research agents for gathering technical context
- Code generation tools for translating specifications to implementation
- Version control systems adapted for specification-first workflows
- Consistency checking through AI analysis of specification documents

The key is treating specifications as the source of truth, with code as the generated output that serves the specification rather than the other way around.

## Streamlining SDD with Commands

The SDD methodology is significantly enhanced through three powerful commands that automate the specification → planning → tasking workflow:

### The `/specify` Command

This command transforms a simple feature description (the user-prompt) into a complete, structured specification with automatic repository management:

1. **Automatic Feature Numbering**: Scans existing specs to determine the next feature number (e.g., 001, 002, 003)
2. **Branch Creation**: Generates a semantic branch name from your description and creates it automatically
3. **Template-Based Generation**: Copies and customizes the feature specification template with your requirements
4. **Directory Structure**: Creates the proper `specs/[branch-name]/` structure for all related documents

### The `/plan` Command

Once a feature specification exists, this command creates a comprehensive implementation plan:

1. **Specification Analysis**: Reads and understands the feature requirements, user stories, and acceptance criteria
2. **Constitutional Compliance**: Ensures alignment with project constitution and architectural principles
3. **Technical Translation**: Converts business requirements into technical architecture and implementation details
4. **Detailed Documentation**: Generates supporting documents for data models, API contracts, and test scenarios
5. **Quickstart Validation**: Produces a quickstart guide capturing key validation scenarios

### The `/tasks` Command

After a plan is created, this command analyzes the plan and related design documents to generate an executable task list:

1. **Inputs**: Reads `plan.md` (required) and, if present, `data-model.md`, `contracts/`, and `research.md`
2. **Task Derivation**: Converts contracts, entities, and scenarios into specific tasks
3. **Parallelization**: Marks independent tasks `[P]` and outlines safe parallel groups
4. **Output**: Writes `tasks.md` in the feature directory, ready for execution by a Task agent

### Example: Building a Chat Feature

Here&apos;s how these commands transform the traditional development workflow:

**Traditional Approach:**

```text
1. Write a PRD in a document (2-3 hours)
2. Create design documents (2-3 hours)
3. Set up project structure manually (30 minutes)
4. Write technical specifications (3-4 hours)
5. Create test plans (2 hours)
Total: ~12 hours of documentation work
```

**SDD with Commands Approach:**

```bash
# Step 1: Create the feature specification (5 minutes)
/specify Real-time chat system with message history and user presence

# This automatically:
# - Creates branch &quot;003-chat-system&quot;
# - Generates specs/003-chat-system/spec.md
# - Populates it with structured requirements

# Step 2: Generate implementation plan (5 minutes)
/plan WebSocket for real-time messaging, PostgreSQL for history, Redis for presence

# Step 3: Generate executable tasks (5 minutes)
/tasks

# This automatically creates:
# - specs/003-chat-system/plan.md
# - specs/003-chat-system/research.md (WebSocket library comparisons)
# - specs/003-chat-system/data-model.md (Message and User schemas)
# - specs/003-chat-system/contracts/ (WebSocket events, REST endpoints)
# - specs/003-chat-system/quickstart.md (Key validation scenarios)
# - specs/003-chat-system/tasks.md (Task list derived from the plan)
```

In 15 minutes, you have:

- A complete feature specification with user stories and acceptance criteria
- A detailed implementation plan with technology choices and rationale
- API contracts and data models ready for code generation
- Comprehensive test scenarios for both automated and manual testing
- All documents properly versioned in a feature branch

### The Power of Structured Automation

These commands don&apos;t just save time—they enforce consistency and completeness:

1. **No Forgotten Details**: Templates ensure every aspect is considered, from non-functional requirements to error handling
2. **Traceable Decisions**: Every technical choice links back to specific requirements
3. **Living Documentation**: Specifications stay in sync with code because they generate it
4. **Rapid Iteration**: Change requirements and regenerate plans in minutes, not days

The commands embody SDD principles by treating specifications as executable artifacts rather than static documents. They transform the specification process from a necessary evil into the driving force of development.

### Template-Driven Quality: How Structure Constrains LLMs for Better Outcomes

The true power of these commands lies not just in automation, but in how the templates guide LLM behavior toward higher-quality specifications. The templates act as sophisticated prompts that constrain the LLM&apos;s output in productive ways:

#### 1. **Preventing Premature Implementation Details**

The feature specification template explicitly instructs:

```text
- ✅ Focus on WHAT users need and WHY
- ❌ Avoid HOW to implement (no tech stack, APIs, code structure)
```

This constraint forces the LLM to maintain proper abstraction levels. When an LLM might naturally jump to &quot;implement using React with Redux,&quot; the template keeps it focused on &quot;users need real-time updates of their data.&quot; This separation ensures specifications remain stable even as implementation technologies change.

#### 2. **Forcing Explicit Uncertainty Markers**

Both templates mandate the use of `[NEEDS CLARIFICATION]` markers:

```text
When creating this spec from a user prompt:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question]
2. **Don&apos;t guess**: If the prompt doesn&apos;t specify something, mark it
```

This prevents the common LLM behavior of making plausible but potentially incorrect assumptions. Instead of guessing that a &quot;login system&quot; uses email/password authentication, the LLM must mark it as `[NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]`.

#### 3. **Structured Thinking Through Checklists**

The templates include comprehensive checklists that act as &quot;unit tests&quot; for the specification:

```markdown
### Requirement Completeness
- [ ] No [NEEDS CLARIFICATION] markers remain
- [ ] Requirements are testable and unambiguous
- [ ] Success criteria are measurable
```

These checklists force the LLM to self-review its output systematically, catching gaps that might otherwise slip through. It&apos;s like giving the LLM a quality assurance framework.

#### 4. **Constitutional Compliance Through Gates**

The implementation plan template enforces architectural principles through phase gates:

```markdown
### Phase -1: Pre-Implementation Gates
#### Simplicity Gate (Article VII)
- [ ] Using ≤3 projects?
- [ ] No future-proofing?
#### Anti-Abstraction Gate (Article VIII)
- [ ] Using framework directly?
- [ ] Single model representation?
```

These gates prevent over-engineering by making the LLM explicitly justify any complexity. If a gate fails, the LLM must document why in the &quot;Complexity Tracking&quot; section, creating accountability for architectural decisions.

#### 5. **Hierarchical Detail Management**

The templates enforce proper information architecture:

```text
**IMPORTANT**: This implementation plan should remain high-level and readable.
Any code samples, detailed algorithms, or extensive technical specifications
must be placed in the appropriate `implementation-details/` file
```

This prevents the common problem of specifications becoming unreadable code dumps. The LLM learns to maintain appropriate detail levels, extracting complexity to separate files while keeping the main document navigable.

#### 6. **Test-First Thinking**

The implementation template enforces test-first development:

```text
### File Creation Order
1. Create `contracts/` with API specifications
2. Create test files in order: contract → integration → e2e → unit
3. Create source files to make tests pass
```

This ordering constraint ensures the LLM thinks about testability and contracts before implementation, leading to more robust and verifiable specifications.

#### 7. **Preventing Speculative Features**

Templates explicitly discourage speculation:

```text
- [ ] No speculative or &quot;might need&quot; features
- [ ] All phases have clear prerequisites and deliverables
```

This stops the LLM from adding &quot;nice to have&quot; features that complicate implementation. Every feature must trace back to a concrete user story with clear acceptance criteria.

### The Compound Effect

These constraints work together to produce specifications that are:

- **Complete**: Checklists ensure nothing is forgotten
- **Unambiguous**: Forced clarification markers highlight uncertainties
- **Testable**: Test-first thinking baked into the process
- **Maintainable**: Proper abstraction levels and information hierarchy
- **Implementable**: Clear phases with concrete deliverables

The templates transform the LLM from a creative writer into a disciplined specification engineer, channeling its capabilities toward producing consistently high-quality, executable specifications that truly drive development.

## The Constitutional Foundation: Enforcing Architectural Discipline

At the heart of SDD lies a constitution—a set of immutable principles that govern how specifications become code. The constitution (`memory/constitution.md`) acts as the architectural DNA of the system, ensuring that every generated implementation maintains consistency, simplicity, and quality.

### The Nine Articles of Development

The constitution defines nine articles that shape every aspect of the development process:

#### Article I: Library-First Principle

Every feature must begin as a standalone library—no exceptions. This forces modular design from the start:

```text
Every feature in Specify MUST begin its existence as a standalone library.
No feature shall be implemented directly within application code without
first being abstracted into a reusable library component.
```

This principle ensures that specifications generate modular, reusable code rather than monolithic applications. When the LLM generates an implementation plan, it must structure features as libraries with clear boundaries and minimal dependencies.

#### Article II: CLI Interface Mandate

Every library must expose its functionality through a command-line interface:

```text
All CLI interfaces MUST:
- Accept text as input (via stdin, arguments, or files)
- Produce text as output (via stdout)
- Support JSON format for structured data exchange
```

This enforces observability and testability. The LLM cannot hide functionality inside opaque classes—everything must be accessible and verifiable through text-based interfaces.

#### Article III: Test-First Imperative

The most transformative article—no code before tests:

```text
This is NON-NEGOTIABLE: All implementation MUST follow strict Test-Driven Development.
No implementation code shall be written before:
1. Unit tests are written
2. Tests are validated and approved by the user
3. Tests are confirmed to FAIL (Red phase)
```

This completely inverts traditional AI code generation. Instead of generating code and hoping it works, the LLM must first generate comprehensive tests that define behavior, get them approved, and only then generate implementation.

#### Articles VII &amp; VIII: Simplicity and Anti-Abstraction

These paired articles combat over-engineering:

```text
Section 7.3: Minimal Project Structure
- Maximum 3 projects for initial implementation
- Additional projects require documented justification

Section 8.1: Framework Trust
- Use framework features directly rather than wrapping them
```

When an LLM might naturally create elaborate abstractions, these articles force it to justify every layer of complexity. The implementation plan template&apos;s &quot;Phase -1 Gates&quot; directly enforce these principles.

#### Article IX: Integration-First Testing

Prioritizes real-world testing over isolated unit tests:

```text
Tests MUST use realistic environments:
- Prefer real databases over mocks
- Use actual service instances over stubs
- Contract tests mandatory before implementation
```

This ensures generated code works in practice, not just in theory.

### Constitutional Enforcement Through Templates

The implementation plan template operationalizes these articles through concrete checkpoints:

```markdown
### Phase -1: Pre-Implementation Gates
#### Simplicity Gate (Article VII)
- [ ] Using ≤3 projects?
- [ ] No future-proofing?

#### Anti-Abstraction Gate (Article VIII)
- [ ] Using framework directly?
- [ ] Single model representation?

#### Integration-First Gate (Article IX)
- [ ] Contracts defined?
- [ ] Contract tests written?
```

These gates act as compile-time checks for architectural principles. The LLM cannot proceed without either passing the gates or documenting justified exceptions in the &quot;Complexity Tracking&quot; section.

### The Power of Immutable Principles

The constitution&apos;s power lies in its immutability. While implementation details can evolve, the core principles remain constant. This provides:

1. **Consistency Across Time**: Code generated today follows the same principles as code generated next year
2. **Consistency Across LLMs**: Different AI models produce architecturally compatible code
3. **Architectural Integrity**: Every feature reinforces rather than undermines the system design
4. **Quality Guarantees**: Test-first, library-first, and simplicity principles ensure maintainable code

### Constitutional Evolution

While principles are immutable, their application can evolve:

```text
Section 4.2: Amendment Process
Modifications to this constitution require:
- Explicit documentation of the rationale for change
- Review and approval by project maintainers
- Backwards compatibility assessment
```

This allows the methodology to learn and improve while maintaining stability. The constitution shows its own evolution with dated amendments, demonstrating how principles can be refined based on real-world experience.

### Beyond Rules: A Development Philosophy

The constitution isn&apos;t just a rulebook—it&apos;s a philosophy that shapes how LLMs think about code generation:

- **Observability Over Opacity**: Everything must be inspectable through CLI interfaces
- **Simplicity Over Cleverness**: Start simple, add complexity only when proven necessary
- **Integration Over Isolation**: Test in real environments, not artificial ones
- **Modularity Over Monoliths**: Every feature is a library with clear boundaries

By embedding these principles into the specification and planning process, SDD ensures that generated code isn&apos;t just functional—it&apos;s maintainable, testable, and architecturally sound. The constitution transforms AI from a code generator into an architectural partner that respects and reinforces system design principles.

## The Transformation

This isn&apos;t about replacing developers or automating creativity. It&apos;s about amplifying human capability by automating mechanical translation. It&apos;s about creating a tight feedback loop where specifications, research, and code evolve together, each iteration bringing deeper understanding and better alignment between intent and implementation.

Software development needs better tools for maintaining alignment between intent and implementation. SDD provides the methodology for achieving this alignment through executable specifications that generate code rather than merely guiding it.</file><file path="SUPPORT.md"># Support 

## How to file issues and get help

This project uses GitHub issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new issue.

For help or questions about using this project, please:

- Open a [GitHub issue](https://github.com/github/spec-kit/issues/new) for bug reports, feature requests, or questions about the Spec-Driven Development methodology
- Check the [comprehensive guide](./spec-driven.md) for detailed documentation on the Spec-Driven Development process
- Review the [README](./README.md) for getting started instructions and troubleshooting tips

## Project Status

**Spec Kit** is under active development and maintained by GitHub staff **AND THE COMMUNITY**. We will do our best to respond to support, feature requests, and community questions in a timely manner.

## GitHub Support Policy

Support for this project is limited to the resources listed above.</file></files></repomix>